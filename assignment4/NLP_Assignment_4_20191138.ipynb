{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "38583105",
      "metadata": {
        "id": "38583105"
      },
      "source": [
        "# Assignment 4: Attention\n",
        "- In this assignment, you have to implement the attention mechanism for the machine translation task.\n",
        "\n",
        "- Problems\n",
        "  - 1. Implement Dot Product Attention (10 pts)\n",
        "  - 2. Implement Attention in Batch (12 pts)\n",
        "  - 3. Implement Seq2seq with Attention (14 pts)\n",
        "  - 4. Implement Transformer-like Attention (14 pts)\n",
        "    - Self-attention\n",
        "    - Cross-attention\n",
        "\n",
        "- Submission\n",
        "  - You have to copy and paste your answer to ``NLP_Assignment_4.py`` file and submit it to Cyber Campus\n",
        "    - Submssion file name has to be ``NLP_Assignment_4_{your_student_id}.py``\n",
        "    - { } character is just placeholder, so don't inlcude it in your filename.\n",
        "\n",
        "- CAUTION:\n",
        "  - You have to implement most of the functions with **vectorized matrix multiplication**, not with for loop\n",
        "\n",
        "\n",
        "\n",
        "- If you find any error, please do not hesitate to report or make a question on Cyber Campus\n",
        "    - Don't waste too much time on solving the error. The code is not thoroughly checked, and the error can be not your fault."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095f3a9c",
      "metadata": {
        "id": "095f3a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1862194-2ffa-4f42-9e8c-da203b6945b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting koreanize-matplotlib\n",
            "  Downloading koreanize_matplotlib-0.1.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from koreanize-matplotlib) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->koreanize-matplotlib) (1.16.0)\n",
            "Installing collected packages: koreanize-matplotlib\n",
            "Successfully installed koreanize-matplotlib-0.1.1\n"
          ]
        }
      ],
      "source": [
        "# If you are in Colab, install transformers\n",
        "!pip -q install transformers\n",
        "!pip install koreanize-matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ace9b0",
      "metadata": {
        "id": "40ace9b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bc440d-167a-418b-f892-590f0712f447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-26 09:32:26--  https://raw.githubusercontent.com/jdasam/aat3020/main/NLP_Assignment_4.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41850 (41K) [text/plain]\n",
            "Saving to: ‘NLP_Assignment_4.py’\n",
            "\n",
            "\rNLP_Assignment_4.py   0%[                    ]       0  --.-KB/s               \rNLP_Assignment_4.py 100%[===================>]  40.87K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-06-26 09:32:26 (3.48 MB/s) - ‘NLP_Assignment_4.py’ saved [41850/41850]\n",
            "\n",
            "--2024-06-26 09:32:26--  https://raw.githubusercontent.com/jdasam/aat3020/main/assignment4_pre_defined.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13768 (13K) [text/plain]\n",
            "Saving to: ‘assignment4_pre_defined.py’\n",
            "\n",
            "assignment4_pre_def 100%[===================>]  13.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-26 09:32:26 (82.7 MB/s) - ‘assignment4_pre_defined.py’ saved [13768/13768]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the py file for submission\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/main/NLP_Assignment_4.py\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/main/assignment4_pre_defined.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8635fb79",
      "metadata": {
        "id": "8635fb79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import koreanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "import os\n",
        "\n",
        "# Below helps to run tokenizer with multiprocessing\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd6024f",
      "metadata": {
        "id": "ecd6024f"
      },
      "source": [
        "#### Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f182b757",
      "metadata": {
        "id": "f182b757",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92eda82-6e45-44bb-d408-b234ac6832c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6500, -4.2786,  1.2358,  ..., -6.6063,  4.3687,  2.4036],\n",
              "        [ 2.3086,  3.3307,  0.1186,  ..., -0.2289, -1.8618, -1.9732],\n",
              "        [ 1.4200, -9.4868,  0.0942,  ...,  5.3254, -0.9110, -1.3864],\n",
              "        ...,\n",
              "        [-0.3171, -2.3594, -0.6865,  ..., -0.5154,  2.3111,  0.1890],\n",
              "        [ 2.0520,  6.1466, -6.2863,  ...,  0.3353, -6.4856,  0.6394],\n",
              "        [ 1.7735, -3.4298,  4.5868,  ...,  0.2979, -2.8560,  0.5707]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "This is the example of vectorization of dot product for two different sequence length.\n",
        "'''\n",
        "\n",
        "e_states = torch.randn(100, 16)\n",
        "d_states = torch.randn(80, 16)\n",
        "\n",
        "dot_product = torch.mm(e_states, d_states.permute(1,0)) # (100, 16) x (16, 80) = (100, 80)\n",
        "dot_product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfefeae",
      "metadata": {
        "id": "fcfefeae"
      },
      "source": [
        "## Problem 1: Implement Dot Product Attention\n",
        "\n",
        "- Optimizing computation time is really important\n",
        "    - Use `torch.mm()` or `torch.matmul()`\n",
        "    - `torch.mm(a, b)` is a function for calculating matrix multiplcation of two matrices `a` and `b`\n",
        "        - `a` and `b` has to be 2-dim tensors\n",
        "        - `a.shape[1]` has to be equal to `b.shape[0]`\n",
        "    - `torch.matmul()` is a function for matrix multiplication but with broadcasting\n",
        "        - https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "        - It has less restriction on its input shape.\n",
        "            - It automatically matches the dimension of two tensors following some rules\n",
        "            - Therefore, it is a bit risky to use this funciton if you don't understand how it works\n",
        "- **DO NOT** use element-wise product or for loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2059e903",
      "metadata": {
        "id": "2059e903"
      },
      "source": [
        "### Hint: Dot product as matrix multiplcation.\n",
        "\n",
        "- Let's say there are two vector, $u=\\begin{bmatrix}-3 \\\\ 2 \\\\ 1\\end{bmatrix}$ and $v = \\begin{bmatrix} 5 \\\\ 4 \\\\ 6\\end{bmatrix}$\n",
        "    - The dot product of the two vectors is $(-3 \\times 5) + (2 \\times 4) + (1 \\times 6) = 1$\n",
        "    - It is equivalent to $u^T \\times v$\n",
        "        - In this case $u\\in\\mathbb{R}^{3\\times1}$ and $v\\in\\mathbb{R}^{3\\times1}$\n",
        "- In PyTorch, this can be described as below:\n",
        "    - `u = torch.Tensor([-3, 2, 1])`\n",
        "    - `v = torch.Tensor([5, 4, 6])`\n",
        "    - Dot product of u and v can be calculated by one of belows:\n",
        "        - `torch.mm(u.unsqueeze(0), v.unsqueeze(1))`\n",
        "            - `u.unsqueeze(0).shape == [1, 3]`\n",
        "            - `v.unsqueeze(1).shape == [3, 1]`\n",
        "            - `unsqueeze()` returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "            - The result has shape of [1,1]\n",
        "        - `torch.matmul(u, v)`\n",
        "        - `u @ v`\n",
        "            - `@` denotes matrix multiplication, which was introduced from Python 3.5\n",
        "        - `(u * v).sum()`\n",
        "            - This will be much slower than others, because it first do element-wise multiplcation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4aed8a2",
      "metadata": {
        "id": "a4aed8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f359d432-7289-45ad-afe4-9d9685d79986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of (u * v).sum() is -1.0. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\n",
            "Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is tensor([[-1.]])\n",
            "Result of torch.matmul(u, v) is -1.0\n",
            "Result of u @ v is -1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Hint: Dot product as matrix multiplcation.\n",
        "'''\n",
        "\n",
        "u = torch.Tensor([-3, 2, 1])\n",
        "v = torch.Tensor([5, 4, 6])\n",
        "\n",
        "print(f\"Result of (u * v).sum() is {(u * v).sum()}. This computation is much slower than others because it use element-wise multiplication instead of matrix multiplication\")\n",
        "print(f\"Result of torch.mm(u.unsqueeze(0), v.unsqueeze(1)) is {torch.mm(u.unsqueeze(0), v.unsqueeze(1))}\")\n",
        "print(f\"Result of torch.matmul(u, v) is {torch.matmul(u, v)}\")\n",
        "print(f\"Result of u @ v is {u @ v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15156e7",
      "metadata": {
        "id": "b15156e7"
      },
      "source": [
        "### 1-1 Get attention score with dot product (4 pts)\n",
        "- From a sequence of key and a single query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a20b45",
      "metadata": {
        "id": "66a20b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803f5f99-4c1a-4509-87d7-a2bcbf502031"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
              "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
              "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def get_attention_score_for_a_single_query(keys, query):\n",
        "  '''\n",
        "  This function returns an attention score for each vector in keys for a given query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [T]\n",
        "\n",
        "    attention_score[i] has to be a dot product value between keys[i] and query\n",
        "\n",
        "\n",
        "  TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
        "  Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
        "  '''\n",
        "\n",
        "  attention_scores= torch.mm(keys, query.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "  return attention_scores\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_t, h_size)\n",
        "query = torch.randn(h_size)\n",
        "\n",
        "att_score = get_attention_score_for_a_single_query(keys, query)\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce66bf6c",
      "metadata": {
        "id": "ce66bf6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e76eab4-eae5-4a0d-c8ca-6666cbefa4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test Case\n",
        "'''\n",
        "assert att_score.ndim == 1 and len(att_score) == num_t, \"Error: Check output shape\"\n",
        "answer = torch.Tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
        "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
        "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])\n",
        "assert torch.allclose(att_score, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e9cd39",
      "metadata": {
        "id": "97e9cd39"
      },
      "source": [
        "### 1-2 Get attention weight from score (2pts)\n",
        "- Convert attention score to attention weight\n",
        "- Use softmax function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63d332d",
      "metadata": {
        "id": "b63d332d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080a746e-4e64-4ee9-9665-024ade2847ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.7782e-06, 1.2936e-03, 8.8653e-04, 9.4370e-07, 4.0957e-03, 1.9541e-04,\n",
              "        5.5277e-05, 2.2321e-05, 1.9005e-04, 2.7829e-03, 1.2303e-04, 5.1099e-05,\n",
              "        1.7052e-04, 5.7296e-05, 3.5821e-04, 3.2593e-06, 1.1314e-05, 4.5893e-05,\n",
              "        1.5795e-05, 2.2107e-03, 8.2463e-05, 9.7556e-01, 1.1777e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def get_attention_weight_from_score(attention_score):\n",
        "  '''\n",
        "  This function converts attention score to attention weight.\n",
        "\n",
        "  Argument:\n",
        "    attention_score (torch.Tensor): Tensor of real number. Has a shape of [T]\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): Tensor of real number between 0 and 1. Sum of attention_weight is 1. Has a shape of [T]\n",
        "\n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  assert attention_score.ndim == 1\n",
        "\n",
        "  attention_weight = torch.softmax(attention_score, dim=0)\n",
        "  return attention_weight\n",
        "\n",
        "att_weight = get_attention_weight_from_score(att_score)\n",
        "att_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8030b21a",
      "metadata": {
        "id": "8030b21a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e112bd-1a99-48e6-95c4-59085c4ef8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([0.0000,     0.0013,     0.0009,     0.0000,     0.0041,     0.0002,\n",
        "            0.0001,     0.0000,     0.0002,     0.0028,     0.0001,     0.0001,\n",
        "            0.0002,     0.0001,     0.0004,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0022,     0.0001,     0.9756,     0.0118])\n",
        "assert att_weight.shape == att_score.shape, 'Shape has to be remained the same'\n",
        "assert att_weight.sum() == 1, \"Sum of attention weight has to be 1\"\n",
        "assert torch.allclose(att_weight, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e2309e",
      "metadata": {
        "id": "26e2309e"
      },
      "source": [
        "### 1-3 Get weighted sum (4 pts)\n",
        "- Using attention weight and values, get the weighted sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf8e701",
      "metadata": {
        "id": "6bf8e701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e886e6-7780-4821-d937-b8b5176a5e54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
              "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def get_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [T, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [T], which represents the weight for each vector to compose the attention vector\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
        "\n",
        "  TODO: Complete this function using torch.mm\n",
        "  '''\n",
        "\n",
        "  attention_weight = attention_weight.unsqueeze(1)\n",
        "  attention_vector = torch.mm(values.t(), attention_weight).squeeze(-1)\n",
        "  return attention_vector\n",
        "\n",
        "\n",
        "\n",
        "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
        "att_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1253844f",
      "metadata": {
        "id": "1253844f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20686d90-1275-4dfa-b9df-2beec5f4b580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
        "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])\n",
        "assert att_vec.shape == query.shape, 'Shape has to be remained the same'\n",
        "assert torch.allclose(att_vec, answer, atol=1e-4) , \"Error: The output value is different\"\n",
        "print(\"Passed all the cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc00df77",
      "metadata": {
        "id": "cc00df77"
      },
      "source": [
        "## Problem 2: Attention in Batch ( 16 pts)\n",
        "- In this problem, you have to calculate attention with batch\n",
        "- You can use `torch.bmm()` for batch matrix multiplication https://pytorch.org/docs/stable/generated/torch.bmm.html\n",
        "    - `torch.bmm()` takes two 3-dim tensor as its input\n",
        "    - Each tensor has to be 3-dim (atensor.ndim==3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062446fe",
      "metadata": {
        "id": "062446fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e291f596-88ff-416e-b164-92269942911c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix_left1: \n",
            "tensor([[ 1.5410, -0.2934, -2.1788],\n",
            "        [ 0.5684, -1.0845, -1.3986],\n",
            "        [ 0.4033,  0.8380, -0.7193],\n",
            "        [-0.4033, -0.5966,  0.1820],\n",
            "        [-0.8567,  1.1006, -1.0712]])\n",
            "matrix_left2: \n",
            "tensor([[ 0.1227, -0.5663,  0.3731],\n",
            "        [-0.8920, -1.5091,  0.3704],\n",
            "        [ 1.4565,  0.9398,  0.7748],\n",
            "        [ 0.1919,  1.2638, -1.2904],\n",
            "        [-0.7911, -0.0209, -0.7185]])\n",
            "matrix_right1: \n",
            "tensor([[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
            "        [-2.2188,  0.2590, -1.0297, -0.5008],\n",
            "        [ 0.2734, -0.9181, -0.0404,  0.2881]])\n",
            "matrix_right2: \n",
            "tensor([[-0.0075, -0.9145, -1.0886, -0.2666],\n",
            "        [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
            "        [ 0.0627, -0.7663,  1.0993,  2.7565]])\n",
            "Let's assume that we have batch of matrix, which is stack of these two matices\n",
            "matrix_left: \n",
            "tensor([[[ 1.5410, -0.2934, -2.1788],\n",
            "         [ 0.5684, -1.0845, -1.3986],\n",
            "         [ 0.4033,  0.8380, -0.7193],\n",
            "         [-0.4033, -0.5966,  0.1820],\n",
            "         [-0.8567,  1.1006, -1.0712]],\n",
            "\n",
            "        [[ 0.1227, -0.5663,  0.3731],\n",
            "         [-0.8920, -1.5091,  0.3704],\n",
            "         [ 1.4565,  0.9398,  0.7748],\n",
            "         [ 0.1919,  1.2638, -1.2904],\n",
            "         [-0.7911, -0.0209, -0.7185]]]) \n",
            " which is shape of torch.Size([2, 5, 3])\n",
            "matrix_right: \n",
            "tensor([[[ 0.5186, -1.3125,  0.1920,  0.5428],\n",
            "         [-2.2188,  0.2590, -1.0297, -0.5008],\n",
            "         [ 0.2734, -0.9181, -0.0404,  0.2881]],\n",
            "\n",
            "        [[-0.0075, -0.9145, -1.0886, -0.2666],\n",
            "         [ 0.1894, -0.2190,  2.0576, -0.0354],\n",
            "         [ 0.0627, -0.7663,  1.0993,  2.7565]]])\n",
            " which is shape of torch.Size([2, 3, 4])\n",
            "mat_mul_stack: \n",
            "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
            "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
            "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
            "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
            "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
            "\n",
            "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
            "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
            "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
            "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
            "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
            " which is shape of torch.Size([2, 5, 4])\n",
            "mat_mul_bmm: \n",
            "tensor([[[ 0.8547, -0.0982,  0.6861,  0.3556],\n",
            "         [ 2.3188,  0.2571,  1.2824,  0.4487],\n",
            "         [-1.8468,  0.3480, -0.7564, -0.4080],\n",
            "         [ 1.1644,  0.2078,  0.5296,  0.1323],\n",
            "         [-3.1791,  2.3929, -1.2545, -1.3247]],\n",
            "\n",
            "        [[-0.0848, -0.2741, -0.8887,  1.0159],\n",
            "         [-0.2559,  0.8624, -1.7270,  1.3123],\n",
            "         [ 0.2156, -2.1316,  1.2000,  1.7143],\n",
            "         [ 0.1570,  0.5366,  0.9730, -3.6531],\n",
            "         [-0.0431,  1.2786,  0.0284, -1.7689]]])\n",
            " which is shape of torch.Size([2, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Hint for Problem 2\n",
        "\n",
        "You can calculate matrix multiplication of matrices in batch effectively using torch.bmm() or torch.matmul()\n",
        "'''\n",
        "\n",
        "torch.manual_seed(0)\n",
        "matrix_left1 = torch.randn(5, 3)\n",
        "matrix_left2 = torch.randn(5, 3)\n",
        "\n",
        "print(f\"matrix_left1: \\n{matrix_left1}\")\n",
        "print(f\"matrix_left2: \\n{matrix_left2}\")\n",
        "\n",
        "matrix_right1 = torch.randn(3, 4)\n",
        "matrix_right2 = torch.randn(3, 4)\n",
        "print(f\"matrix_right1: \\n{matrix_right1}\")\n",
        "print(f\"matrix_right2: \\n{matrix_right2}\")\n",
        "\n",
        "print(\"Let's assume that we have batch of matrix, which is stack of these two matices\")\n",
        "matrix_left = torch.stack([matrix_left1, matrix_left2])\n",
        "matrix_right = torch.stack([matrix_right1, matrix_right2])\n",
        "\n",
        "print(f\"matrix_left: \\n{matrix_left} \\n which is shape of {matrix_left.shape}\")\n",
        "print(f\"matrix_right: \\n{matrix_right}\\n which is shape of {matrix_right.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Exhaustive method: using torch.mm() only with for loop (This is SLOW when matrix gets much larger)\n",
        "'''\n",
        "\n",
        "mm_forloop_output = []\n",
        "for sample_index in range(matrix_left.shape[0]):\n",
        "  mat_left = matrix_left[sample_index]\n",
        "  mat_right = matrix_right[sample_index]\n",
        "\n",
        "  mm_result = torch.mm(mat_left, mat_right)\n",
        "  mm_forloop_output.append(mm_result)\n",
        "\n",
        "mm_forloop_stack = torch.stack(mm_forloop_output)\n",
        "print(f\"mat_mul_stack: \\n{mm_forloop_stack}\\n which is shape of {mm_forloop_stack.shape}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Good method: using torch.bmm()\n",
        "'''\n",
        "\n",
        "mat_mul_bmm = torch.bmm(matrix_left, matrix_right)\n",
        "print(f\"mat_mul_bmm: \\n{mat_mul_bmm}\\n which is shape of {mat_mul_bmm.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd08038",
      "metadata": {
        "id": "9bd08038"
      },
      "source": [
        "### 2-1 Get attention score in batch (4 pts)\n",
        "- Now keys and query exist in batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe689641",
      "metadata": {
        "id": "fe689641"
      },
      "outputs": [],
      "source": [
        "def get_attention_score_for_a_batch_query(keys, query):\n",
        "  '''\n",
        "  This function returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, T, C]. These are vectors that a query wants attend to\n",
        "    query (torch.Tensor): Has a shape of [N, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, T]\n",
        "\n",
        "    attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
        "\n",
        "  '''\n",
        "\n",
        "  attention_scores = torch.bmm(keys, query.unsqueeze(2)).squeeze()\n",
        "  return attention_scores\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b,num_t, h_size)\n",
        "query = torch.randn(num_b, h_size)\n",
        "out = get_attention_score_for_a_batch_query(keys, query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59da18f0",
      "metadata": {
        "id": "59da18f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18dc01e-cecc-4403-9788-1935ebefbe38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([ -2.7744,   1.3793,   5.0969,   0.7559,   2.5898,  -0.9475,  -1.1960,\n",
        "           5.4975,   0.4018,   5.9949,  -5.9428,  -0.4441,   0.6729,  -0.8326,\n",
        "           3.7091,   1.4913,   2.2062,  -0.2244,  -4.0612,   2.9037,  10.6111,\n",
        "           4.1383,  -4.6549])\n",
        "\n",
        "assert out.ndim == 2 and out.shape == torch.Size([num_b, num_t]), \"Error: Check output shape\"\n",
        "assert torch.allclose(out[2], answer, atol=1e-4), \"Error: The output value is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a24922",
      "metadata": {
        "id": "25a24922"
      },
      "source": [
        "### 2-2 Get attention score in batch (4 pts)\n",
        "- Implement the same function but in batchified queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae30995b",
      "metadata": {
        "id": "ae30995b",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1216979-4ea6-4561-caa3-ea5610d89d5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  6.1462,   2.7917,   3.2981,  ...,  -1.7558,  -1.9945,   1.7817],\n",
              "         [ -2.0761,   1.5621,  -4.6314,  ...,  -2.9616,   5.0151,  -0.5098],\n",
              "         [ -3.7923,   0.6755,  -2.5517,  ...,  -6.6489,   2.2012,   0.5882],\n",
              "         ...,\n",
              "         [  2.9819,  12.6860,   6.7435,  ...,   3.5522,  -7.0258,   2.3800],\n",
              "         [ -5.4682,  -2.9139,  -0.3054,  ...,   6.4960,  -1.4581, -12.5525],\n",
              "         [ -1.0037,   1.1092,   1.3248,  ...,   2.8827,   3.8804,  -5.4968]],\n",
              "\n",
              "        [[ -2.9907,  -0.1470,  -0.1703,  ...,   2.4992,  -1.8304,   1.1768],\n",
              "         [-15.4518,   2.2430,   4.9486,  ...,   4.4271,  -4.3865,  -9.2907],\n",
              "         [  4.2723,  -0.6171,   2.6252,  ...,  -2.2281,  -2.5648,  -4.1481],\n",
              "         ...,\n",
              "         [  1.3169,  -1.1141,  -1.6058,  ...,   1.9466,   2.6665,  -4.1625],\n",
              "         [ -7.9481,   5.0494,   0.7725,  ...,   0.5016,  -3.3123,  -7.8802],\n",
              "         [  1.8112,  -3.9315,   1.6521,  ...,  -0.2215,  -0.1541,  -6.4050]],\n",
              "\n",
              "        [[ -3.0427,   1.6135,  -0.4640,  ...,   5.9792,   2.2480,  -3.4328],\n",
              "         [ -5.7212,  -8.7972,  -5.4037,  ...,   3.7540,   0.4384,  -3.4441],\n",
              "         [  1.0249,   4.4344,   1.3304,  ...,  -1.3935,   2.5567,  -3.9180],\n",
              "         ...,\n",
              "         [ 10.2565,   6.2974,  -2.4342,  ..., -10.0146,   3.6096,   1.5109],\n",
              "         [  9.3230,  -1.0268,  11.0852,  ...,  -3.6774,   4.8100,  -2.3779],\n",
              "         [  6.1441,  -4.8747,   0.2777,  ...,   7.1300,  -3.4380,   1.1166]],\n",
              "\n",
              "        [[ -5.6300,   1.6126,   2.5171,  ...,   3.5575,   3.1118,   2.0304],\n",
              "         [  2.6453,  -0.9799,   0.6216,  ...,  -3.8061,   3.2136,  -0.5925],\n",
              "         [  3.2414,   5.6337,   3.7929,  ...,   0.3125,   1.4103,  -5.6959],\n",
              "         ...,\n",
              "         [ -1.5721,   2.2379,   4.6053,  ...,  -1.0397,  -3.7134,  -4.8251],\n",
              "         [ -4.5574,   3.2444,   2.8786,  ...,   1.6313,   0.1190,  -1.0030],\n",
              "         [ -1.4957,   0.2640,  -2.1453,  ...,  -2.5968,   0.1317,   2.6204]],\n",
              "\n",
              "        [[ -4.5165,  11.9102,  -1.3893,  ...,  -4.5206,   1.6530,   9.5429],\n",
              "         [  0.5686,  -5.3511,   2.3664,  ...,  -4.7446,   5.1878,   1.1045],\n",
              "         [  6.2639,   0.1570,  -0.1588,  ...,   2.1696,  -5.3117,   3.4494],\n",
              "         ...,\n",
              "         [ -3.1651,  -8.5573,  -2.6017,  ...,  -2.2827,   2.3201, -11.6823],\n",
              "         [  8.1099,   3.2290,  -7.5239,  ...,  -1.7162,  -8.0600,  -0.5222],\n",
              "         [  1.5312,  -0.7118,   5.2198,  ...,   3.8161,  -3.3262,  -2.4121]],\n",
              "\n",
              "        [[  4.3940,  -1.3437,  -0.7749,  ...,   2.1454,  -2.6441,   0.9273],\n",
              "         [ -4.2766,   2.7691,  -5.5275,  ...,  10.0679,  -3.9916,  10.4126],\n",
              "         [ -0.5561,  -5.9345,  -0.2756,  ...,  -2.7162,   2.0791,  -0.3644],\n",
              "         ...,\n",
              "         [  0.8519,   1.8976,   0.7429,  ...,   0.3895,   5.3467,   2.4843],\n",
              "         [ -1.2661,   4.2320,  -1.3352,  ...,   0.2109,  -4.6615,   2.1014],\n",
              "         [ -2.1632,   1.4186,  -0.8282,  ...,   2.8492,  -1.6369,  -0.5276]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
        "  '''\n",
        "  Now you have to implement the attention score for not only single query, but multiple queries.\n",
        "\n",
        "  This function returns a batch of attention score for each vector in keys for given queries.\n",
        "  You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
        "  Name 'keys' are used because it is used for calculating attention score (match rate between given vector and query).\n",
        "\n",
        "  For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "  Arguments:\n",
        "    keys (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that a query wants attend to\n",
        "    queries (torch.Tensor): Has a shape of [N, Tt, C]. This is a vector that attends to other set of vectors (keys and values)\n",
        "\n",
        "  Output:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "\n",
        "    attention_score[n, i, t] has to be a dot product value between keys[n, i] and query[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
        "        Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
        "        and how the result tensor has to look like (shape)\n",
        "  '''\n",
        "\n",
        "  attention_scores = torch.bmm(keys, queries.permute(0, 2, 1))\n",
        "  return attention_scores\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_ts = 23\n",
        "num_tt = 14\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b, num_ts, h_size)\n",
        "queries = torch.randn(num_b, num_tt, h_size)\n",
        "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "\n",
        "att_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb45f70",
      "metadata": {
        "id": "1cb45f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62855526-bc1c-4f7a-ba78-f27c2081faec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 4.9620, -9.6091, -4.9472,  1.4543, -5.6273,  9.1436,  1.4172,  0.0464,\n",
        "        -5.7033,  4.5473,  7.7498,  1.3405, -3.1877,  2.8759])\n",
        "answer2 = torch.Tensor([[ 2.5171,  0.6216,  3.7929,  2.6163,  5.3290,  0.3592,  2.3067, -0.1099,\n",
        "         1.8963,  0.4175, -1.4283,  1.4388, -2.7825, -1.3690, -1.9615, -1.9514,\n",
        "        -6.4635,  1.9574,  0.1868,  8.5354,  4.6053,  2.8786, -2.1453]])\n",
        "assert att_score.ndim == 3 and att_score.shape == torch.Size([num_b, num_ts, num_tt]), 'Check the output shape'\n",
        "assert torch.allclose(att_score[2,4], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_score[3,:,2], answer2, atol=1e-4),  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ab274c",
      "metadata": {
        "id": "51ab274c"
      },
      "source": [
        "### 2-3 Get Masked Softmax (4 pts)\n",
        "- Implement masked softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ef2611",
      "metadata": {
        "id": "19ef2611",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b2b662-7ab5-458b-d58d-a78b4e68d2b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[8.5095e-02, 5.0265e-05, 2.6409e-02,  ..., 1.8632e-04,\n",
              "           4.1884e-05, 1.4554e-03],\n",
              "          [2.2856e-05, 1.4698e-05, 9.5063e-06,  ..., 5.5794e-05,\n",
              "           4.6370e-02, 1.4716e-04],\n",
              "          [4.1080e-06, 6.0563e-06, 7.6066e-05,  ..., 1.3970e-06,\n",
              "           2.7809e-03, 4.4123e-04],\n",
              "          ...,\n",
              "          [3.5946e-03, 9.9608e-01, 8.2803e-01,  ..., 3.7625e-02,\n",
              "           2.7350e-07, 2.6474e-03],\n",
              "          [7.6878e-07, 1.6724e-07, 7.1906e-04,  ..., 7.1448e-01,\n",
              "           7.1609e-05, 8.6639e-10],\n",
              "          [6.6794e-05, 9.3445e-06, 3.6707e-03,  ..., 1.9264e-02,\n",
              "           1.4910e-02, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 7.2275e-05, 8.3620e-06,  ..., 7.2220e-02,\n",
              "           2.6008e-04, 2.2393e-03],\n",
              "          [5.6399e-12, 7.8883e-04, 1.3978e-03,  ..., 4.9651e-01,\n",
              "           2.0184e-05, 6.3696e-08],\n",
              "          [2.0765e-03, 4.5171e-05, 1.3690e-04,  ..., 6.3913e-04,\n",
              "           1.2479e-04, 1.0902e-05],\n",
              "          ...,\n",
              "          [1.0810e-04, 2.7478e-05, 1.9901e-06,  ..., 4.1556e-02,\n",
              "           2.3341e-02, 1.0746e-05],\n",
              "          [1.0234e-08, 1.3054e-02, 2.1468e-05,  ..., 9.7970e-03,\n",
              "           5.9093e-05, 2.6103e-07],\n",
              "          [1.7722e-04, 1.6422e-06, 5.1737e-05,  ..., 4.7540e-03,\n",
              "           1.3903e-03, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 4.8125e-03, 9.6156e-06,  ..., 1.0299e-02,\n",
              "           1.1371e-03, 1.3092e-04],\n",
              "          [6.9203e-08, 1.4490e-07, 6.8819e-08,  ..., 1.1127e-03,\n",
              "           1.8616e-04, 1.2944e-04],\n",
              "          [5.8871e-05, 8.0813e-02, 5.7844e-05,  ..., 6.4695e-06,\n",
              "           1.5483e-03, 8.0587e-05],\n",
              "          ...,\n",
              "          [6.0139e-01, 5.2067e-01, 1.3407e-06,  ..., 1.1662e-09,\n",
              "           4.4373e-03, 1.8367e-02],\n",
              "          [2.3644e-01, 3.4332e-04, 9.9705e-01,  ..., 6.5917e-07,\n",
              "           1.4739e-02, 3.7597e-04],\n",
              "          [9.8438e-03, 7.3208e-06, 2.0188e-05,  ..., 3.2553e-02,\n",
              "           3.8583e-06, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.4234e-03, 2.2441e-03,  ..., 1.1613e-01,\n",
              "           1.2482e-01, 4.2618e-04],\n",
              "          [1.2970e-03, 1.0651e-04, 3.3717e-04,  ..., 7.3622e-05,\n",
              "           1.3819e-01, 3.0935e-05],\n",
              "          [2.3540e-03, 7.9368e-02, 8.0376e-03,  ..., 4.5255e-03,\n",
              "           2.2768e-02, 1.8797e-07],\n",
              "          ...,\n",
              "          [1.9115e-05, 2.6601e-03, 1.8111e-02,  ..., 1.1706e-03,\n",
              "           1.3556e-04, 4.4903e-07],\n",
              "          [9.6572e-07, 7.2776e-03, 3.2214e-03,  ..., 1.6921e-02,\n",
              "           6.2594e-03, 2.0521e-05],\n",
              "          [2.0632e-05, 3.6950e-04, 2.1194e-05,  ..., 2.4671e-04,\n",
              "           6.3394e-03, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 9.9943e-01, 7.3903e-05,  ..., 3.6373e-06,\n",
              "           2.0599e-02, 9.4030e-01],\n",
              "          [8.8486e-04, 3.1861e-08, 3.1606e-03,  ..., 2.9076e-06,\n",
              "           7.0629e-01, 2.0348e-04],\n",
              "          [2.6322e-01, 7.8598e-06, 2.5298e-04,  ..., 2.9262e-03,\n",
              "           1.9459e-05, 2.1226e-03],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 7.1575e-05, 5.9149e-03,  ..., 3.4783e-04,\n",
              "           3.0966e-04, 7.3722e-05],\n",
              "          [4.4262e-05, 4.3747e-03, 5.1039e-05,  ..., 9.5951e-01,\n",
              "           8.0478e-05, 9.7059e-01],\n",
              "          [1.8274e-03, 7.2610e-07, 9.7449e-03,  ..., 2.6915e-06,\n",
              "           3.4847e-02, 2.0260e-05],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]),\n",
              " tensor([[[8.5095e-02, 5.0265e-05, 2.6409e-02,  ..., 1.8632e-04,\n",
              "           4.1884e-05, 1.4554e-03],\n",
              "          [2.2856e-05, 1.4698e-05, 9.5063e-06,  ..., 5.5794e-05,\n",
              "           4.6370e-02, 1.4716e-04],\n",
              "          [4.1080e-06, 6.0563e-06, 7.6066e-05,  ..., 1.3970e-06,\n",
              "           2.7809e-03, 4.4123e-04],\n",
              "          ...,\n",
              "          [3.5946e-03, 9.9608e-01, 8.2803e-01,  ..., 3.7625e-02,\n",
              "           2.7350e-07, 2.6474e-03],\n",
              "          [7.6878e-07, 1.6724e-07, 7.1906e-04,  ..., 7.1448e-01,\n",
              "           7.1609e-05, 8.6639e-10],\n",
              "          [6.6794e-05, 9.3445e-06, 3.6707e-03,  ..., 1.9264e-02,\n",
              "           1.4910e-02, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 7.2275e-05, 8.3620e-06,  ..., 7.2220e-02,\n",
              "           2.6008e-04, 2.2393e-03],\n",
              "          [5.6399e-12, 7.8883e-04, 1.3978e-03,  ..., 4.9651e-01,\n",
              "           2.0184e-05, 6.3696e-08],\n",
              "          [2.0765e-03, 4.5171e-05, 1.3690e-04,  ..., 6.3913e-04,\n",
              "           1.2479e-04, 1.0902e-05],\n",
              "          ...,\n",
              "          [1.0810e-04, 2.7478e-05, 1.9901e-06,  ..., 4.1556e-02,\n",
              "           2.3341e-02, 1.0746e-05],\n",
              "          [1.0234e-08, 1.3054e-02, 2.1468e-05,  ..., 9.7970e-03,\n",
              "           5.9093e-05, 2.6103e-07],\n",
              "          [1.7722e-04, 1.6422e-06, 5.1737e-05,  ..., 4.7540e-03,\n",
              "           1.3903e-03, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 4.8125e-03, 9.6156e-06,  ..., 1.0299e-02,\n",
              "           1.1371e-03, 1.3092e-04],\n",
              "          [6.9203e-08, 1.4490e-07, 6.8819e-08,  ..., 1.1127e-03,\n",
              "           1.8616e-04, 1.2944e-04],\n",
              "          [5.8871e-05, 8.0813e-02, 5.7844e-05,  ..., 6.4695e-06,\n",
              "           1.5483e-03, 8.0587e-05],\n",
              "          ...,\n",
              "          [6.0139e-01, 5.2067e-01, 1.3407e-06,  ..., 1.1662e-09,\n",
              "           4.4373e-03, 1.8367e-02],\n",
              "          [2.3644e-01, 3.4332e-04, 9.9705e-01,  ..., 6.5917e-07,\n",
              "           1.4739e-02, 3.7597e-04],\n",
              "          [9.8438e-03, 7.3208e-06, 2.0188e-05,  ..., 3.2553e-02,\n",
              "           3.8583e-06, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.4234e-03, 2.2441e-03,  ..., 1.1613e-01,\n",
              "           1.2482e-01, 4.2618e-04],\n",
              "          [1.2970e-03, 1.0651e-04, 3.3717e-04,  ..., 7.3622e-05,\n",
              "           1.3819e-01, 3.0935e-05],\n",
              "          [2.3540e-03, 7.9368e-02, 8.0376e-03,  ..., 4.5255e-03,\n",
              "           2.2768e-02, 1.8797e-07],\n",
              "          ...,\n",
              "          [1.9115e-05, 2.6601e-03, 1.8111e-02,  ..., 1.1706e-03,\n",
              "           1.3556e-04, 4.4903e-07],\n",
              "          [9.6572e-07, 7.2776e-03, 3.2214e-03,  ..., 1.6921e-02,\n",
              "           6.2594e-03, 2.0521e-05],\n",
              "          [2.0632e-05, 3.6950e-04, 2.1194e-05,  ..., 2.4671e-04,\n",
              "           6.3394e-03, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 9.9943e-01, 7.3903e-05,  ..., 3.6373e-06,\n",
              "           2.0599e-02, 9.4030e-01],\n",
              "          [8.8486e-04, 3.1861e-08, 3.1606e-03,  ..., 2.9076e-06,\n",
              "           7.0629e-01, 2.0348e-04],\n",
              "          [2.6322e-01, 7.8598e-06, 2.5298e-04,  ..., 2.9262e-03,\n",
              "           1.9459e-05, 2.1226e-03],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 7.1575e-05, 5.9149e-03,  ..., 3.4783e-04,\n",
              "           3.0966e-04, 7.3722e-05],\n",
              "          [4.4262e-05, 4.3747e-03, 5.1039e-05,  ..., 9.5951e-01,\n",
              "           8.0478e-05, 9.7059e-01],\n",
              "          [1.8274e-03, 7.2610e-07, 9.7449e-03,  ..., 2.6915e-06,\n",
              "           3.4847e-02, 2.0260e-05],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def get_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                         If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Ts, Tt]\n",
        "\n",
        "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: You can give -infinity value by -float(\"inf\")\n",
        "\n",
        "  '''\n",
        "  mask = mask.unsqueeze(-1).expand(-1, -1, attention_score.size()[2])\n",
        "  attention_score_masked = attention_score.masked_fill(mask == 0, -float(\"inf\"))\n",
        "  attention_weight = torch.softmax(attention_score_masked, dim=1)\n",
        "\n",
        "  return attention_weight\n",
        "\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "mask = torch.ones_like(att_score)[..., 0]\n",
        "mask[4, 15:] = 0\n",
        "mask[5, 17:] = 0\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[4, 15:] = 0\n",
        "attention_weight = get_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99abd97a",
      "metadata": {
        "id": "99abd97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d778520c-eb76-4711-ff30-2a93886f80c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "answer = torch.Tensor([0.0120,     0.0002,     0.0901,     0.0003,     0.0259,     0.0036,\n",
        "            0.5617,     0.0108,     0.2508,     0.0054,     0.0001,     0.0010,\n",
        "            0.0000,     0.0005,     0.0375,     0.0000,     0.0000,     0.0000,\n",
        "            0.0000,     0.0000,     0.0000,     0.0000,     0.0000])\n",
        "assert torch.allclose(attention_weight[4,:,3], answer, atol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(attention_weight.sum(1),  torch.tensor([1.0]) , atol=1e-6 ), 'Sum of attention weight has to be 1'\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified), \"Output is different even though only masked part is different\"\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2030f5",
      "metadata": {
        "id": "ca2030f5"
      },
      "source": [
        "### 2-4 Implement weighted sum in batchified version (4 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2e5c36",
      "metadata": {
        "id": "9b2e5c36",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce332f9-d29e-4282-a492-43e52d270e06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.3373e-01, -2.0938e+00, -4.5334e-02,  ...,  9.5458e-01,\n",
              "           1.0229e+00,  1.1447e+00],\n",
              "         [-1.3264e-01, -5.9684e-02,  4.0668e-01,  ..., -2.2277e+00,\n",
              "           1.4654e+00, -1.2140e+00],\n",
              "         [-2.1483e-01, -3.9624e-02,  3.4638e-01,  ..., -1.8784e+00,\n",
              "           1.3381e+00, -1.0378e+00],\n",
              "         ...,\n",
              "         [ 4.7269e-01,  2.7082e+00, -1.5966e-01,  ..., -8.5101e-01,\n",
              "           1.6295e+00, -6.9686e-01],\n",
              "         [ 1.3049e+00, -7.9595e-01,  5.8837e-01,  ..., -4.2750e-01,\n",
              "           9.4185e-02, -6.0422e-01],\n",
              "         [-2.1447e-01,  4.5967e-01,  9.0154e-01,  ..., -1.1010e+00,\n",
              "           1.5383e+00, -6.2885e-01]],\n",
              "\n",
              "        [[-2.0093e-02,  9.5785e-01, -1.0546e+00,  ..., -5.6727e-01,\n",
              "           9.5331e-01, -1.4754e+00],\n",
              "         [ 2.6579e-01, -9.2264e-01,  8.4929e-01,  ...,  2.4586e+00,\n",
              "          -2.5894e+00,  2.3314e+00],\n",
              "         [ 5.3756e-01, -9.0356e-01,  1.7743e-01,  ...,  2.2658e+00,\n",
              "           3.0777e-01,  2.0951e+00],\n",
              "         ...,\n",
              "         [ 1.3244e-01,  8.5970e-01,  2.4145e-01,  ..., -1.7893e-01,\n",
              "           9.2332e-01, -6.8233e-01],\n",
              "         [-8.8633e-02,  5.1850e-01, -1.4503e+00,  ..., -5.1185e-01,\n",
              "           5.6031e-01,  8.2076e-01],\n",
              "         [-1.1267e-01, -5.4648e-01, -4.8170e-02,  ...,  1.8755e+00,\n",
              "          -3.0700e-01,  9.4322e-01]],\n",
              "\n",
              "        [[-6.1817e-01,  2.6989e-01, -1.7159e+00,  ..., -1.2042e+00,\n",
              "          -1.3739e+00,  6.5000e-01],\n",
              "         [ 8.7003e-01,  8.7786e-02, -1.3764e+00,  ..., -1.0438e+00,\n",
              "          -1.3338e-01,  4.9795e-01],\n",
              "         [-5.9019e-01,  4.2505e-01, -1.2334e+00,  ..., -7.7825e-01,\n",
              "          -2.1349e+00,  3.3232e-01],\n",
              "         ...,\n",
              "         [-4.5776e-01,  5.3027e-01,  8.0297e-01,  ...,  1.1601e+00,\n",
              "          -4.1216e-01, -8.4225e-01],\n",
              "         [-2.7609e-01,  5.8881e-01,  5.0533e-01,  ..., -5.1114e-01,\n",
              "           1.7841e+00, -5.5431e-01],\n",
              "         [ 2.9698e-02, -1.3222e+00, -7.9683e-01,  ...,  3.8490e-01,\n",
              "          -1.0175e+00,  1.4906e-01]],\n",
              "\n",
              "        [[ 8.3192e-01,  1.6496e-01, -9.2035e-01,  ..., -1.6308e+00,\n",
              "          -5.2375e-01, -1.1106e+00],\n",
              "         [-7.9787e-01, -3.0595e-01, -9.7095e-01,  ...,  2.6117e-03,\n",
              "           2.1266e-01,  7.9024e-01],\n",
              "         [ 3.6286e-01,  6.3939e-01,  3.0618e-01,  ...,  1.3938e+00,\n",
              "           2.0934e+00,  1.1223e+00],\n",
              "         ...,\n",
              "         [ 4.2950e-01,  2.7868e-01, -6.7143e-01,  ..., -4.3766e-01,\n",
              "          -7.7644e-01, -2.6162e-01],\n",
              "         [-8.4428e-01, -5.4181e-02, -1.7641e-01,  ...,  1.1040e-01,\n",
              "           1.6086e-01,  7.0424e-01],\n",
              "         [ 1.2658e+00,  1.9420e+00, -1.2539e-01,  ...,  1.0030e+00,\n",
              "          -1.3475e-01, -5.0187e-01]],\n",
              "\n",
              "        [[ 5.8191e-03, -5.3975e-01, -3.6032e-01,  ...,  5.1161e-01,\n",
              "           1.4710e-01,  1.0211e+00],\n",
              "         [ 3.7232e+00,  1.7317e+00, -7.4160e-01,  ..., -4.5058e-01,\n",
              "          -2.1989e+00,  1.4684e+00],\n",
              "         [-7.7349e-01,  1.1662e+00, -1.5678e+00,  ..., -5.2115e-01,\n",
              "           1.4653e+00, -1.6642e+00],\n",
              "         ...,\n",
              "         [-4.5703e-01,  1.6784e+00, -6.8822e-02,  ..., -3.6136e-01,\n",
              "          -6.9498e-01, -6.5073e-01],\n",
              "         [-1.2242e+00,  8.1537e-01,  1.2802e+00,  ...,  2.8726e-01,\n",
              "          -6.7655e-01,  9.0936e-01],\n",
              "         [ 3.4806e+00,  1.7345e+00, -6.9836e-01,  ..., -4.4237e-01,\n",
              "          -2.1103e+00,  1.3477e+00]],\n",
              "\n",
              "        [[ 7.4298e-01,  6.8085e-01, -4.0109e-01,  ..., -6.9539e-01,\n",
              "          -7.1683e-01,  1.3663e-01],\n",
              "         [-1.7923e-01, -4.7257e-01, -4.0078e-01,  ..., -8.6471e-01,\n",
              "           1.2872e-01, -2.4034e-02],\n",
              "         [-3.2743e-01, -1.8710e-01,  6.0103e-02,  ...,  6.2887e-01,\n",
              "          -8.4385e-02,  4.1515e-01],\n",
              "         ...,\n",
              "         [-2.2279e-01,  2.0214e-01, -1.1098e+00,  ..., -2.4794e+00,\n",
              "          -1.3261e+00,  1.8497e+00],\n",
              "         [-4.9566e-01, -1.3428e+00,  4.1454e-01,  ..., -7.3784e-01,\n",
              "           3.7670e-01, -1.1959e-01],\n",
              "         [-2.2121e-01,  1.5855e-01, -1.1392e+00,  ..., -2.5089e+00,\n",
              "          -1.3221e+00,  1.8876e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "def get_batch_weighted_sum(values, attention_weight):\n",
        "  '''\n",
        "  This function converts attention score to attention weight\n",
        "\n",
        "  Argument:\n",
        "    values (torch.Tensor): Has a shape of [N, Ts, C]. These are vectors that are used to form attention vector\n",
        "    attention_weight: Has a shape of [N, Ts, Tt], which represents the weight for each vector to compose the attention vector\n",
        "                      attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
        "\n",
        "  Output:\n",
        "    attention_vector (torch.Tensor): Weighted sum of values using the attention weight.\n",
        "                                     Has a shape of [N, Tt, C]\n",
        "\n",
        "  TODO: Complete this function using torch.bmm\n",
        "  '''\n",
        "  attention_weight = attention_weight.transpose(1, 2)\n",
        "  attention_vector = torch.bmm(attention_weight, values)\n",
        "\n",
        "  return attention_vector\n",
        "\n",
        "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
        "att_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93231ea",
      "metadata": {
        "id": "c93231ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ba6486-1e68-4815-b84a-498b3e715b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.9348, -1.2628, -0.9189, -0.3434, -1.6476,  0.1031, -0.6963, -0.7462,\n",
        "         0.1484,  0.6810,  0.7950,  1.0277, -1.5988,  0.4232, -1.5540,  0.1801])\n",
        "answer2 = torch.Tensor([-0.9204, -0.9710,  0.3062, -1.0122,  1.1933,  0.1302, -1.0280,  0.0095,\n",
        "         0.6124,  0.0615, -1.2312, -0.6714, -0.1764, -0.1254])\n",
        "assert att_out.ndim == 3 and att_out.shape == torch.Size([num_b, num_tt, h_size]), 'Check the output shape'\n",
        "assert torch.max(torch.abs(att_out[2, 5] - answer)) < 1e-4, 'Calculated result is wrong'\n",
        "assert torch.max(torch.abs(att_out[3,:,2] - answer2)) < 1e-4,  'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebbdbf2b",
      "metadata": {
        "id": "ebbdbf2b"
      },
      "source": [
        "## Problem 3: Make seq2seq with attention (14 pts)\n",
        "- Using Pre-defined `TranslatorBi` class, complete a new `TranslatorAtt` class\n",
        "- If you implement it correctly, you can translate a sentence and get the corresponding attention map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904d44a3",
      "metadata": {
        "id": "904d44a3"
      },
      "source": [
        "### 3-0 Prepare dataset and tokenizer\n",
        "- To use the pretrained model correctly, you can use the pretrained vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c670709",
      "metadata": {
        "id": "9c670709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4434890e-8c77-4619-accc-0e698702f564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
            "From (redirected): https://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c&confirm=t&uuid=389fd312-6c7b-49c2-80c5-6cc4371f4b5b\n",
            "To: /content/nia_korean_english_csv.zip\n",
            "100% 190M/190M [00:01<00:00, 99.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Download dataset (originally from NIA AI-Hub)\n",
        "'''\n",
        "\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
        "!unzip -q nia_korean_english_csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4edd48e",
      "metadata": {
        "id": "d4edd48e"
      },
      "outputs": [],
      "source": [
        "# Load data and tokenizer\n",
        "\n",
        "df = pd.read_csv('nia_korean_english.csv')\n",
        "\n",
        "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n",
        "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                       strip_accents=False,\n",
        "                                                       lowercase=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8fa529",
      "metadata": {
        "id": "ab8fa529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7444d978-87cd-4066-9cf1-d719e12cfe71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Item Example: (tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3]), tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18]), tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3]))\n",
            "Length of split : Train 1442176, Valid 80120, Test 80122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PackedSequence(data=tensor([   2,    2,    2,  ..., 7076,   18,    3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 63, 60, 57, 55, 52, 50, 48, 42, 38, 36,\n",
              "         35, 34, 33, 33, 31, 31, 31, 31, 29, 26, 23, 21, 18, 18, 17, 15, 14, 13,\n",
              "         11, 10,  8,  8,  7,  6,  6,  6,  6,  5,  4,  3,  3,  3,  2,  2,  1,  1,\n",
              "          1,  1]), sorted_indices=tensor([55, 42, 60, 26, 37, 29, 20,  7,  1, 33,  6,  9, 56, 23, 47, 57,  0, 49,\n",
              "         15, 41, 24, 38, 14, 18, 46,  4, 13, 44, 61, 50, 40,  2, 52, 21, 39, 51,\n",
              "         10, 53, 63, 31, 19, 22,  3, 36,  8, 11, 12, 34, 62,  5, 43, 59, 45, 17,\n",
              "         54, 30, 32, 28, 25, 16, 35, 58, 27, 48]), unsorted_indices=tensor([16,  8, 31, 42, 25, 49, 10,  7, 44, 11, 36, 45, 46, 26, 22, 18, 59, 53,\n",
              "         23, 40,  6, 33, 41, 13, 20, 58,  3, 62, 57,  5, 55, 39, 56,  9, 47, 60,\n",
              "         43,  4, 21, 34, 30, 19,  1, 50, 27, 52, 24, 14, 63, 17, 29, 35, 32, 37,\n",
              "         54,  0, 12, 15, 61, 51,  2, 28, 48, 38])),\n",
              " PackedSequence(data=tensor([   2,    2,    2,  ...,   18, 1913,   18]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "         41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "         19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "          4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "         41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "         63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "         25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "          8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "         51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "         44,  1, 23, 14, 60, 53,  6, 22, 43, 36])),\n",
              " PackedSequence(data=tensor([16406,  1023, 26808,  ...,     3,    18,     3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "         41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "         19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "          4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "          2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "         41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "         63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "         25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "          8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "         51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "         44,  1, 23, 14, 60, 53,  6, 22, 43, 36])))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "class TranslationSet:\n",
        "  def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "    self.data = df\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    selected_row = self.data.iloc[idx]\n",
        "    source = selected_row['원문']\n",
        "    target = selected_row['번역문']\n",
        "\n",
        "    source_enc = self.src_tokenizer(source)['input_ids']\n",
        "    target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "    return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "\n",
        "entireset = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
        "trainset, validset, testset = torch.utils.data.random_split(entireset, [int(len(entireset)*0.9), int(len(entireset)*0.05), len(entireset)-int(len(entireset)*0.9)-int(len(entireset)*0.05)], generator=torch.Generator().manual_seed(42))\n",
        "# trainset, validset, testset = torch.utils.data.random_split(entireset, [360000, 20000, 20000], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f'Dataset Item Example: {entireset[0]}')\n",
        "print(f'Length of split : Train {len(trainset)}, Valid {len(validset)}, Test {len(testset)}')\n",
        "\n",
        "def pack_collate(raw_batch):\n",
        "  source, target, shifted_target = zip(*raw_batch)\n",
        "  return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a254bfb6",
      "metadata": {
        "id": "a254bfb6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "'''\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = loss_fn\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "\n",
        "    self.model.to(device)\n",
        "\n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "\n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='kor_eng_translator_attention_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "\n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      with tqdm(self.train_loader, leave=False) as pbar:\n",
        "        for batch in pbar:\n",
        "          loss_value = self._train_by_single_batch(batch)\n",
        "          self.training_loss.append(loss_value)\n",
        "          pbar.set_description(f\"Epoch {epoch+1}, Loss {loss_value:.4f}\")\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "\n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('kor_eng_translator_attention_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('kor_eng_translator_attention_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "    src, tgt, shifted_tgt = batch\n",
        "    src = src.to(self.device)\n",
        "    tgt = tgt.to(self.device)\n",
        "    shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "    prob = self.model(src, tgt)\n",
        "\n",
        "    if isinstance(prob, PackedSequence):\n",
        "      loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "    else:\n",
        "      loss = self.loss_fn(prob, shifted_tgt)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    '''\n",
        "\n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt, shifted_tgt = batch\n",
        "        src = src.to(self.device)\n",
        "        tgt = tgt.to(self.device)\n",
        "        shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "        prob = self.model(src, tgt)\n",
        "\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          loss = self.loss_fn(prob.data, shifted_tgt.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(prob, shifted_tgt)\n",
        "\n",
        "        validation_loss += loss.item() * len(prob.data)\n",
        "        if isinstance(prob, PackedSequence):\n",
        "          num_correct_guess += (prob.data.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (prob.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "        num_data += len(prob.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "\n",
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  '''\n",
        "  for PackedSequence, the input is 2D tensor\n",
        "\n",
        "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
        "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
        "  '''\n",
        "\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11a011e3",
      "metadata": {
        "id": "11a011e3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "\n",
        "You don't need to change this code\n",
        "'''\n",
        "class TranslatorBi(nn.Module):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
        "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
        "\n",
        "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
        "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
        "\n",
        "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
        "\n",
        "  def run_encoder(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb_x = self.src_embedder(x)\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
        "\n",
        "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
        "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
        "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
        "    if isinstance(x, PackedSequence):\n",
        "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
        "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
        "    else:\n",
        "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
        "\n",
        "\n",
        "    return enc_hidden_state_by_t, last_hidden_sum\n",
        "\n",
        "  def run_decoder(self, y, last_hidden_state):\n",
        "    if isinstance(y, PackedSequence):\n",
        "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      emb_y = self.tgt_embedder(y)\n",
        "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
        "    return out, decoder_last_hidden\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    '''\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if isinstance(out, PackedSequence):\n",
        "      logits = self.decoder_proj(out.data)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      logits = self.decoder_proj(out)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2060d164",
      "metadata": {
        "id": "2060d164"
      },
      "source": [
        "### Problem 3.1: Complete the Seq2Seq with Attention (8 pts)\n",
        "- **Caution**: You have to concatenate [decoder_hidden_state; attention_out] for this implementation\n",
        "    - You can use different order of concatenation, but the pre-trained model used that specific order, so please follow it so that you can use the pre-trained weight correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e285f77",
      "metadata": {
        "id": "9e285f77",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69323d1-1152-4fcf-ef32-db722de7c15c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PackedSequence(data=tensor([[3.2341e-05, 3.1183e-05, 3.4485e-05,  ..., 3.0108e-05, 2.4773e-05,\n",
              "         3.5819e-05],\n",
              "        [3.2799e-05, 3.0913e-05, 3.3578e-05,  ..., 2.9381e-05, 2.5592e-05,\n",
              "         3.6779e-05],\n",
              "        [3.2543e-05, 3.0949e-05, 3.3119e-05,  ..., 3.0053e-05, 2.5236e-05,\n",
              "         3.7833e-05],\n",
              "        ...,\n",
              "        [3.2339e-05, 2.7429e-05, 3.2675e-05,  ..., 3.2805e-05, 2.4759e-05,\n",
              "         3.0996e-05],\n",
              "        [3.4378e-05, 2.8730e-05, 3.7218e-05,  ..., 2.9706e-05, 2.7278e-05,\n",
              "         3.1409e-05],\n",
              "        [3.4210e-05, 2.8178e-05, 3.6602e-05,  ..., 3.0610e-05, 2.6272e-05,\n",
              "         3.0359e-05]], grad_fn=<SoftmaxBackward0>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 62, 61, 58, 55, 54, 52, 49, 48, 45, 43,\n",
              "        41, 38, 35, 33, 33, 33, 33, 32, 32, 31, 30, 30, 30, 29, 28, 27, 24, 23,\n",
              "        19, 18, 17, 16, 14, 13, 11, 11, 10, 10,  9,  7,  7,  5,  5,  5,  5,  4,\n",
              "         4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
              "         2,  2,  2,  2,  2,  2,  2,  2,  1,  1]), sorted_indices=tensor([42, 55, 26,  7,  1, 37, 60, 49, 18,  0, 33, 23, 29, 24, 57,  6, 46,  9,\n",
              "        41, 40, 38, 47, 61, 56, 50, 20, 15,  4, 14, 44, 13, 52, 21,  2, 11, 53,\n",
              "        63, 10, 39,  5, 31, 34,  8, 62, 54, 45, 51, 22,  3, 28, 43, 36, 19, 59,\n",
              "        25, 16, 12, 35, 48, 30, 58, 17, 27, 32]), unsorted_indices=tensor([ 9,  4, 33, 48, 27, 39, 15,  3, 42, 17, 37, 34, 56, 30, 28, 26, 55, 61,\n",
              "         8, 52, 25, 32, 47, 11, 13, 54,  2, 62, 49, 12, 59, 40, 63, 10, 41, 57,\n",
              "        51,  5, 20, 38, 19, 18,  0, 50, 29, 45, 16, 21, 58,  7, 24, 46, 31, 35,\n",
              "        44,  1, 23, 14, 60, 53,  6, 22, 43, 36]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "class TranslatorAtt(TranslatorBi):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
        "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
        "\n",
        "    # define new self.decoder_proj\n",
        "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
        "\n",
        "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
        "    '''\n",
        "    Arguments:\n",
        "      encoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of encoder GRU. Shape: [N, Ts, C]\n",
        "      decoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of decoder GRU. Shape: [N, Tt, C]\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ts]\n",
        "\n",
        "    Outputs:\n",
        "      attention_vectors (torch.Tensor or PackedSequence): Attention vectors that has the same shape as decoder_hidden_states\n",
        "      attention_weights (torch.Tensor): Zero-padded attention weights.\n",
        "                                You don't need to return it during the training, but it will help you to implement later problem\n",
        "\n",
        "    TODO: Complete this function using following functions\n",
        "      get_attention_score_for_a_batch_multiple_query\n",
        "      get_masked_softmax\n",
        "      get_batch_weighted_sum\n",
        "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
        "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True) to convert PackedSequence to Tensor\n",
        "    Use torch.nn.utils.rnn.pack_padded_sequence(tensor, batch_lens, batch_first=True) to convert Tensor to PackedSequence\n",
        "    '''\n",
        "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
        "    if is_packed:\n",
        "      encoder_hidden_states, source_lens = pad_packed_sequence(encoder_hidden_states, batch_first=True)\n",
        "      decoder_hidden_states, target_lens = pad_packed_sequence(decoder_hidden_states, batch_first=True)\n",
        "\n",
        "    # Write your code from here\n",
        "\n",
        "    # 1. Calculate attention score using encoder_hidden_states and decoder_hidden_states\n",
        "    # 2. Mask the attention score using mask and apply softmax to get attention weight\n",
        "    # 3. Calculate attention vector using attention weight and encoder_hidden_states\n",
        "\n",
        "\n",
        "    attention_scores = get_attention_score_for_a_batch_multiple_query(encoder_hidden_states, decoder_hidden_states)\n",
        "    attention_weights = get_masked_softmax(attention_scores, mask)\n",
        "    attention_vectors = get_batch_weighted_sum(encoder_hidden_states, attention_weights)\n",
        "\n",
        "    if is_packed:\n",
        "      attention_vectors = pack_padded_sequence(attention_vectors, target_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    return attention_vectors, attention_weights\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    Arguments:\n",
        "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    Output:\n",
        "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    is_packed = isinstance(x, PackedSequence)\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if is_packed:\n",
        "      mask = pad_packed_sequence(x, batch_first=True)[0] != 0\n",
        "    else:\n",
        "      mask = torch.ones(x.shape[0], x.shape[1])\n",
        "\n",
        "    attention_vec, attention_weight = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
        "\n",
        "    # TODO: Write your code from here\n",
        "    # CAUTION:\n",
        "    #   For the concatenation, you have to concat [dec_hidden_state_by_t; attention_vec], not [attention_vec; dec_hidden_state_by_t]\n",
        "\n",
        "    if is_packed:\n",
        "        dec_hidden_state_by_t, _ = pad_packed_sequence(dec_hidden_state_by_t, batch_first=True)\n",
        "        attention_vec, _ = pad_packed_sequence(attention_vec, batch_first=True)\n",
        "        y_padded, y_len = pad_packed_sequence(y, batch_first=True)\n",
        "        concatenated_hidden_state = torch.cat([dec_hidden_state_by_t, attention_vec], dim=-1)\n",
        "        concatenated_hidden_state = pack_padded_sequence(concatenated_hidden_state, y_len, batch_first=True, enforce_sorted = False)\n",
        "        logits = self.decoder_proj(concatenated_hidden_state.data)\n",
        "        probs=torch.softmax(logits, dim=-1)\n",
        "        probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "\n",
        "    else:\n",
        "        concatenated_hidden_state = torch.cat([dec_hidden_state_by_t, attention_vec], dim=-1)\n",
        "        batch_size, seq_len, _ = concatenated_hidden_state.shape\n",
        "        concatenated_hidden_state = concatenated_hidden_state.view(batch_size * seq_len, -1)\n",
        "        logits = self.decoder_proj(concatenated_hidden_state)\n",
        "        probs=torch.softmax(logits, dim=-1)\n",
        "        probs = probs.view(batch_size, seq_len, -1)\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
        "\n",
        "model(batch[0], batch[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3212344b",
      "metadata": {
        "id": "3212344b"
      },
      "source": [
        "#### Test your model\n",
        "- To evaluate your implementation, you have to load the pretrained weight of the same model.\n",
        "- If your implementation is correct, the resulting value would be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d4cba9e",
      "metadata": {
        "id": "2d4cba9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f18ff80-38d0-4a76-ea53-740efa2ca08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4\n",
            "From (redirected): https://drive.google.com/uc?id=1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4&confirm=t&uuid=1bd2f917-f028-4f4c-af89-a3a53afe5342\n",
            "To: /content/kor_eng_translator_attention_model_best.pt\n",
            "100% 995M/995M [00:04<00:00, 221MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3\n",
            "From (redirected): https://drive.google.com/uc?id=1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3&confirm=t&uuid=50b2f5e3-f2c8-49c6-9fbc-611993473908\n",
            "To: /content/assignment4_values.pt\n",
            "100% 267M/267M [00:01<00:00, 161MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download pre-trained model and tensor data\n",
        "!gdown 1IpqJ6U1fuIUf1b8fuwHPSnp7pMU6Qzl4\n",
        "!gdown 1ch3LdtNprU5QBteP_UJLHaBP9jasUbc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b291ec1",
      "metadata": {
        "id": "3b291ec1"
      },
      "outputs": [],
      "source": [
        "# Load pretrained weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.eval()\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Load the pre-calculated example and result\n",
        "prob3_values = torch.load('assignment4_values.pt')\n",
        "single_batch_example, packed_batch_example, correct_single_out, correct_packed_out = prob3_values['single_test_batch'], prob3_values['packed_test_batch'], prob3_values['correct_single_out'],  prob3_values['correct_packed_out']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6051c11f",
      "metadata": {
        "id": "6051c11f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case for Single-size Batch\n",
        "'''\n",
        "single_out = model(single_batch_example[0], single_batch_example[1])\n",
        "\n",
        "assert isinstance(single_out, torch.Tensor), \"The output of model for Tensor has to be Tensor\"\n",
        "assert torch.allclose(single_out, correct_single_out, atol=1e-4), \"The output value is different from the expected\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8027b42c",
      "metadata": {
        "id": "8027b42c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test Case for Batch with PackedSequence\n",
        "'''\n",
        "packed_out = model(packed_batch_example[0], packed_batch_example[1])\n",
        "\n",
        "assert isinstance(packed_out, PackedSequence), \"The output of model for PackedSequence has to be PackedSequence\"\n",
        "assert (packed_out.batch_sizes == correct_packed_out.batch_sizes).all(), \"Output's batch_sizes is wrong\"\n",
        "assert (packed_out.sorted_indices == correct_packed_out.sorted_indices).all(), \"Output's sorted_indices is wrong\"\n",
        "\n",
        "# print(packed_out)\n",
        "# print(correct_packed_out)\n",
        "\n",
        "assert torch.allclose(packed_out.data, correct_packed_out.data, atol=1e-4),  \"The output value is different from the expected\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e0ab23",
      "metadata": {
        "id": "12e0ab23"
      },
      "source": [
        "### Train the model (Optional)\n",
        "- You can try to train your model, but you can just load the pretrained data\n",
        "- You don't have to train the model yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d4bf8b",
      "metadata": {
        "id": "75d4bf8b"
      },
      "outputs": [],
      "source": [
        "# model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# trainer = Trainer(model, optimizer, nll_loss, train_loader, valid_loader, 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "najCuqk2WIbQ",
      "metadata": {
        "id": "najCuqk2WIbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ed8207-a650-420c-ac61-b31e6aaaf6fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TranslatorAtt(\n",
              "  (src_embedder): Embedding(32000, 512)\n",
              "  (tgt_embedder): Embedding(32000, 512)\n",
              "  (encoder): GRU(512, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (decoder): GRU(512, 512, num_layers=3, batch_first=True)\n",
              "  (decoder_proj): Linear(in_features=1024, out_features=32000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Otherwise, you have to load the model weight\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, 512)\n",
        "state_dict = torch.load('kor_eng_translator_attention_model_best.pt', map_location='cpu')['model']\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea22364",
      "metadata": {
        "id": "cea22364"
      },
      "source": [
        "### Problem 3.2: Implement Inference with Attention Weights (6 pts)\n",
        "- In this problem, you have to implement an inference code that returns translation for given source sentence, but also **attention weights** between source sentence and target sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef0e100",
      "metadata": {
        "id": "1ef0e100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f464a0-cff0-4668-a6bd-8622992ad4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using this algorithm, you can see how korean words and english words are connected.\n"
          ]
        }
      ],
      "source": [
        "def translate(model, source_sentence):\n",
        "  '''\n",
        "  This function translates a given sentence using a given model.\n",
        "  It returns the tokenized source sentence, tokenized translated sentence, translated sentence in string, and attention map\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "    attention_map (torch.Tensor): Attention weight between each token of source sentence and target sentence. Has a shape of [Ts, Tt]\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = model.src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  mask = torch.ones_like(input_tensor)\n",
        "  enc_hidden_state_by_t, last_hidden_sum = model.run_encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_hidden = last_hidden_sum\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "  total_output = []\n",
        "  total_attetion_weights = []\n",
        "\n",
        "  for i in range(100): # You can chage it to while True:\n",
        "    emb = model.tgt_embedder(current_decoder_token)\n",
        "    '''\n",
        "    TODO: Complete the code here\n",
        "\n",
        "    You have to\n",
        "      1) run decoder rnn for a single step\n",
        "      2) get attention weight (variable name: att_weight) and attention vector.\n",
        "         att_weight.shape == torch.Size([1, len(tokenized_sentence), 1])\n",
        "      3) concat decoder out and attention vector\n",
        "      4) calculate probabilty logit (variable name: logit)\n",
        "    '''\n",
        "\n",
        "    decoder_out, last_hidden = model.decoder(emb, current_hidden)\n",
        "    att_vector, att_weight = model.get_attention_vector(enc_hidden_state_by_t, decoder_out, mask)\n",
        "    concatenated_hidden_state = torch.cat([decoder_out, att_vector], dim=-1)\n",
        "    logit = model.decoder_proj(concatenated_hidden_state)\n",
        "\n",
        "\n",
        "    # You don't have to change the codes below.\n",
        "    # Declare logit and last_hidden properly so that the code below can run without error\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = selected_token\n",
        "    current_hidden = last_hidden\n",
        "    if current_decoder_token == 3: ## end of sentence token\n",
        "      break\n",
        "    total_output.append(selected_token[0])\n",
        "    total_attetion_weights.append(att_weight[0,:,0])\n",
        "  predicted_tokens = torch.cat(total_output, dim=0).tolist()\n",
        "  attention_map = torch.stack(total_attetion_weights, dim=1)\n",
        "\n",
        "  return  input_tokens, predicted_tokens, model.tgt_tokenizer.decode(predicted_tokens), attention_map\n",
        "\n",
        "model.cpu()\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "print(translated_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491f62ca",
      "metadata": {
        "id": "491f62ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed2734e-cf24-4712-aebf-b5f0f2a5ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test Case\n",
        "'''\n",
        "test_sentence = '이 알고리즘을 사용하면 한국어 단어와 영어 단어가 어떻게 연결되는지를 알 수 있습니다.'\n",
        "input_tokens, pred_tokens, translated_string, att_weights  = translate(model, test_sentence)\n",
        "\n",
        "correct_output = 'using this algorithm, you can see how korean words and english words are connected.'\n",
        "answer = torch.tensor([1.6955e-07, 1.1118e-07, 6.0198e-10, 9.1661e-01, 7.8413e-18, 1.2012e-17,\n",
        "        7.6764e-29, 1.2549e-20, 5.7431e-19, 3.3563e-31, 1.1225e-21, 1.3192e-27,\n",
        "        3.6772e-26, 2.2244e-23, 3.6098e-20, 1.5309e-21, 7.5093e-05])\n",
        "answer2 = torch.tensor([1.1643e-11, 3.9906e-30, 1.2813e-33, 2.7519e-13, 2.3483e-15, 4.2758e-12,\n",
        "        1.9385e-18, 6.8541e-16, 4.9662e-18, 5.0304e-33, 7.2299e-26, 4.4580e-25,\n",
        "        3.7096e-23, 7.5614e-22, 4.5226e-22, 2.3576e-25, 1.7577e-12])\n",
        "answer3 = torch.tensor([1.2012e-17, 1.7528e-21, 1.1316e-18, 2.7204e-17, 8.2384e-10, 3.4510e-11,\n",
        "        1.8289e-09, 1.1806e-12, 3.9218e-19, 2.8321e-16, 1.3933e-12, 3.6876e-10,\n",
        "        4.1782e-07, 1.0905e-02, 9.8673e-01, 2.3651e-03, 1.7207e-08, 4.2758e-12])\n",
        "\n",
        "\n",
        "assert translated_string == correct_output, 'Translated sentence is wrong'\n",
        "assert att_weights.shape == torch.Size([18, 17]), 'Attention weight has wrong shape'\n",
        "assert torch.allclose(att_weights[0], answer, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[-1], answer2, rtol=1e-4), 'Calculated result is wrong'\n",
        "assert torch.allclose(att_weights[:,5], answer3, rtol=1e-4), 'Calculated result is wrong'\n",
        "\n",
        "print(\"Passed all the cases!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e466b4f4",
      "metadata": {
        "id": "e466b4f4"
      },
      "source": [
        "### Plot attention map\n",
        "- If you completed `translate()`, you can visualize the result of attention weight as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bf81f6",
      "metadata": {
        "id": "f1bf81f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c2b2900-9a47-4499-f8c8-c7e988cccae5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1360x1440 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIMAAATHCAYAAAB+wreNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADhiUlEQVR4nOzdeZhXdd0//ueHZQZRGBRBEEFBEdRM3HDDpVzQ0jRF7qSyxcr0a2VqGS51e1tR3eaS2W2mWd25ZO4tLmm3IqZpiluCgIAigrgxAwjDNr8/vJhf0wwwA8N8gPN4XNe5hnPe7/N+v85gXRfP633ep1RXV1cXAAAAAAqhXbkLAAAAAKDtCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAXSodwFkCxfvjyvv/56unTpklKpVO5yAAAAgA1MXV1d5s2bl6233jrt2q167Y8waD3w+uuvp2/fvuUuAwAAANjAzZgxI9tss80q+wiD1gNdunRJkgzLR9IhHctcDawbFzz/dLlLKIvv7rpHuUsAAAAKYGmWZFz+XJ8xrIowaD2w4tWwDumYDiVhEBunTbsUc4sy/5sGAADaRN37P5qz/Uwx/3UGAAAAUFDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABRIh3IX0ByLFi3K0qVLkySVlZXp2LFjmStqbMmSJamtrU2StGvXLp07dy5zRQAAAACNbRArgzp37pwuXbqkS5cuOfXUU8tdTpNOPfXU+ho322yzcpcDAAAA0KQNIgyqq6vLueeem0ceeSQXXnhhuctp0oUXXphHHnkk5557burq6spdDgAAAECTNogwKEkGDx6cYcOGpX///ivt88c//jGf/OQnM2DAgGy66aYplUrZbLPNMmTIkPzhD3+o73fjjTemVCpl6tSpq513+fLl+fWvf51DDjkkVVVVqayszNZbb52PfvSjmT17dn2//v37Z9iwYRk8ePDaPSgAAADAOrRB7Bm0OrNnz87IkSPzyCOP5NBDD81pp52WbbfdNu3atcu7776bp59+OtXV1fX9Fy9e3ODnyixbtizHHXdc7rvvvowaNSonn3xyunfvntdffz333XdfNt1003X6XAAAAACtbYMPg958883su+++Wbp0aR599NHsv//+rTb2tddemz/+8Y+54447ctxxxzVoO+2001ptHgAAAIC2ssGHQaecckqqq6szfvz4bLfddq069j333JNBgwY1CoIAAAAANlQbdBj0j3/8I3/4wx9y5ZVXtnoQlLz/ifiamposXbo0HTps0L8qAAAAgCQb0AbSTfn973+fioqKfO5zn1sn45900kmZNWtWTjnllCxbtqzVxq2trU1NTU2DAwAAAKAtbNBh0N///vcMGTJknW3kfOKJJ+b888/Pb37zmwwbNiwvvPBCq4w7ZsyYVFVV1R99+/ZtlXEBAAAAVmeDDoPmzJmT3r17r9M5vvvd7+auu+7Ka6+9liFDhuT0009v8GWyNTF69OhUV1fXHzNmzGilagEAAABWbYMOg+rq6tpkno997GOZMGFCzj777Fx33XXZZZdd8sQTT6zxeJWVlenatWuDAwAAAKAtbNBhUI8ePTJr1qw2mWuzzTbLD3/4wzz//PPp1q1bPvzhD2fy5MltMjcAAABAa9mgw6A99tgjzz77bBYuXNhmc+64447561//mk6dOuWcc85ps3kBAAAAWsMGHQYdd9xxqa2tzW9/+9s2nbdnz5455JBD8uyzz7bpvAAAAABra4MOgw455JAcdNBBOe+88zJz5sw2nXvq1Knp2bNnm84JAAAAsLY26DAoSW644YZ07NgxBx54YJ566qlWHXvx4sVNXr/11lszfvz4fOpTn2rV+QAAAADWtQ7lLmBtbbPNNnnssccyYsSI7L333hk+fHgOP/zw9O3bN6VSKe+++27++c9/Zs8998ynP/3pBvc+9NBDmTJlSqMxd9999/Tp0yennHJKZsyYkWOPPTZ9+vTJ/Pnz89BDD+WGG27Isccem9NPP72tHhMAAACgVWzwYVCSbLvttnniiSdy++2355ZbbslPfvKTzJkzJ4sXL05VVVV22223HHPMMfX9KyoqkiSnnXZak+NdddVVOf3003Pcccfluuuuyw9/+MO8++67qayszO67757rr78+n/70p1Mqldrk+QAAAABay0YRBiVJqVTKCSeckBNOOGG1fUeNGpVRo0attl9zxwMAAADYUGwwYdDTTz+dLbfcMv37988uu+xS7nIa+ec//5lp06a1+r5FAAAAAK1pg9hAun379rnyyitzzDHH5PLLLy93OU267LLLcswxx+SnP/1p2rdvX+5yAAAAAJq0QawMWrp0ablLWK1rr7021157bbnLAAAAAFilDWJlEAAAAACtQxgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBdCh3ARTbfa8/U+4S2tzwrYeUu4SyuLD/3uUuAQAAgFgZBAAAAFAowiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgxqZU899VT+8pe/lLsMAAAAgCZ1KHcBG5srr7wyzzzzTJ555plylwIAAADQiJVBLTBnzpy8/fbb5S4DAAAAYI0Jg5rhJz/5SbbeeutstdVW2XLLLTNw4MD87ne/K3dZAAAAAC0mDFqNb37zm/nGN76R8847L2+99VZmzZqVUaNG5aSTTspPf/rTcpcHAAAA0CL2DFqF5557LpdcckmuvPLK/L//9//qr1900UVZunRpzjnnnHzsYx9Lv379ylglAAAAQPNZGbQK1113XTbddNOccsopjdq+9rWvZenSpbn22mvLUBkAAADAmhEGrcLYsWNz2GGHpVOnTo3aevbsmQMPPDAPPvhgGSoDAAAAWDPCoFWYOnVqtt1225W277DDDpk0aVKLx62trU1NTU2DAwAAAKAtCINWYf78+amqqlpp+5Zbbpl58+a1eNwxY8akqqqq/ujbt+/alAkAAADQbMKgVdhss81SXV290vY333wzXbt2bfG4o0ePTnV1df0xY8aMtSkTAAAAoNl8TWwVtt9++0yfPn2l7ZMmTcqgQYNaPG5lZWUqKyvXojIAAACANWNl0CocdNBBefDBB1NbW9uobfbs2Xn00Udz2GGHlaEyAAAAgDUjDFqFU045JQsWLMj111/fqO2yyy5LRUVFvvCFL5ShMgAAAIA14zWxVdh1111zzjnn5Mwzz0xdXV1OOumkLF26ND/96U/z3//93/mf//mf9OnTp9xlAgAAADSbMGg1fvjDH6ZXr175z//8z5x++ulJkgEDBuTmm2/OyJEjy1wdAAAAQMsIg1ajVCrlrLPOyplnnpnZs2enffv22WqrrcpdFgAAAMAaEQY1U7t27bL11luXuwwAAACAtWIDaQAAAIACEQa1soqKilRUVJS7DAAAAIAmeU2slV1zzTXlLgEAAABgpawMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKpEO5C6DYhm89pNwltLn7Xn+m3CWURRH/rgEAANZHVgYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEUPgx67bXXctNNN2X58uXlLgUAAABgnWuTMKimpibjx49vi6la7IEHHsioUaNSU1NT7lIAAAAA1rk1DoPq6upy99135wtf+EI+8pGP5OSTT85vf/vbLF68uFHfO+64I3vssUeqq6vXqthyuOKKK1IqlVZ7tG/fPttvv30mTpxY7pIBAAAAVmqNwqCFCxfmqKOOyrHHHptnn3023bp1yyuvvJLPfvaz2XPPPfPqq6826F9XV9fg57+aOXNm2rVr16zApVQqZZNNNsk3v/nNJuu6//77V3lv796988wzz7ToWT/3uc9lwoQJqzwefvjhbLbZZqmpqcnmm2/eovEBAAAA2lKHNbnp7LPPzkMPPZT7778/hx9+eP31F154IUcddVSOO+64PPnkk2nfvv1qx9p6660zceLEZu/Z8/3vfz8333xzfvSjHzVqO/jggzNhwoQm73vvvfey55575tFHH82QIUOaNVeSdO3aNV27dm2ybfny5fn1r3+db33rW+nVq1fuuOOObLXVVs0eGwAAAKCttTgMeu+993L99dfn3HPPbRAEJckHPvCBXH/99Tn88MPz4IMP5ogjjljteKVSKTvuuGOz5x80aFDuv//+JtsqKyszePDgVd6/ZMmSZs+1MvPmzctvf/vbXHbZZZk+fXq+8pWv5D//8z/TpUuXtR4bAAAAYF1q8Wti06ZNy6JFi7L//vs32X7ooYemQ4cOefbZZxu1HXfccTnssMNy+OGH55///GfLq02ybNmytGvX9h9BW7hwYe64446MGjUqW2+9dc4888xMnz49m2yySd54443ccccdmTlzZpvXBQAAANASLV4ZtMkmmyTJSr++NW/evCxdurS+37/afffdU1lZmVKptNJXr1Zn3rx52XTTTdfo3pa677778uijj+axxx7LY489loULF+aQQw7JZZddlhNOOCHt2rXLn/70p/z5z3/ON7/5zbzxxhsZMGBA9t1335x55pnZe++926ROAAAAgOZqcRjUv3//bLfddrnmmmty4oknNmr/8Y9/nFKp1OgVsiT5zne+k27duq1RoSvMmzdvta9jLV68uNEeRAsWLEiSZu1jtMK5556bTp065YADDsiXv/zlHHjggenZs2eDPqNGjcqoUaOSJP/85z/zyCOP5Kmnnlrlc9bW1qa2trb+3GftAQAAgLbS4jCoVCrlkksuyYknnpjPfOYzueiii7Lddttl9uzZufLKK/ODH/wgX/3qVzNo0KB1UW/mzJmTHj16NNn20ksv5aCDDsqcOXOabK+oqMiee+7Z7Lla+uWxXXbZJbvssstq+40ZMyYXXXRRi8YGAAAAaA1r9DWxE044ITfeeGO+9rWv5Te/+U06duyYJUuWpHPnzrnwwgvz7W9/e60L6927d8aMGZPPfvazDa7PmjVrpZtEP/HEE5kzZ06eeOKJRoFRqVTKlltu2axXzGpqavL666+vce0rbLrppunbt2+j66NHj85ZZ53VYL6m+gEAAAC0tjUKg5LkE5/4RI477rg8+uijmTlzZrp3755hw4alqqqqUd+Kioq0a9cuHTo0f7rZs2dn7ty5ja6/9tprOfLII5u8Z9myZUmSgQMHrtXraFdddVXOO++8Nb5/hUGDBmXixImNrldWVqaysnKtxwcAAABoqTUOg5L399854IADGlxbtGhRo34jR46s31dnbcyfPz8zZ85cZ6+grTB69OiMHj16nc4BAAAAUA5rFQb17ds3b7zxRrP67rPPPnn88cfXZrosWrQoX/ziFxsFUAAAAAA0z1qFQePHj2/wVayV+e1vf5sLL7wwNTU1a/xJ+STZcsst8/Of/3yl7Su+FPbOO++kc+fOWb58eRYsWJB33nknc+bMybRp0/Lyyy/n2WefzbPPPpuHH364WfO+9tprmT9//mr7dezYMX379k1FRUXzHggAAACgja1VGNS7d+9m9dtyyy2TJHV1dWsz3WrttttuqayszPbbb9+orbKyMttss0369euXnXfeOcOHD2/WvkLf/e53c+GFFza7hl133TXPPfdcS8oGAAAAaDNrFQaNHj06l19+eZP7BP27/v37N9hcemUbRP+rOXPmNLkB87/q0aNHunfvniT54Ac/mDfffDMzZ87M8uXLkySbbbZZunXrtsYrkl5++eXssMMOmThxYv3Ko5X5xje+kauvvnqN5gEAAABoC2sVBt122205+uijc/HFF696kg4d0q9fvwbXPvOZz+T+++9f5X1jxozJmDFjVtnn1FNPbRDAdOnSZaWfnl8TdXV1qaysXG0QlLz/Kfl1vfoJAAAAYG2sVRi0ePHi9OnTZ43Cl/vuu29tpm4zpVIpNTU1efHFF9OuXbtV9n311VdTKpXaqDIAAACAllurMKiysjIzZ85c7atcK/Tu3bvBq2Lrg4qKirRr1y4dOjT9q9h7771z0003ZZdddlntWO3bt89HP/rR1i4RAAAAoNWU6tbivabRo0fnsssua9YXxZLke9/7Xs4777w1nW6jVVNTk6qqqhySY9Oh1LHc5bCO3ff6M+UuoSyGbz2k3CUAAABstJbWLclDuSvV1dWr3Td5rVYGNWdPHwAAAADWH6veBAcAAACAjYowCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQDqUuwAomuFbDyl3CWXRoe825S6hLJbOeK3cJQAAADRgZRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAOpS7gA3NvHnz8u6772b58uWpqqpKVVVV2rWTqQEAAAAbBinGaixbtiz/+7//m+OPPz5bbbVVunbtmm233Tb9+/fPFltskW7duuWwww7L5ZdfnoULF5a7XAAAAIBV2uDDoL///e8plUp5/fXXkyTLly9P586dc8sttzTZ/7TTTssRRxxRf37TTTelc+fOTfZdtmxZjjzyyHzpS19K3759c9NNN2XWrFlZtGhRamtrM3v27Nx9990ZMmRILrjgggwbNkwgBAAAAKzXNvjXxCZNmpROnTqld+/eSZIZM2Zk4cKFGTBgwEr7Dxw4sMH5yvref//9eeCBB3Lbbbfl+OOPb9S+1VZbZauttsohhxySj370o/nwhz+cW2+9NZ/+9Kdb4ckAAAAAWt8GvzJo6tSpGTBgQEqlUpJkypQpSdIg8Pn3/ttvv339+ZQpU1bad8WYK36uyop9g+wfBAAAAKzPNvjkYurUqdlhhx3qz6dMmZIePXqkqqqqUd+lS5dmxowZjfqvLAw6/PDDc/jhh+dTn/pUvv71r+ehhx7Km2++mSVLlmTp0qV5++2388gjj+Siiy7Kcccdl6FDh+aEE05o/YcEAAAAaCUbXRj08ssvNzj/V6+88kqWLVvW7P7t27fPPffck6uvvjrTpk3LiBEj0rNnz1RUVKRjx47Zcsstc8wxx+Tvf/97fvCDH+SRRx5Jp06dWvcBAQAAAFpRqa6urq7cRbTUDTfckE996lOr7Td48OBMmDAhhx9+eB544IHV9v/BD36Qc889d6XtdXV1mTt3bqqrq1NXV5eqqqp069atxa+G1dbWpra2tv68pqYmffv2zSE5Nh1KHVs0FmwoOvTdptwllMXSGa+VuwQAAKAAltYtyUO5K9XV1enatesq+26QG0gfffTRGT9+fBYtWpT99tsvV155ZYYNG5Yk+fSnP52hQ4fmK1/5Sv1Xwq655ppUV1fnlltuydVXX52//vWvSZJp06bl+OOPz5133pltt90222zz/j9W58yZk+XLl690/hWrfxYvXpw5c+Y02adUKqVnz55N7jc0ZsyYXHTRRWv+CwAAAABYQxtkGFRVVZUhQ4Zk4sSJSZKDDjooH/zgB5Mkb731Vvbcc88MGTKkvn///v2TJDfffHO22267+rY33ngjyft7A60IjmbOnFkfCq2tcePG5YADDmh0ffTo0TnrrLPqz1esDAIAAABY1zbIMGiFadOmJUm23XbbJMnChQsze/bsbLfddivtv6LvivMePXrUB0FJ0qdPn1RXV69yZVBzlEqlJjexTpLKyspUVlau1fgAAAAAa2KDDIMWLVqUpUuX5qWXXkqXLl3Svn37zJ8/P5MmTUqS9OzZMwsWLKgPed57773U1dVl6tSp2WuvvTJ//vwkyaRJk9K3b9/Mnz8/HTp0qH/9a8W7dUuXLs3MmTPTnG2V2rVrl549e9pAGgAAAFivbZAbSHfv3j3vvPPOavuNHTs2c+fOzcc+9rHV9h0wYEBefvnlBtceeuihfOhDH2p2XZ07d84LL7xQ/1pac9XU1KSqqsoG0mzUbCANAACw7mz0G0jfe++9qa2tzfnnn5+Kiop85zvfSZLceuutuemmm3LbbbelVCplt912y7Jly/LII49kwYIFOfLII3PppZdm7733TpKccsop2X///XPKKac0eFVshUMOOSSLFi1q1sqgl156KUOGDMmECRNaHAYBAAAAtJUNMgxaEebU1NTkqKOOqv+S2O23355BgwbVn68wbNiwPPvss0mSj370o9lxxx2TJG+++WYOPPDARv3/VXP39rEHEAAAALAhaFfuAtbGlClTssMOOzQ4HzhwYJN9J0+enPbt29ev2nnnnXfy7rvvrrR/ksydOzeDBw9O+/btUyqVVnnstNNOqayszKBBg1r3IQEAAABa0Qa5MihJZs2alfnz5zcIgyZPnpz99tuvyf6TJ09Ov3790rFjx/rzJA3u/3eTJk3KSy+9lBtuuCF77LHHKutp165devfunS5durT0UQAAAADazAa7MmjatGlp165dg5U406dPz+DBg5vsP3369Oy8884N7q+qqkrv3r1XOkepVEqSZu0ZtHz58sycOTOzZ89u7iMAAAAAtLkN8mtibaW6ujp77713pkyZ0qxAKEmOOuqo/PnPf27RPL4mRhH4mhgAAMC6s9F/TaytVFVVZdKkSeUuAwAAAKDVbLCviQEAAADQcsIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIAC6VDuAvj/3THp+XTtUqx8bvjWQ8pdAm1k6YzXyl0CAABQIG+fsl+5S2hTyxYvSn5zV7P6Fit5AAAAACg4YRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACaZMw6LXXXstNN92U5cuXt8V0zVZbW5sbb7wxc+bMKXcpAAAAAG2iVcKg6dOnZ+HChSttf+CBBzJq1KjU1NQ0a7zly5fn7bffztSpU1NdXd0aJTZp1qxZ+eQnP5m//e1vrTbmF77whXz9619vtfEAAAAAWlOHtR1g+fLl6d+/fy655JKcffbZazzOO++8k2uuuSb33ntvHnvssSxevLi+bdNNN80+++yTUaNG5eSTT07Hjh1XOdbLL7+cJUuWNLq+3XbbpVOnTs2u6Y9//GOuvfbaLFq0qFFb+/bts9tuu+X73/9+g+tTpkxJt27dmj0HAAAAQFta65VBK4KSTTbZZI3H+Pvf/55Bgwbl6quvzogRI/LEE09k7ty5WbJkSd5555088MADGT58eC666KLsscceee2111Y53k477dTkcf/997eoru9973t5+umn84EPfKDR8d5772XMmDGZNWvWGj83AAAAQFtb65VBK14P23TTTdd4jM985jPZfvvt83//93+NQqXNN988++67b/bdd9986Utfyu67756vfOUrueOOO1Y63r+uKlobCxcuzIc//OFccskljdruvPPOPPTQQ7n88suz+eab119/9dVXrQwCAAAA1ltrHQa98cYbSdIgEGmJxYsX56WXXsr555+/2tVF3bp1ywEHHJBx48at0Vzrwl//+tdUVlbWn7/99ttlrAYAAABg1db6NbEVr0ltt912a3R/RUVFdt1119x66615/fXXV9n35Zdfzv3335999tmnUdvw4cNTKpWafdx4441rVO+/+/3vf59x48bVH7vvvnurjAsAAACwLqz1yqAXX3wxSTJgwIA1HuM3v/lNjj766AwaNChf+MIXMmzYsPrNnhcsWJDJkyfn4Ycfzo033piddtopV1xxRaMxfv3rX2fu3LnNnrNfv34+KQ8AAAAUzlqHQc8++2yS918X22yzzdZojCFDhmTixIn50Ic+lF/+8pe5++678/rrr2fx4sXp1KlTtttuu7z88ss5/vjj89vf/jbt2jVe0NSrV6/06tWrwbWlS5dm4cKF6dKlyxrV1RyXX355gz2Cpk+fniFDhqzyntra2tTW1taf19TUrKPqAAAAABpa69fE/vKXvyRJbrvtttX2nTx5ciZOnJiXXnqp0affN9tss2y11VbZfffd8/LLL2fhwoVZtmxZFixYkH/+85/Zcssts/XWWzcZBP278ePH57DDDssmm2ySrl27plevXvnmN7+ZBQsWNNl/5syZmThxYiZOnJjq6upmPPX/7/HHH89DDz1UfzRnddKYMWNSVVVVf/Tt27dFcwIAAACsqbVaGTR+/Pi8+uqr6du3b37+85/n61//ejp27LjS/kOHDq3/89ixY3PggQeuzfRNuv322zNy5Mjstddeufrqq9OzZ8888cQTufzyy/Pggw9m7Nixjb58dsYZZ9T/+b/+679y4YUXJkkqKyvz8MMP51vf+lajeZ566qkkyV133ZWtttqq/vohhxyy2hpHjx6ds846q/68pqZGIAQAAAC0ibUKg6666qr07Nkz999/f3bbbbf84he/yOmnn77S/u+++279K1WzZs3KxIkTG7QvWLAg7733XqPryfuvfL3zzjuN2nr06JHu3bsnSZYsWZLTTjsthx12WO65556USqUkyTHHHJOPf/zj2W+//XLFFVfkvPPOazDGHXfckeOOO67RnGeffXZ+9atf5R//+Eejtvbt2+e8885rEAQ1V2VlZYMvkAEAAAC0lTUOgyZPnpzf/va3ufDCCzN48OCce+65Of/88/ORj3ykWV8W++xnP5v777+/ybaddtqpyevXX399rr/++gbXvvSlL+XnP/95kuT555/PnDlzcvbZZ9cHQSvsscceOfjgg3Pvvfc2CoNWZuTIkRk5cmSz+gIAAABsCNZoz6Dly5fntNNOy9Zbb52zzz47SXLBBRdk2223zciRI7No0aLVjnHfffelrq5urY8VQVCS+v2EVvaqWkVFRbP2HGrKe++9l7q6ujW6FwAAAGB9sUbJyH/+53/moYceyi9/+ct06tQpyftBy2233ZYpU6bk2GOPzcKFC1s05nvvvZdf/OIX+fjHP55BgwZliy22SMeOHbPllltm5513zkknnZTf/e53Wbp06UrH2HXXXbPNNtvkmmuuadT28ssv56GHHsrRRx/dsodNMm/evGy66ab57W9/u9q+FRUVqaioaPEcAAAAAG2hxWHQz372s1x88cUZM2ZMo82St99++9x555159NFH6zdhbo4nn3wyAwcOzDe/+c0MGDAg3//+9/Pggw/m2WefzZ///Od84xvfSMeOHfOpT30qe+yxR2bMmNHkOO3bt8+1116b22+/PSeccELuvffePPXUU7nmmmty0EEHZciQIfnKV77S0kfOsmXLGvxclfvvvz+33HJLi+cAAAAAaAst3jPo+uuvz+jRo/ONb3yjyfaDDjooDz74YPr169fsMU866aRss802uffee7P55ps3ah86dGg+97nP5YILLsgBBxyQM888c6Wfsh8+fHheeOGFnH/++fnkJz+Z+fPnZ8CAATn99NNzzjnn2LgZAAAAKLQWh0Hjxo1bbaCyzz77NHu86urqvPzyy/nCF77QZBD0r3bcccccdNBBefbZZ1fZb4cddsjvfve7ZtcAAAAAUBQtDoNae2VN165ds8MOO+TOO+/M6aefnq5du66078svv5xHHnkkBx98cKvWsDodOrz/a5o6dWqTn71vSv/+/a1CAgAAANY7a/xp+ZaoqKhIqVRK+/btG7WVSqXceuutOeqoo9K/f/+ccsop2XfffdO/f/906tQp8+bNy8SJE/Pwww/nt7/9bQYPHpzLL7+8Verq2LFjSqXSSr8+tsJmm22Www8/PN///vdz8cUXN2vs22+/PR//+Mdbo0wAAACAVlOqW0++l75gwYL8+te/zn333ZcXXnghs2fPTm1tbaqqqtKrV6/st99+OeywwzJixIj6lTobi5qamlRVVeXdSQPStcsafeBtgzV86yHlLgEAAICN0Nun7FfuEtrUssWL8txvzk91dfUq37pK2mhlUHNsuummOf3003P66aeXuxQAAACAjVaxlqEAAAAAFJwwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQDqUuwD+fx/fcdd0KHUsdxkAwCrc9/oz5S6hLIZvPaTcJQBAi3S/7rFyl9CmltYtaXZfK4MAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgxqZTNnzsxNN92U5cuXl7sUAAAAgEaEQa3sL3/5S0aNGpWamppylwIAAADQSIdyF5C8v5rm6quvTl1dXYPr7dq1ywUXXJCKior6azfeeGM+/elPZ+7cuenSpUuT4/Xo0SNvvfVWs+bu3LlzfvjDH+aMM85YZb8lS5ZkxowZ6dChQ7bZZpu0aydHAwAAADY860UYNH/+/LzwwgtNhkFLly5tEAYtXrw4y5cvz7Jly1Y63hNPPJHa2trVzltXV5fPfOYz+fOf/7zSMGj58uX5/ve/n0svvTTvvvtukqRfv375/ve/n09+8pPNeTwAAACA9cZ6EQYNGjQod9xxR6uN179//2b37dGjxypX+Zx99tm54oorcs4552TEiBGpra3NVVddlU996lOpra3N5z//+dYoGQAAAKBNlDUM2meffZq1gid5f5XQvffe2+o11NTUZPvtt2+y7dlnn80VV1yRiy++OOeff3799QMPPDDt27fP17/+9RxzzDHp0aNHq9cFAAAAsC6UNQw655xzWhQGVVVVtXoNb7zxRg4++OAm2375y1+msrIyZ555ZqO2b3/727nxxhvzv//7vznrrLNavS4AAACAdaGsYdCJJ55Y/+e5c+fmF7/4RZ566qksW7Ysu+66a0499dRstdVW62z+urq6vPrqqxk4cGCT7Y888kgOPfTQbLrppo3aBg0alA9+8IN54IEHWhwG1dbWNgjBfHkMAAAAaCvrxSexxo0blwEDBuSaa67JFltskT59+uS2227L9ttvnxtvvLHJex5//PGMGzcujz76aBYuXLhG806bNi21tbX54Ac/2GT71KlTV/oKWZIMHjw4EydOzNKlS+uP5cuXr3beMWPGpKqqqv7o27fvGtUPAAAA0FJl30B64cKFOeGEE3L44YfnxhtvTPv27ZO8v2rn61//ek455ZQccsgh2XrrrRvcN2LEiCRJqVTKvffem//6r//K/fffv0Y17LHHHvV//tKXvpSf//znSd7/ylm3bt1Wel/Pnj0zbdq0dOzYsUXzjR49usFqopqaGoEQAAAA0CbKHgY999xzmTNnTr72ta/VB0HJ+yHPWWedlSuuuCLjxo3LyJEjG9z32muvNQhqfvWrX6W6unql8zz44IM544wz8sADD6RPnz4r7fevm0Fvsskmq3yF6+23384222yT//u//6u/dvvtt+fcc89d6T1JUllZmcrKylX2AQAAAFgXyh4G9enTJ6VSKc8991z233//Bm3PPvtskqRfv36rHad3797p3bv3StsnTpyYJNl+++2z3XbbNau2/v3755VXXllp++TJkzN48ODssMMO9dd69uzZrLEBAAAAyqHsYdA222yTc845J2effXbee++9HHPMMenYsWMeeOCBnHvuuTn22GOz7777lqW2/fffPzfeeGMWL16cioqKBm3Tp0/PP/7xj4wZM6YstQEAAACsifViA+kf/OAHueSSS3LFFVdkxx13TP/+/XP++efnm9/8Zn73u9+Vra6TTz458+bNy69+9atGbZdcckk22WSTnHzyyW1fGAAAAMAaKvvKoCRp165dTjvttJx22ml56623smzZsvTs2TOlUqnFYy1btixTpkxJXV1dg+szZ85Mkrz88stZtGhR/fXNNtss22yzTZNj7b///hk1alS++tWvZsmSJTnxxBOzaNGi/OQnP8lVV12VSy+9tNHG1gAAAADrs/UiDEqSq666KnPmzMlFF120VuP885//zG677bbS9sMOO6zRtW9/+9srnfe6665L586d87WvfS1nnHFGkmTTTTfNf//3f+frX//6WtUKAAAA0NbWi9fEkuSRRx7Jfffdt9p+m222WbbYYouVfs79gx/8YOrq6pp1LF++PAcddFD+8Y9/rHS+Tp065Re/+EVee+21PPDAA3nooYfy+uuv55xzzlnjZwUAAAAol/VmZVBzjRgxIiNGjGiVsUqlUjbZZJMsW7ZstX179eqVXr16tcq8AAAAAOWy3oRBHTp0yNy5czNhwoRm7RVUUVGRAQMGtEFlAAAAABuP9SYMOuqoo3LXXXdl5513blb/ioqK1NbWruOqWq6ioiLt2rVLhw7rza8WAAAAoN56s2fQJz/5ycybN6/Z+/2sj0FQkowaNSrLli3LZpttVu5SAAAAABop/PKVr371q+UuAQAAAKDNFD4M+shHPlLuEgAAAADazHrzmhgAAAAA654wCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAulQ7gIAADYkw7ceUu4SyuK+158pdwltrqh/1wBs/KwMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCoFb0+9//PlOnTi13GQAAAAArVegwqKamJuPHj2+18UaOHJm777671cYDAAAAaG0dyl1Aa1u4cGFqa2ubbCuVSqmqqqo/v+OOO/LZz342c+fObXD9382ePTvV1dXp27dvOnfu3Oo1AwAAALSVjWpl0BlnnJHOnTtn8803b/Lo1q1bJkyYUN+/rq6uwc9/N27cuAwZMiS9e/fO4MGDs8UWW+TLX/5y5s+f3ybPAwAAANDaNqqVQX/7299y9NFH58orr2yyvVQqpW/fvs0a64knnsihhx6avfbaK3/605/Sq1evjB07NhdccEEmTZqUv/zlL2nfvn1rlg8AAACwzm1UYdDy5cvTvXv3bLfddms91mmnnZYdd9wxDz74YDp16pQk2WOPPbLzzjtn+PDhufbaa3Pqqaeu9TwAAAAAbWmjek2stYwfPz5PP/10zjzzzPogaIUjjjgi++67b6666qoyVQcAAACw5oRBSR5//PGMGzcujz76aBYuXJhHHnkkSXLsscc22X/kyJF5/vnn88Ybb7RlmQAAAABrbaN6TWxNjRgxIsn7ewrde++9mTp1arp27Zott9yyyf6DBw9OkkyYMCHdu3dv8Xy1tbUNvnhWU1OzBlUDAAAAtJyVQUlee+21zJ8/P/PmzcsBBxyQ+fPnp1u3bivt37NnzyTJhz70oXTs2LH+aK4xY8akqqqq/mjuptYAAAAAa2ujWhlUUVGRV155JU899VTatWuXZcuWZf78+amurs7s2bMzc+bMvPTSS5k0aVLuuuuulY6zySabrHK1zttvv50kueGGGzJ06ND66wMHDmxWnaNHj85ZZ51Vf15TUyMQAgAAANrERhUGnXTSSTnvvPOy1157JUk6duyYLl26pGvXrunRo0e22mqr9O3bN1/4whdWufKnf//+mTt3bqqrq1NVVdWoffLkyUmSgw8+OH369GlxnZWVlamsrGzxfQAAAABra6MKg77+9a/nzDPPzJIlS9KuXbt06LBmj7f//vsnSf785z/npJNOatR+6623ZtCgQWsUBAEAAACU00a3Z1CpVEpFRcUaB0FJMnTo0AwaNChXXXVVli9f3qDtiSeeyNixY/PlL395bUsFAAAAaHMbXRiUJL/61a9y0UUXrfH97dq1yxVXXJG//e1vGTlyZF588cW88847uf3223P00Udnjz32yOmnn96KFQMAAAC0jY0yDHrooYdyxx13rLZfRUXFSl8nGz58eH73u99l7Nix2WWXXdK9e/eccMIJGTp0aO65555UVFSsi9IBAAAA1qmNas+glho1alRGjRq10vYTTzwxH/vYxzJ+/PhUV1dn4MCBGTBgQBtWCAAAANC6Ch0GNUdlZWX23XffcpcBAAAA0Co2yjCoY8eOqampyYQJE1IqlVbbv6KiwoofAAAAoBA2yjBo+PDhueWWW7Lzzjs3q39FRUVqa2vXet6OHTvaSwgAAABYr22UYdCIESMyYsSINp938eLFbT4nAAAAQEtslF8TAwAAAKBpwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUSIdyFwAAwPpv+NZDyl1Cm7vv9WfKXUJZFPHvGqBorAwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBrWS5cuX56abbsprr71W7lIAAAAAVkoY1EpqamoyatSoPPDAA+UuBQAAAGClhEHNtGjRorz33nvlLgMAAABgrWzwYdDf//73lEqlvP7660nef12rc+fOueWWW5rsf9ppp+WII46oP7/pppvSuXPnJvvW1tbmu9/9bnbYYYdssskm2XTTTdOvX79861vfSk1NTes/DAAAAMA61qHcBaytSZMmpVOnTundu3eSZMaMGVm4cGEGDBiw0v4DBw5scN5U39ra2hx66KF57LHH8slPfjIXXnhhOnbsmHHjxuWyyy7LXXfdlXHjxqV79+7r5sEAAAAA1oENPgyaOnVqBgwYkFKplCSZMmVKkjQIfP69/5FHHll/PmXKlCb7XnXVVXn00Udzyy235MQTT6y/PmrUqHz+85/PsGHD8q1vfSu/+MUvWvNxAAAAANapDf41salTp2aHHXaoP58yZUp69OiRqqqqRn2XLl2aGTNmNOrfVBh0xx13ZNiwYQ2CoBX22muvnH766bnpppuyfPnyVnoSAAAAgHVvowuDXn755Qbn/+qVV17JsmXLmtV/zpw52XHHHVc67+DBg7NgwYJUV1evRfUAAAAAbWuDDINuuOGGlEqllEqljBs3Lpdeemn9+X//93/nscceS6lUyk477ZQkOfzww1MqlepDnw9+8IP1/d98882ceuqpKZVK+eEPf1g/x7bbbptXX311pTW89NJL2XzzzZtcgbQ6tbW1qampaXAAAAAAtIUNMgw6+uijM378+Dz22GNJkiuvvDLjx4/P+PHj84EPfCCf//znM378+Nx1111JkmuuuSbjx4/P6NGjs/nmm9f3vf3225Mkd955Z8aPH59TTjmlfo6vfOUrefDBB/Pggw82mv/555/P//zP/+SLX/xi2rVr+a9wzJgxqaqqqj/69u27Jr8GAAAAgBbbIMOgqqqqDBkyJN26dUuSHHTQQRkyZEiGDBmSt956K3vuuWeGDBlS/5pX//79M2TIkCxfvjzbbbddfd8Vn5Q//PDDM2TIkGy55Zb1cxxzzDH5yle+kqOPPjoXXHBBHn744Tz66KP57ne/mwMOOCAf/vCHc9FFFzWq7XOf+1z9qqNzzz23yfpHjx6d6urq+mPGjBmt/BsCAAAAaNoG/TWxadOmJXn/la4kWbhwYWbPnp3ttttupf1X9F1x3qNHj/pQ6N9dccUVOfjgg3PZZZflxz/+cRYvXpy99947l1xySb74xS/Wf8HsX33/+9/Pxz/+8SRJz549mxy3srIylZWVzX5OAAAAgNayQYZBixYtytKlS/PSSy+lS5cuad++febPn59JkyYleT+EWbBgQX3I895776Wuri5Tp07NXnvtlfnz5ydJJk2alL59+2b+/Pnp0KFDOnXq1Giu448/Pscff3zOPPPM3HnnnXn88cdXWVvv3r0zePDgVn5iAAAAgNaxQYZBffr0yTvvvFN/3qVLlwbte++9d5Jk7NixmTt3bj72sY/Vt/3jH//I1Vdf3aB/ly5dMmDAgLz88svrsGoAAACA8tsgw6B77703tbW1Of/881NRUZHvfOc7SZJbb701N910U2677baUSqXstttuWbZsWR555JEsWLAgRx55ZC699NL6sOiUU07J/vvvn1NOOWWlr4oBAAAAbEw2yDBoRZhTU1OTo446KsOGDUuS3H777Rk0aFD9+QrDhg3Ls88+myT56Ec/Wr+x9JtvvpkDDzywUf/p06enVCqlffv2qaury+LFi/POO++ktrY29913X956663Mnj07r732WqZNm5YpU6bkgQceWNePDQAAALDWNsgwaIUpU6Zkhx12aHA+cODAJvtOnjw57du3T//+/ZMk77zzTt59991G/ZctW5YBAwakrq6u0RilUikjR45Mz54906dPn2yzzTbZddddM3LkyLRv374VnwwAAABg3dhgw6BZs2Zl/vz5DcKgyZMnZ7/99muy/+TJk9OvX7907Nix/jxJg/uTpH379nn99ddTW1uburq6lEqldOjQIZ07d07Xrl1XGvrMnTu3FZ4KAAAAYN3aYMOgadOmpV27dhk0aFD9tenTp6/0S17Tp0/Pzjvv3OD+qqqq9O7du1HfXr16tX7BAAAAAOuBDTYM2n///bNs2bIG1xYuXLjS/j//+c8bnH/iE5/IJz7xiVarp0OHDmnXrl0qKipabUwAAACA1rbBhkHrm80226xROAUAAACwvmlX7gIAAAAAaDvCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABRIh3IXAAAA66PhWw8pdwllcd/rz5S7hLIY+NvTyl1CmxvwzcfKXQJQJlYGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCINa2e9///tMnTq13GUAAAAANEkY1MpGjhyZu+++u9xlAAAAADRpow6DFi9enOnTp+fNN99cZb//9//+X4YNG7bKPrNnz85LL72U9957rzVLBAAAAGhTG2UY9Oqrr2bUqFHp1q1b+vfvn549e6Z///750Y9+lCVLljTqv2DBgsyfP7/JscaNG5chQ4akd+/eGTx4cLbYYot8+ctfXml/AAAAgPVZh3IX0NqmTp2afffdN/3798+f/vSn7Lnnnqmurs5dd92Vb3/723nooYdy6aWXNrinurq6ybGeeOKJHHroodlrr73ypz/9Kb169crYsWNzwQUXZNKkSfnLX/6S9u3bt8VjAQAAALSKjS4MOvPMM9OlS5c8+OCD2WyzzZIkXbt2zRlnnJFddtklhx12WHbaaadG9+22226Nrp122mnZcccd8+CDD6ZTp05Jkj322CM777xzhg8fnmuvvTannnrqun0gAAAAgFa0Ub0mtmjRotxzzz358pe/XB8E/asPfehDGTZsWI444ojU1dXVH5/5zGca9R0/fnyefvrpnHnmmfVB0ApHHHFE9t1331x11VXr7FkAAAAA1oWNKgx69913s3Tp0vTq1Wulffr06ZPZs2evdqxHHnkkSXLsscc22T5y5Mg8//zzeeONN9asWAAAAIAy2KheE9tyyy3TuXPnTJkyZaV9XnzxxfTr12+1Y02dOjVdu3bNlltu2WT74MGDkyQTJkxI9+7dW1RnbW1tamtr689rampadD8AAADAmtqoVgZ17Ngxn/zkJ/OTn/wkM2fObNR+zTXX5Nlnn824ceOyww471B+33357o77z589Pt27dVjpXz549k7z/6lnHjh3rj+YYM2ZMqqqq6o++ffs27wEBAAAA1tJGtTIoSX70ox/liSeeyF577ZULL7wwe+21V+bOnZtbb7011113XY4//vgcfvjhDe759a9/nYULFza4tskmm6xyxc7bb7+dJLnhhhsydOjQ+usDBw5cbY2jR4/OWWedVX9eU1MjEAIAAADaxEYXBnXr1i1/+9vfcumll+ZnP/tZpkyZkk6dOmX33XfPb37zm4waNSqlUqnBPY8//nieeeaZBtf69++fuXPnprq6OlVVVY3mmTx5cpLk4IMPTp8+fVpUY2VlZSorK1v2YAAAAACtYKMLg5Kkc+fOueCCC3LBBRes8Rj7779/kuTPf/5zTjrppEbtt956awYNGtTiIAgAAACgnDaqPYNWZ+7cuZk6dWpeeeWVvPfee/XXd9hhh+y6664N+g4dOjSDBg3KVVddleXLlzdoe+KJJzJ27Nh8+ctfbpO6AQAAAFrLRrky6F9NnDgxP/7xj/OnP/0ps2bNqr9eKpWy0047ZcSIEfnqV7/a6Itg7dq1yxVXXJGjjjoqI0eOzH/913+lV69eeeihh/LlL385e+yxR04//fS2fhwAAACAtbJRrwy6/fbbM2TIkEydOjWXXHJJpk6dmvfeey/vvvtunn322Xz+85/PzTffnF122SXPPfdco/uHDx+e3/3udxk7dmx22WWXdO/ePSeccEKGDh2ae+65JxUVFWV4KgAAAIA1t9GuDFq6dGlOPfXUfPSjH81tt93WoG2TTTZJt27dsuuuu+a0007Lfvvtl1NPPTWPPfZYo3FOPPHEfOxjH8v48eNTXV2dgQMHZsCAAW31GAAAAACtaqMNg95555289dZbOfLII1fZr3PnzjniiCNyzTXXrLRPZWVl9t1339YuEQAAAKDNbbSvifXo0SPbb799br755ixevHil/d54443ccccdOfjgg9uwOgAAAIDy2GhXBpVKpdx44405+uijs8suu+Tkk0/OPvvsk549e2bJkiWZOXNm7r///tx8883p379/fvGLX7TKvB07drSXEAAAALDe2mjDoOT9z8NPnDgx1113Xe6555789Kc/zbvvvpt27dqlV69e2XPPPfPTn/40I0eOTIcOrfOrWNUqJAAAAIBy26jDoCTZYost8o1vfCPf+MY3yl0KAAAAQNlttHsGAQAAANCYMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUSIdyFwAAAKw/hm89pNwllMWyK5eXu4Q2d9/rz5S7hLIo6n/j8K+sDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAil8GPT73/8+U6dOXetxamtrc+ONN2bOnDmtUBUAAADAurHBhUE1NTUZP358q403cuTI3H333Ws9zqxZs/LJT34yf/vb31qhKgAAAIB1o0O5C0iShQsXpra2tsm2UqmUqqqq+vM77rgjn/3sZzN37twG1//d7NmzU11dnb59+6Zz586tXjMAAADAhqjsK4POOOOMdO7cOZtvvnmTR7du3TJhwoT6/nV1dQ1+/rtx48ZlyJAh6d27dwYPHpwtttgiX/7ylzN//vwW1TVw4MCUSqUmj06dOuW8885b84cGAAAAKJOyrwz629/+lqOPPjpXXnllk+2lUil9+/Zt1lhPPPFEDj300Oy1117505/+lF69emXs2LG54IILMmnSpPzlL39J+/btmzXWAw88kIULFzbZ9oUvfCF//etfmzUOAAAAwPqk7GHQ8uXL071792y33XZrPdZpp52WHXfcMQ8++GA6deqUJNljjz2y8847Z/jw4bn22mtz6qmnNmusbbfddqVtS5cubXZABQAAALA+KftrYq1l/Pjxefrpp3PmmWfWB0ErHHHEEdl3331z1VVXrfU8S5YsyYsvvphdd911rccCAAAAaGsbbBj0+OOPZ9y4cXn00UezcOHCPPLII0mSY489tsn+I0eOzPPPP5833nhjreYdN25c5s2blyOOOGKtxgEAAAAoh7K/JramRowYkeT9PYXuvffeTJ06NV27ds2WW27ZZP/BgwcnSSZMmJDu3buv8bzXXntttt9+++yzzz5rPEZtbW2Dr6fV1NSs8VgAAAAALbHBrgx67bXXMn/+/MybNy8HHHBA5s+fn27duq20f8+ePZMkH/rQh9KxY8f6oyWmTJmS3//+9znjjDNSKpWa7PPxj3+8/qtjF198cZN9xowZk6qqqvrD/kMAAABAWyn7yqCKioq88soreeqpp9KuXbssW7Ys8+fPT3V1dWbPnp2ZM2fmpZdeyqRJk3LXXXetdJxNNtlklSts3n777STJDTfckKFDh9ZfHzhwYLNrPeOMM9KvX7+cdtppK+3z05/+NIceemiSpHfv3k32GT16dM4666z685qaGoEQAAAA0CbKHgaddNJJOe+887LXXnslSTp27JguXbqka9eu6dGjR7baaqv07ds3X/jCF1a58qd///6ZO3duqqurU1VV1ah98uTJSZKDDz44ffr0aXGdY8aMyV/+8pfcf//9qaysXGm/Pn361L+StjKVlZWrHAMAAABgXSl7GPT1r389Z555ZpYsWZJ27dqlQ4c1K2n//fdPkvz5z3/OSSed1Kj91ltvzaBBg9YoCPrVr36VCy64IBdffHH9qh8AAACADdF6sWdQqVRKRUXFGgdBSTJ06NAMGjQoV111VZYvX96g7YknnsjYsWPz5S9/ucXjXnLJJfn85z+fr33taznvvPPWuD4AAACA9cF6EQYl76++ueiii9b4/nbt2uWKK67I3/72t4wcOTIvvvhi3nnnndx+++05+uijs8cee+T0009v9nivv/56jjvuuJx77rm56KKLcumll65xbQAAAADri/UmDHrooYdyxx13rLZfRUXFSl8nGz58eH73u99l7Nix2WWXXdK9e/eccMIJGTp0aO65555UVFQ0q5b7778/gwYNylNPPZU//elPufDCC1v8PAAAAADro/UmDGquUaNGZdmyZdlss82abD/xxBMzY8aMPPbYY7n33nvz8ssv549//GO23HLLZs+x22675ayzzsqECRNy5JFHtlbpAAAAAGVX9g2k14XKysrsu+++a3z/VltttVavrAEAAACsr9abMKhjx46pqanJhAkTUiqVVtu/oqIiAwYMaIPKAAAAADYe600YNHz48Nxyyy3Zeeedm9W/oqIitbW1az1vx44dm72X0OrGWfFVNAAAAID11XoTBo0YMSIjRoxo83kXL17cKuP06dOn0SftAQAAANY3G9wG0gAAAACsOWEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACqRDuQsAAAAot4Ff+Xu5S2hzw78ypNwllEepVO4KyqOurtwVtLkF9w4odwltaumC2uT45vW1MggAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACmSDDIOefvrp/OUvfyl3GU166qmn8vDDD5e7DAAAAIAmdSh3AWviJz/5SZ555pk888wz5S4lNTU1mT59ej74wQ8mSa688spMnz49Dz30UHkLAwAAAGjCerUyaMGCBXnppZcyZ86cNbr/3XffzUsvvZSJEydm4sSJefvtt7Ns2bJMnTq1/tpLL72UZcuW1d/To0eP/OhHP1rjmu+4447stttua3w/AAAAQFtaL8Kg6urqnHzyydl8880zePDgbLXVVtlnn33y1FNPNXuMJ598Mj169MjgwYOz0047Zaeddsp3v/vdHHPMMdl+++3rrw0ePDgTJ06sv2/evHl57733Go33pS99KaVSqdGx2Wab5bLLLqvvV1dXt3YPDwAAANCGyh4GLV68OEcccUT+8Ic/5PLLL8/TTz+du+++O0uXLs0hhxyS5557rlnjTJgwIcuWLUttbW3q6upSV1eXyy67LC+88EIuuuii+mt1dXXZZZddVjve9773vUyYMKHB8eKLL2a33XbL9ddfv7aPDQAAAFAWZd8z6Oqrr86TTz6Zhx9+OAceeGCSZPfdd8+HPvSh7Lnnnjn11FPz2GOPrXac5cuXJ0kqKioaXf/Xa1OmTMnSpUvrz1e2sqdHjx7p0aNHo+ubbrppozkAAAAANhRlXxl03XXX5YADDqgPglbYbLPNctZZZ+Xxxx/P008/3SpzLV++vMFrZDvttFMWL17cojEmTZqU7bffvlXqAQAAAGhrZQ2Dqqur89xzz+XYY49tsn3EiBEplUp54IEHWmW+du3aZenSpQ1eGausrGz2/a+++mpeeeWV7LPPPq1SDwAAAEBbK+trYtOmTUuSla606d69e3r06JEJEyY069Wude2uu+5KkvTr169+E+pZs2a1eJza2trU1tbWn9fU1LROgQAAAACrUdYwaP78+UmSbt26rbRPz54986tf/Sq/+tWvGlz/98+5d+rUKcn7XxXr2bNnFi1alM6dO6djx46ZMWNGnnvuuSxYsCCvv/56/vnPf+bII4/M0KFDm13r8uXLc/XVVydJjjzyyGbf15QxY8bkoosuWqsxAAAAANZEWV8T22STTZKsemXM22+/nRNOOCGTJ0+uP44//vhG/T784Q9nzz33zD777JPtttsuu+22W7773e/mP/7jP3Lddddl//33z8c//vFcfPHFef755zNv3rwW1Xr99ddnwoQJ+fvf/97gNbM1+bLY6NGjU11dXX/MmDGjxWMAAAAArImyrgzq379/kuSVV15psr2mpiZvvPFGdt999+ywww7117t06dKob8+ePfOPf/wjdXV1WbJkSYMvfv3gBz9YaQ0DBw5Mz549V1nntGnTcs455+QLX/hCi1YTrUxlZWWL9ioCAAAAaC1lXRm0xRZbZNCgQfnTn/7UZPsdd9yR5cuX59BDD232mKVSqUEQdPHFF6dUKjU6fv/73ydJnn/++Zx++ukrHe/dd9/Ncccdl379+uXyyy9vdh0AAAAA66OyrgxKkpNPPjn/+Z//mX/+85/ZZZdd6q8vWbIkl19+eXbbbbfsu+++azz+V7/61Zx44omNrm+77barvXf27Nk5+uij8+6772bs2LHp3LnzGtcBAAAAsD4oexj01a9+Nb/+9a9z1FFH5X/+53+y//77Z8aMGfnmN7+ZF198MQ8//HCLx/zEJz6R6dOnN7t/RUVFxo4d2+DauHHjMmrUqFRWVmbcuHHp169fi+sAAAAAWN+UPQzabLPN8sADD+STn/xkjj766Prr22yzTf74xz+u0aqgk08+Oe+8806z+//ra2VJ8tJLL+Xggw/O0UcfnV/96lfZfPPNW1wDAAAAwPqo7GFQkvTt2zdjx47NpEmTMm3atGy++ebZc8890759+zUa7yMf+UiS5L333ss111yTO+64I5MmTco777yT7t27Z9CgQTn++OPzxS9+sf6T9P9q0KBBuf/++1u0VxEAAADAhmC9CINW2HHHHbPjjju2ylhLly7NgQcemNmzZ+ecc87J0KFD07Vr18ydOzePP/54/uu//iu/+93v8vDDDzcZOgmCAAAAgI3RehUGtaYXX3wxTz/9dP7whz80eP0sSQ488MD07t07n/70pzNp0qTstNNOZaoSAAAAoG1tkGFQRUVFo31+/t3AgQPTr1+/fO9730vnzp2zxx57pEuXLqmpqcmTTz6ZH//4x9l+++0zYMCAta6lY8eOLaoNAAAAoFxKdXV1deUuYl157bXXcvHFF+f222/PW2+9VX+9R48eGTFiRC688ML07t27jBW+r6amJlVVVTkkx6ZDqePqbwAAAGDNlErlrqA8Nt5/+q/UgnvXbvHHhmbpgto8efwVqa6uTteuXVfZd4NcGdRc22yzTX7+85/n6quvzrvvvpu5c+emW7du2XzzzVMq6v8BAAAAAIW2UYdBK5RKpWyxxRbZYostyl0KAAAAQFm1K3cBAAAAALQdYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAokA7lLgAAAADayp9fe6rcJZRF+1Lx1oIM37rcFbStpXVLmt23eP81AAAAABSYMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBFD4Mmjp1am655ZZylwEAAADQJjbIMGjs2LGZPXt2q4x199135z/+4z9aZSwAAACA9V2HtR1gyZIleeutt9KtW7dssskmK+03cuTIzJ07N/fff3+D65dffvlKg53OnTvn4x//eHbdddcG1w8++OBcfvnl+drXvra25a/Wn//85/zP//xP7r777pRKpXU+HwAAAMC6tMZh0OTJk3PuuefmnnvuyaJFi5Ike+65Z84999yceOKJjfovXrw4ixcvbnT9+eefzyuvvNLo+tKlS/O3v/0tDzzwQMaOHduova6ursm65s6dmzfeeGOl7UnSq1evdOvWbaXt/2rSpEn54x//mGXLlqVDh7XOzgAAAADKao3SjRdffDEHHHBAevXqlWuuuSa77rpr5syZk5tvvjmf+MQnMn78+Hz1q19tcE9tbW2TY1133XUrnWf77bdP9+7d89ZbbzW7riFDhmTJkiWr7HfRRRfl29/+drPGBAAAANiYrFEYdPrpp6d79+558skns9lmm9VfP+KIIzJ48OCce+65GTNmTKP7Dj744GbPMWnSpEydOjVTp07NnXfe2ax7Jk+enCVLluS5555r9GoZAAAAAGuwgfSbb76Zhx9+ON/61rcaBEErfOMb30ivXr1y2mmn5d13360/PvKRj7Ronu985zvp2bNnnn/++UyYMKHBsTIrXg3r0qVLyx4KAAAAoCBavDJo1qxZSZJtt922yfZSqZT+/ftn1qxZDfbl6dixY7Pn+OUvf5mbb745v/jFL/KBD3ygpSW2qoqKiiTJ+PHjmxUyrXj+Ffc1pba2tsFrczU1NWtfKAAAAEAztDgM6tWrV5Lk1VdfbbJ9+fLlmTp1aj74wQ+uUUHXXnttTjvttHzuc5/LoEGD8tOf/nSNxmkthxxySLbZZpsMHTq02ffcf//9Ofzww1faPmbMmFx00UWtUR4AAABAi7Q4DOrZs2cOPPDA/OhHP8qoUaMafU7+hz/8Yd5444088cQTGTFiRP31J598MgMHDlzpuG+//Xa+9rWv5YYbbsjpp5+eK6+8Mj/4wQ9yyy23NLu2FZ9+nzdvXgufauV23nnnzJgxo9XGS5LRo0fnrLPOqj+vqalJ3759W3UOAAAAgKas0QbSV199dQ444IDsu+++GTNmTIYMGZI333wzV111Va699tqMHDmy0QbOL774YpNjVVdX52c/+1l++MMfpkOHDrnhhhsyatSoJMl5552X8847r9E9K0Kffzdw4MB07NhxtauSPvWpT+V///d/m/Oo60RlZWUqKyvLNj8AAABQXGsUBu2888558sknc/bZZ+e4446r/5T7jjvumF/84hc55ZRTGt3zj3/8I3Pnzm1w7bHHHsvw4cOzdOnSnHbaaTn//POzxRZbrHb+jh07Nrknz84775w5c+bkjTfeSF1dXebNm5ehQ4fm+9//fj7+8Y/X91vdHNXV1fV7I62NTTfd1IofAAAAYL2yRmFQkuywww656667snDhwsyaNStdu3bNlltu2aIxPvCBD+SCCy7I5z73uXTv3j2LFy/OokWLVnlPqVTK4sWLV9rerVu3+o2rV4RPvXv3zuDBg5td189+9rMmVyS11I477piXXnpprccBAAAAaC0t/rT8v9tkk00yYMCAFgdByfufgP/mN7+ZHj165Lzzzssmm2yy2qNTp07ZfPPN8/jjj69t6Ss1evTo1NXVrfUhCAIAAADWN2sdBg0ZMqRVvvh1/vnnZ9q0aas9JkyYkLlz5+aJJ55Y6zmb4ytf+UqGDRvWJnMBAAAArGtr/JrYCtOnT89bb7212n7bbLNNunbtutL2Ll26pEuXLqsdZ8VrZHV1dQ2uv/LKK1myZEk6dOiQurq6LFu2LG+++WaS5IUXXsjNN9+c2bNnZ+bMmZk+fXomTpyYY445Jj179lzlfPPmzWu01xEAAADAhmqtw6DmWt3qob///e857rjjMnv27NWO1a5du0ZfKzv00EPz8ssvN+rbqVOn3Hjjjbn//vvTs2fP9OnTJ4MHD87RRx+dYcOG5Q9/+EPLHgQAAABgA9ZmYdDqPPjgg6murs5zzz2Xjh07rrRfqVRKjx49Gn0R7IUXXkhtbW3q6upSKpXSvn37dOrUKR06rDePCAAAAFB2a52UdOjQITNmzMjEiROb1X/bbbfNJpts0uj64sWLU1FR0WjFT3N16tQpnTp1WqN7V6WioiLz5s3LhAkTUiqVmnVPz549V/v5egAAAIByWOsw6LjjjstvfvOb/PKXv2xW/xtvvDEnnXRSo+uVlZWpra3N888/v8qVQSt07tw5/fr1a3G9/66iomKV8330ox/NLbfckp133rnZY55++um56qqr1ro2AAAAgNZWqvv3nZjL5IknnsgxxxyTOXPmNKv/wIEDM2nSpHVcVduoqalJVVVVDsmx6VBafRAGAADAmvnzzKfLXUJZtC+t9cfENzjDtx5S7hLa1NK6JXkod6W6unqVH/BK1qM9g4YOHZo33nij3GUAAAAAbNSKFw0CAAAAFJgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABRIh3IXAABsuBaM2KfcJbS5TW/9e7lLAGAtfKTPHuUuAcrOyiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGEQAAAAQIEIgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKJBCh0GvvfZabrrppixfvrzcpQAAAAC0iQ7lLqCcHnjggXzuc5/LUUcdlW7dujVqr66uzqxZs5o1VqdOndKvX7+0a1fofA0AAABYzxU6DFqdK664It/5znea3f/YY4/NnXfeue4KAgAAAFhLbb6M5e9//3tKpVJef/31JMny5cvTuXPn3HLLLU32P+2003LEEUfUn990003p3Llzo36PPvpoSqXSSo/u3btn7NixLar129/+durq6pp1fPGLX8zTTz/dovEBAAAA2lqbrwyaNGlSOnXqlN69eydJZsyYkYULF2bAgAEr7T9w4MAG50313XvvvTNhwoQmx7jrrrvyrW99KwsXLmxxvW+99Vbefvvt1NXVrbJfdXW1V8QAAACA9V6bh0FTp07NgAEDUiqVkiRTpkxJkgaBz7/3P/LII+vPp0yZ0mTfioqKDB48uMkxLr300vTu3TuHHnpoi2q9/fbbM2LEiNUGQSvmP/XUU1s0PgAAAEBbK0sYtMMOO9SfT5kyJT169EhVVVWjvkuXLs2MGTMa9T/wwAObPd/cuXNz880356tf/Wo6dGjZ4z7zzDPZfPPNM2fOnLRv375F9wIAAACsj8oSBg0dOrT+/OWXX24Q9vyrV155JcuWLWvQ/vLLL+dzn/tcs+f78Y9/nCQ544wzWlxr+/bts3Tp0kyYMKFZQVKpVMr222/f4tAJAAAAoK20SWpxww035FOf+lT9+bhx43LppZc26FMqlTJ48OBMmDAhhx9+eB544IH6tg9+8IMN+p566qk59dRT84Mf/CDnnnvuSud96aWXcskll+Tiiy9Or169Wlz3hz70ofz0pz/Nrrvu2ux7nnrqqeyxxx6r7FNbW5va2tr685qamhbXBgAAALAm2iQMOvroozN+/PgsWrQo++23X6688soMGzYsSfLpT386Q4cOzVe+8pX6r4Rdc801qa6uzi233JKrr746f/3rX5Mk06ZNy/HHH58777wz2267bbbZZpuVzrlw4cKcdNJJWbRoUYYPH75GdR900EF588031+jeVRkzZkwuuuiiVh8XAAAAYHXaJAyqqqrKkCFDMnHixCTvhywrVvu89dZb2XPPPTNkyJD6/v3790+S3Hzzzdluu+3q2954440kyeGHH97k5+VXWL58eT71qU9lxowZ2WOPPfIf//Efefzxx9O1a9fV1lpdXZ1Zs2atyWM2sOmmm6Zv375Nto0ePTpnnXVW/XlNTc1K+wIAAAC0pjbd3GbatGlJkm233TbJ+6t3Zs+ene22226l/Vf0XXHeo0ePVQZBy5Yty8knn5x77rknf/nLX9K/f//svffeOe6443LPPfeksrJylTX+7Gc/y3nnndfCJ2ts0KBB9eHXv6usrFxtHQAAAADrQru2mGTRokWZP39+XnrppXTp0iXt27fP/PnzM2HChCRJz549s2DBgtTV1aWuri4LFizI/PnzM3Xq1PTq1Svz58/P/PnzM2nSpPTt2zfz58/PokWLGs0zb968fOxjH8udd96ZP/zhDznggAOy9dZb57777suzzz6bj3/8403e969Gjx5dX8faHCsLggAAAADKqU1WBvXp0yfvvPNO/XmXLl0atO+9995JkrFjx2bu3Ln52Mc+Vt/2j3/8I1dffXWD/l26dMmAAQPy8ssv11+bOHFiRowYkbfffjv/93//1+CLZR/4wAfy0EMP5YgjjsgBBxyQe+65Jz179lxt3W+99VbeeuutZj1jZWVl+vXr5xP0AAAAwHqtTcKge++9N7W1tTn//PNTUVGR73znO0mSW2+9NTfddFNuu+22lEql7Lbbblm2bFkeeeSRLFiwIEceeWQuvfTS+rDolFNOyf77759TTjmlwatis2bNyl577ZU99tgj9957b5MbS++666558sknc+GFF2bTTTdtVt3f/OY3c/311zf7OYcPH55777232f0BAAAA2lqbhEErwpyampocddRR9V8Su/322zNo0KD68xWGDRuWZ599Nkny0Y9+NDvuuGOS5M0338yBBx7YqH/v3r3zu9/9LkceeeQqV+Zss802LQp3fvnLX+aXv/xls/p+61vfatHYAAAAAOXQphtIT5kyJTvssEOD84EDBzbZd/LkyWnfvn39l8XeeeedvPvuuyvt/9GPfrT1C04yc+bMzJ8/P3V1davs9+abb6ZUKq2TGgAAAABaS5uFQbNmzcr8+fMbhEGTJ0/Ofvvt12T/yZMnp1+/funYsWP9eZIG969rP/zhD/Otb32rWX0rKiryuc99bh1XBAAAALB22iwMmjZtWtq1a5dBgwbVX5s+fXoGDx7cZP/p06dn5513bnB/VVVVevfuvc5r/dca+vXrl3vuuSft2jXvw2vvvfdeg/2MAAAAANYnbRYG7b///lm2bFmDawsXLlxp/5///OcNzj/xiU/kE5/4RKvWVFFRkXbt2qVDh6Z/Dfvtt1/+93//N7vsskuzx/z1r3+dk08+ubVKBAAAAGhVpbrVbYbDOldTU5OqqqockmPTodSx3OUAQLMtGLFPuUtoc5ve+vdylwAA0MjSuiV5KHeluro6Xbt2XWXf5r37BAAAAMBGQRgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAulQ7gIA2Mi0a1/uCsrivteeKncJZfGJaVuUu4Q29+6t5a4AAGDtWBkEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCAAAAKBBhEAAAAECBCIMAAAAACkQYBAAAAFAgwiAAAACAAhEGAQAAABSIMAgAAACgQIRBAAAAAAUiDAIAAAAoEGFQK6upqcn48ePLXQYAAABAkzqUu4ANQV1dXf7whz/k7rvvzuuvv54tt9wyRxxxREaOHJmKiooGfe+444589rOfzdy5c1NVVVWmigEAAACaZmXQaixcuDBHHXVUjj322Dz77LPp1q1bXnnllXz2s5/NnnvumVdffbVB/7q6ugY/AQAAANYnVgatxtlnn52HHnoo999/fw4//PD66y+88EKOOuqoHHfccXnyySfTvn37MlYJAAAA0DxWBq3Ce++9l+uvvz7nnntugyAoST7wgQ/k+uuvz/jx4/Pggw+WqUIAAACAlhEGrcK0adOyaNGi7L///k22H3rooenQoUOeffbZRm3HHXdcDjvssBx++OH55z//ua5LBQAAAGgWr4mtwiabbJLk/S+ENWXevHlZunRpfb9/tfvuu6eysjKlUildu3Zdp3UCAAAANJcwaBX69++f7bbbLtdcc01OPPHERu0//vGPUyqVGr1CliTf+c530q1btybHra2tTW1tbf35ysImAAAAgNbmNbFVKJVKueSSS/Lggw/mM5/5TKZPn54kmT17ds4///x897vfzVe/+tUMGjSoReOOGTMmVVVV9Uffvn3XQfUAAAAAjQmDVuOEE07IjTfemHvvvTf9+/dPRUVFevfuncsvvzwXXnhhLr300haPOXr06FRXV9cfM2bMWAeVAwAAADTmNbFm+MQnPpHjjjsujz76aGbOnJnu3btn2LBhqaqqatS3oqIi7dq1S4cOK//VVlZWprKycl2WDAAAANAkYVAztW/fPgcccECDa4sWLWrUb+TIkRk1alRblQUAAADQIsKgZurbt2/eeOONZvXdZ5998vjjj6/jigAAAABaThjUTOPHj2/wBbCV+e1vf5sLL7wwNTU1PikPAAAArHeEQc3Uu3fvZvXbcsstkyR1dXXrshwAAACANSIMaqbRo0fn8ssvb3KfoH/Xv3//JjeXBgAAACg3YVAz3XbbbTn66KNz8cUXr7Jfhw4d0q9fvzaqCgAAAKBlhEHNtHjx4vTp0yeDBw8udykAAAAAa0wY1EyVlZWZOXNmJk6c2Kz+vXv39qoYAAAAsN4RBjXT8ccfn8suuyy33nprs/p/73vfy3nnnbeOqwIAAABoGWFQM40ZMyZjxowpdxkAAAAAa6VduQsAAAAAoO0IgwAAAAAKRBgEAAAAUCDCIAAAAIACEQYBAAAAFIgwCAAAAKBAhEEAAAAABSIMAgAAACgQYRAAAABAgQiDAAAAAApEGAQAAABQIMIgAAAAgAIRBgEAAAAUiDAIAAAAoECEQQAAAAAFIgwCgP+vvbuOjup6vwb+3DjE8BBISEIgRhIkJEGKu7sECsUpFIpbKZRCkRYo/isUKE4p0qKlFFrcgltxt6Bxl/3+wTv3O0MSnEwmsz9rsdrcuTM5J3N13yNEREREREaEYRARERERERERkREx03cBiIhysh0PTuu7CFkuEcn6LoJe1CsSqO8i6MlzfReAiIiIiN4SWwYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRj0jkqXLi0LFy7UdzGIiIiIiIiIiN4Kw6B3cObMGTl79qyYmprquyhERERERERERG+FYdA7mDhxooiILF26VL8FISIiIiIiIiJ6SwyD3tLy5ctl3bp18tVXX8nx48dl7Nix+i4SEREREREREdEbM9N3AQzJ+vXrpUePHtKuXTuZOHGilClTRkJCQuTx48cyd+5cMTPjn5OIiIiIiIiIsje2DHoDSUlJMmrUKGnbtq00b95cVqxYISIibdq0kT///FPWrFkjgYGB8u+//+q5pEREREREREREr8Yw6BWSkpJk9erVUqpUKZk5c6ZMmTJFfv31VzE3N1fXqVu3rpw4cUIcHR2lVq1aUqVKFZk/f76Eh4dn+rmJiYkSFRWl84+IiIiIiIiIKCswDHqFcuXKyWeffSbBwcFy9uxZGT58eIYziLm7u8uff/4pf/31lxQoUEC+/PJL+fXXXzP93MmTJ4u9vb36z9nZ+WNWg4iIiIiIiIhIpQCAvguRXe3evVvc3NzE1dX1rd4XHh4uNjY2Oi2ItCUmJkpiYqL6c1RUlDg7O0t1aSZmSsbvISLDtOPBaX0XIcslIlnfRdCLpkUD9V0EIiIiIjJiKUiWPbJJIiMjxc7O7pXrcsTjV6hRo8Y7vS9v3ryvfN3S0lIsLS3f6bOJiIiIiIiIiN4Hw6AMxMfHy+3bt9/7cywsLKR48eIfoERERERERERERB8Gw6AMbNy4UTp06PDen2NpaSkJCQkfoERERERERERERB8GB5DOQEhIiAB45T9LS0uZMGHCK9dhEERERERERERE2Q3DICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMOgd2RhYSEWFhb6LgYRERERERER0Vvh1PLvKCoqSt9FICIiIiIiIiJ6a2wZRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREbETN8FICLKyeoVKaPvIhAREREREelgyyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIGH0YdO/ePfn1118lLS3tvT/r0qVLsnHjxvcvFBERERERERHRR5Jtw6ALFy7IoEGDJDw8/KP+nl27dkmHDh0kKirqvT9rzZo10qVLl/cvFBERERERERHRR/LeYdCtW7dk9+7dcvz4cUlKSsp0vW7dukmlSpXe+HNXr14tM2fOlAcPHrxVec6dOycxMTFv9R4iIiIiIiIiImPxzmHQsWPHJDAwUNzc3KRmzZoSGBgohQoVkm+//VZSUlLSrZ+UlPTKsEjb06dP5eeffxZTU1MZOXKkAHij90VGRoq/v79s2LDhreqSkZSUFAkMDBRTU1NRFCXDf25ubu/9e4iIiIiIiIiIspLZu7zp7NmzUr16dSlRooRs3rxZypQpI8+ePZO1a9fKpEmT5PLly7J69ep3KlBMTIy0bt1a8ufPL7///rvUr19f+vTpI3PmzBFzc/NXvlcTGr1pePQqYWFhcvz4cZk2bZo0atQow3Vy5cr13r+HiIiIiIiIiCgrvVMYNHz4cClQoIDs379f7OzsRETE2dlZypQpI8WLF5eePXtK165dpU6dOm/1uTt37pS+ffsKAPnzzz/Fw8NDtm7dKq1bt5bTp0/LL7/8Ij4+Pu9S5Lemad3k7u4uXl5eWfI7iYiIiIiIiIg+trfuJpaQkCD//POP9O3bVw2CtPXo0UOcnJzeuKtWTEyMrFy5UipVqiT16tWTwMBAOXr0qHh4eIiISI0aNSQ0NFTMzc3Fz89PWrduLZs3b5bY2Ni3LToRERERERERkdF76zDo+fPnkpKSIoULF850HWdnZ3n48OFrPys5OVlcXFykW7duUqRIETly5IisXr1a8ufPr7Oeu7u77N+/X37//Xd5+vSptGjRQmrVqvW2Rc82EhMTJSoqSucfEREREREREVFWeOtuYvnz5xdzc3O5evVqhq+npaXJzZs3xc/P77WfZW5uLmvXrhU/Pz8pVKjQa9dv1qyZNGvWTMLCwiQ6Ovpti55tTJ48Wb799lt9F4OIiIiIiIiIjNBbtwyytLSUBg0ayOLFiyUiIiLd69OnT5ewsDBp2bLlG31erVq13igI0la4cGEpWbLkW70nq6SlpcmlS5fk0qVLcuXKlQwHsx41apRERkaq/+7evauHkhIRERERERGRMXqnqeWnTp0q8fHxUrFiRdm0aZPcvXtXTp06JYMGDZKRI0dK27ZtpV69eq/8jLp162Y6Zfvb/Ovdu/c7VfxjiY6OFm9vb/H29hYvL68Mgx5LS0uxs7PT+UdERERERERElBXeaTYxDw8POXjwoPTt21eaN2+uLre1tZWvv/5axowZ89rPWLZsmURGRr7Lr9dRsGDB9/6MD8ne3j7DFlNERERERERERNnBO4VBIiKlSpWSvXv3yoMHD+T69etibW0tvr6+YmFhkeH6efLk0RkY2tHRURwdHUVEZNKkSeLr6ytNmzZ97e9dsGCBmJubS7du3d616ERERERERERERuudwyCNIkWKSJEiReT27dsZjo+TlpYmly9flqlTp0quXLky/Iyff/5ZGjdu/EZh0K+//io2NjYZhkGaIOrWrVty7949EXkxY1lUVJRERETIgwcP5P79+3LlyhW5ePGi/PLLL5n+HnNzcxERndZLcXFx8vTpUwkLC5Nbt27JzZs35b///pNnz57J1q1bX1t2IiIiIiIiIiJ9e+8wSEQkIiJCXF1dZdWqVdKhQwed1y5duiSlSpWS7du3S/369T/Er8tU7ty5pWHDhvLtt9/qzNZlZWUl9vb2UqhQIXF0dBRnZ2dp3rx5uinstRUuXFg8PDykS5cu0qVLF53XbG1tpWjRouLk5CTu7u7SpEmTj1UlIiIiIiIiIqIP6oOEQa8a+0czgHJcXNwrPyMiIkIuXbr02t8VFxcnNjY2mb6+bds2iY6OlpSUFDE1NRUrK6tMu669iqmpqZw9e1auXbsmycnJYmlpqXZ1e5fPIyIiIiIiIiLKDj5IGBQaGioiImfPnk3XMujff/8VEZFz5869crr5VatWyapVq97o9zVq1OiVr9va2r7R57yOpaWllCpV6oN8FhERERERERFRdvBOU8trAyCzZ8+W3Llzy/LlyyUpKUl9LTExUdasWSNFixaVBQsWSEJCQqaf88UXXwiA1/6rVq3a+xaZiIiIiIiIiMhovXcY9M0338jRo0dl586dYmpqKl999ZX62qRJkyQ+Pl4OHz4spqam0rNnT0lLS0tfCBMTCQsLk0uXLr3y38WLFyU8PFxMTN672ERERERERERERumdu4lFRETI4MGDZfny5bJ8+XKpVKmSrF69WurWrSsAxMXFRb7//nvZtGmTODs7y8aNG6VWrVrSvHlzmTdvnjg7O6ufVbt2bVmyZIls2LDhtb83V65cH3RaeQsLCzExMREzs/fvMWdhYaHOQkZERERERERElB0pyGg++NdITU0VNzc3ERFZtmyZ1KhRQ33t33//lfbt20t8fLwsX75cWrRoob524cIFad++vURGRsrt27dFUZQPUAXDFxUVJfb29lJdmomZwjCJiIiIiIiIiN5OCpJlj2ySyMhIsbOze+W67xQGiYgcOnRISpcuLdbW1ulee/bsmcTHx4uTk1O615KTk+X69evi5eX1Lr82R2IYRERERERERETv423CoHfuG1WpUqVMX8ufP3+mr5mbmzMIIiIiIiIiIiLSE47ETERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZETM9F0AY5SYmCiJiYnqz1FRUXosDREREREREREZE7YM0oPJkyeLvb29+s/Z2VnfRSIiIiIiIiIiI6EAgL4LYWwyahnk7Ows1aWZmCnmeiwZERERERERERmiFCTLHtkkkZGRYmdn98p12U1MDywtLcXS0lLfxSAiIiIiIiIiI8RuYkRERERERERERoRhEBERERERERGREWEYRERERERERERkRBgGEREREREREREZEYZBRERERERERERGhGHQB5aamiqPHj3SdzGIiIiIiIiIiDLEMOgD69u3rxQpUkQOHjyo76IQEREREREREaXDMOgDK1q0qOTJk0fs7e31XRQiIiIiIiIionQUANB3IYxdVFSU2NvbS3VpJmaKub6LQ0REREREREQGJgXJskc2SWRkpNjZ2b1yXbYMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIyImb4L8CYSEhIkJSVFREQsLS3F3NxczyVKLykpSZKSkkRExMzMTKysrPRcIiIiIiIiIiKi9AyiZVDu3LnF1tZWbG1tpXfv3vouTobq16+vltHHx0ffxSEiIiIiIiIiypBBtAwCICNGjJDGjRtL0aJF9V2cDM2bN0+ePXsmCxculL179+q7OEREREREREREGTKIlkEiIl5eXvLJJ5+Im5ubzvKkpCSZNWuWBAcHi7W1teTKlUtcXFykdevWarctDU9PT1EUJdN/Z8+e1Vnfx8dH53VTU1NxcHCQ5s2by8GDB3XW9fb2zrB8RERERERERETZiUG0DMpMTEyM1KpVS86fPy9du3aVL774Quzs7OTWrVty4MABsbCw0Fk/MTFRqlSpIsOHD0/3WYqiSIkSJXSWxcXF6awPQG7duiVLliyRqlWryvLly6Vjx44fr4JERERERERERB+YQYdBEydOlNOnT8u+ffskODhY57WBAwdm+J4iRYpI48aN3/h3ZLT+559/LuXLl5f+/ftLixYtJHfu3G9ddiIiIiIiIiIifTCYbmIZ2b59u9SoUSNdEPSxmZubS58+fSQ8PFwOHz6cpb+biIiIiIiIiOh9GHTLIBMTE3n+/Llefrerq6uIiDx69Oit35uYmCiJiYnqz1FRUR+qWEREREREREREr2TQLYNCQkLk2LFj8vXXX2f57w4PDxcRkYIFC771eydPniz29vbqP2dn5w9dPCIiIiIiIiKiDBl0GDRkyBDp3LmzTJw4URo1aiS3b99+7XtSUlIkJiYm3T/tljpvYuXKlWJvby+VK1d+63KPGjVKIiMj1X937959688gIiIiIiIiInoXBh0GmZiYyLJly2TRokVy5MgR8fLykrFjx0pCQkKm79mwYYPY2tqm++fp6Znh+prwKDIyUu7cuSP79u2TNm3ayJ9//ik//vjjOw0ebWlpKXZ2djr/iIiIiIiIiIiygkGPGaTRvXt3adq0qXz11VcyceJE2bBhg2zcuFFKliyZbt1atWrJuHHj0i23sbHJ8LM3bNggGzZs0Fnm5eUlGzZskJYtW36Q8hMRERERERERZZUcEQaJvBi7Z+HChdKjRw9p3bq1VKtWTc6dOyf58+fXWa9AgQLyySefvPHnaodH5ubmUqRIEY7xQ0REREREREQGK8eEQRrBwcGyY8cOKVu2rIwfP15mzZr1Xp/3tuEREREREREREVF2ZtBjBmXGx8dH/Pz85MyZM/ouChERERERERFRtpIjwyAAcuvWLSlUqJC+i0JERERERERElK0YdBiUlJSU4fLp06fLs2fP5NNPP83iEhERERERERERZW8GPWZQnTp1xM7OTurWrSuOjo7y7Nkz+fPPP2Xz5s3yxRdfSNOmTdO958GDB7J169YMP69MmTLi5OT0sYtNRERERERERKQ3Bh0GdezYUdasWSPjx4+XiIgIsbW1laCgINm0aVOGQZClpaXs379f9u/fn+HnzZo1S7788kv1ZwsLC7GwsPho5SciIiIiIiIiymoGHQb16tVLevXq9cbrX758+a0+/8qVK29bJCIiIiIiIiKibM1gwqCTJ09KgQIFxM3NTUqVKqXv4qRz7NgxefTokVy8eFHfRSEiIiIiIiIiypQCAPouxOuYmZlJamqqiIj06NFDFi5cqOcSpVe7dm35559/RETE3d1drl279sbvjYqKEnt7e6kuzcRMMf9YRSQiIiIiIiKiHCoFybJHNklkZKTY2dm9cl2DaBmUkpKi7yK81q5du/RdBCIiIiIiIiKi1zLoqeWJiIiIiIiIiOjtMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIiIiIjAjDICIiIiIiIiIiI8IwiIiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDiIiIiIiIiIiMCMMgIiIiIiIiIiIjwjCIiIiIiIiIiMiIMAwiIiIiIiIiIjIiDIOIiIiIiIiIiIyImb4LQCIAREQkRZJFoOfCEBEREREREZHBSZFkEflfxvAqDIOygejoaBEROSB/6rkkRERERERERGTIoqOjxd7e/pXrKHiTyIg+qrS0NHnw4IHY2tqKoihZ+rujoqLE2dlZ7t69K3Z2dln6u/XJGOttjHUWMc56G2OdRVhvY6q3MdZZxDjrbYx1FmG9janexlhnEeOstzHWWYT1zup6A5Do6GgpUqSImJi8elQgtgzKBkxMTMTJyUmvZbCzszOqnVPDGOttjHUWMc56G2OdRVhvY2KMdRYxznobY51FWG9jYox1FjHOehtjnUVY76z0uhZBGhxAmoiIiIiIiIjIiDAMIiIiIiIiIiIyIgyDjJylpaV88803Ymlpqe+iZCljrLcx1lnEOOttjHUWYb2Nqd7GWGcR46y3MdZZhPU2pnobY51FjLPexlhnEdY7O9ebA0gTERERERERERkRtgwiIiIiIiIiIjIiDIOIiIiIiIiIiIwIwyAiIiIiIiIiIiPCMIiIiIiIiIiIyIgwDCIiIiIiIiIiMiIMg4iIiIiIsrGUlBQREUlLS9NzSYiIKKdgGERERERElE1FR0dL2bJl5e+//xYTExMGQkREGYiIiJD4+Hh9F8OgMAwiIiIiIsqmIiMjxdnZWZo1ayZ79uxhIERE9JLU1FRp2bKlBAYGSnh4uL6LYzAYBtFbA6DvInwUL19Y5dR6EvGpCRmL1NRUEeHx3FjllMDEyclJZs2aJU2bNpU6deoYfSBkrPWmF/j9U0ZMTU3lu+++k8TERGnatKlERETou0gGgWEQvTVFUeT+/fv6LsYHlZKSIiYmJpKYmChHjx4VkRf1JMpp/vvvPwkODpZDhw7xBplyPFNTU4mLi5Nx48bJvXv39F0cykKa83pCQoKcOXNG38V5Z5rjdMmSJWXChAnSokULqVOnjuzevdvoAqELFy7IiRMnjK7e9D85Zb9+FV6bvbtKlSrJihUr5N69e9KkSRO2EHoDDIPorUVHR4ufn58MHDhQ30X5IFJTU8XMzExiYmKkcePG8uOPP8rBgwf1XSyDlNkJjCe27MPExESio6Ole/fucuTIEX0XJ8sZww1EUlKShIaGysaNG2X79u2SlJRk1PvggQMHZMKECeqNgzFsA5nR3g4A5NjtQvu8Xq9ePRk8eLD8999/+i7WO9F+MOXh4SGTJk2S9u3bS926deXAgQNGE4wkJSXJxIkTpWHDhnLq1CmjqbdI+muozOqdU/dnjZy0X2ckISFBRPgw+n0FBwfLqlWr5N69e9K4cWMGQq/BMIjeiObEExMTI9HR0dKhQweZPXu2TJkyRc8le3vaJ1EAYmpqKjExMVKhQgWJjo6WTz/9VAIDA/VYQsOUlpamtho7c+aMHDhwQGJiYkTkxYktp1+kGAovLy/ZuHGjWFpaSqdOneTw4cP6LtJHlZycLLdu3ZIbN25IfHy8mJjk7NNedHS01KtXTzp16iTt27eXZs2aSYUKFWTu3LlG2z2wbt268umnn8rQoUPl8ePHOX4byExKSoooiiKpqamSkJAg4eHhOeamI7PzelBQkACQ3r17i7u7ux5L+H40M4mlpKRIamqq+Pj4iLOzs9SuXVuOHj2a44ORhw8fys2bNyVXrlxibW0t7du3l7Nnz+b4eicnJ4uI7jWUpmVMUlKSnD17Vk6cOCG3b99Ot15OkNP3axGR2NhYGT16tDRo0EDq168vXbt2ldOnT0tsbKyI5PyA70PTnOcqVaokGzdulLt370pISIg8e/ZM30XLvkD0GqmpqQCA0NBQ1KlTB25ubvDw8ICiKFAUBWPGjNFzCV/v+fPnWLNmDSIjI9O9lpKSgg4dOqBcuXI4ffo00tLS1OXaNMspPc02cujQIbi6uqJAgQJQFAVly5bFpEmT0q1H+nf69GmULl0a7u7uOHTokL6L81FER0ejSZMmcHJygoODA0qXLo19+/YhISFB30X7KGJjY+Hv74+KFSvijz/+wIULF3DgwAHUqVMHRYoUQZcuXZCYmKjvYurFtm3b4OnpiZ9//hmA8R2LkpOTAbzYJ9q0aQMvLy/Y2tqid+/e2Llzp55L924eP36MpUuXqj9rf6cpKSno2rUrgoKCcOLECfX8rfk7GNL3r7kWiYqKQrVq1RAYGAh7e3uULVsWiqLAwsIC+/btA2BY9XpThw4dgre3N8qUKQMHBweULFkSiqLAw8MDx48fB5Az6x0REYFu3bph3bp16jLN8TsqKgrVq1eHs7MzFEWBj48PfvjhB30V9YMylv0aeHE8LlWqFPz9/dG6dWt07twZRYsWRaFChfDtt98iLCxM30U0KJrtICYmBgsXLkS3bt1QsWJFKIqC5s2b49mzZ3ouYfbEMIheSXOgDQ0Nha2tLTp06IA1a9bg9u3bWL16NZo1awZFUTB8+HA9lzRzaWlp6NatGxRFwbJly9K9Hh4ejnLlymHcuHEA/ncw0X4/vd7ly5fh5OSEjh07YvPmzTh//jyaN28Ob29vDBs2TF3P0E7WOUVGf3dNIFSiRAkcOnQoR23rMTEx8Pb2RunSpTFhwgQsWLAA9evXR+7cuTF79myEh4fru4gf3Lx58zL8Lu/fvw9FURASEpKjL4ZePnYDutt9s2bN4Ofnp/6ck7b3V9HUMzo6Gj4+PvD390e/fv0wZcoUeHh4oFKlSpg3b56eS/n25syZA0VRMGvWLHWZpq6JiYkIDAzEwIEDM3zvy999dt8WNEFv5cqV8eeff+LRo0eIi4vDnDlzEBAQAHNzc+zevRtAzjrHnjhxAra2tujWrRv279+P+Ph4REVFYdKkSShVqhRKlCiB0NBQADmr3gBw8uRJlCpVCrVr18amTZvU5ZGRkfD19UXZsmUxf/58rF27Fj169ICpqSm++OILPZb4wzCW/To5ORlNmzZFUFAQTp48qfNao0aNYG5ujiVLlmTrOmQn2qF5mTJlEBwcjDp16mDMmDEoUaIEFEVBjRo18Pz5cz2XNPthGESv9eTJEwQFBaFRo0a4ffu2zmv379/H9OnTYWZmlq0DoWvXrqF79+6IiYlJ99rz58/h6uqKwYMH6yzfs2cPxo8fj44dO6JPnz64ceNGVhXXIC1ZsgTBwcG4fPmyevIKCwvDoEGDUKpUKQZCenDx4kXMmTNH/Xtn9Hc/ceIEvL294e3tjUOHDuWY7+bzzz+Hn58fjh49qrN80KBBMDExwfTp03NcINS7d28EBgYC+N9F8NOnT+Hv74+goCBcunRJn8X7qDT1jYmJwfbt23WeqGqept+8eRMODg74+uuv9VJGfdD8XVJTU9G1a1eUK1cOR48eVS+cT506hZCQEPj6+mL27Nn6LOpbCwsLw9dffw1FUTBjxgx1eVJSEh4+fIiiRYtiwoQJ6vLU1FTs3LkTo0aNQq1atTBw4EBs3bpVDyV/e9999x0KFiyIf//9N12r5ePHj6Nx48YwNzfPUS2EoqKi0Lx5c1SsWBHXrl1L9/rWrVtRu3ZtlChRIse2EDpw4AAqV66MGjVqqIFQ//794evri4MHD6r1ff78OZYsWYJcuXKhV69e+izyezOW/frJkyfw9vbGjBkzdB5kpKWloVSpUqhevTrvO95SQkICPvnkEwQFBeHo0aPq/hEWFoZFixahSJEiqFKlCgOhlzAMotc6deoU7OzsMHfuXHWZ9sVIfHw8Zs+eDUVR1NY12V1ycrLaVSQ2NhbdunWDr68vunbtiq+++gqenp7w9PREsWLFEBAQAHt7e7i7u6s3U0zq0/8NBg8ejDp16qg/a05uT548YSCkB6mpqejRowdKlCiBadOmZRoIpaSkYNWqVTA3N0dwcLDBtxCKj49HZGQkOnfujF69eqnHKu1j1tChQ3NkINS1a1dUrFhR/TkiIgLe3t4ICgpSb6YM6cnp20pOTkalSpXUVlDaT5aBF0HRgAEDULVqVVy5ckVPpcx6ycnJiIqKQunSpTF69Oh0+8SlS5fQqVMngw2Evvrqq3Q3jgAwZMgQmJubY8iQIejfvz/c3d1RqlQpuLu7o379+nBwcICrq6tOq4vsqmXLlvD09NRZpr3vhoaGomzZsrC0tFQDcEM/x4aHh8PV1TVduKFdr+3bt8PLywseHh44e/YsgJx1TAOA/fv3o3LlyqhZsyZWr16NKlWqYNiwYenqmZycjNWrV+eYQCin79cnTpyAoig4ePCguiwyMhJeXl4IDAzMMADNadv2h7Zr1y7Y2tpiwYIF6j2I5r+JiYnYuXMnHB0dUaNGjRzdSvptMQyi19q7dy8URcGGDRsAZHwwunfvntov8+UDd3YTHx+PatWq4fvvv0d0dDQA4L///sOQIUNQpkwZhISEYPTo0bh69SqSkpKQlpaG0NBQ5MmTB3379tVz6bMHzcXYrVu3sGvXLmzbtg3Tpk1D9erVER8frx58NdtKdgqE0tLSdFov5WRPnz7Fp59+ijJlymDq1KmZBkLPnz9HuXLl1LEHDhw4oI/ivreUlBR06dIFbm5uKFiwoDrugHbrCI2cGAgNHz4cDg4OSElJQUJCgnpRefXq1XTrao7nOU3Pnj2hKAq6d+8ONzc31KlTB7t370ZUVBSA/3V5njp1qp5LmjVSUlIQEBCA1q1bw8/PD8eOHQOQvkvdpUuX0LlzZ4MNhEaPHp3u+iM8PByjR4+Gv78/OnXqhLFjx+LevXtqC+FDhw7B2toaI0aM0FPJ30xCQgIaNGiAkiVLIjIyUufcpf3/s2bNgqIosLS0zBEthC5fvoy8efOqrba1x3rTrvfIkSOhKArKlSuHCxcuZHk5s4ImEAoMDISJiQn+/vtvAC9ay2jLaYFQTtuvtcfsu3TpEmxsbLBq1SoAL67XvL29Mz1n0+stWLAAiqKovVhePv4lJibi559/VscQevr0qT6Kme0wDKLXOn/+PExMTNQTcmYXF9OmTVMHldYe/C078vT0hL29PWbOnKneJCQlJWU6sGxsbCzc3Nzw2WefZWEpsyfN93/48GG4uLjAyckJiqLA1tYWDg4OuHnzJoD/PXXWDoQGDx4MX19fvZ6kd+zYgfr16+fI5rfaF8iai8Rnz54hJCQEZcqUwQ8//JAuENLcFP7www8YMWIE/P390apVqywu+YezbNkyNGnSBIqiYMiQIQDSD0CpoQmEZsyYkSOaDV+/fh158uRBu3bt4Ofnh+Dg4Ay7hi1duhQeHh64fPmyHkr5cWi+49u3b6Nq1aoYMWIEjh49ivLly8Pf3x8tW7ZUWwP9+OOPKFSokBqM5HSLFi2CjY0NFEXB5MmTM11POxCaM2dOFpbw3WjfCN+7dw99+/aFoihYtGiRznqvGjDew8MDbdu2/Whl/FA0rSQ0g/2/3K0E+F9LwPLly0NRFDUQMmT16tVD8eLF1eO29rFc8/8nTpxAwYIFUaZMGfTs2TPD4QAMlfY5/cCBA+pD17Fjx2a4DqAbCPXp0yfLyvqh5NT9OioqCiVKlNAJtoKCglCrVi1ERUXB19c30yBo8ODB6Vq6Unpr1qyBoihYsWJFpg98Hz9+jGLFikFRFNSvXz9HXPu9L4ZBpEOz86SlpensSJ9//jly586NEydOANC9odJclKxbtw6urq4ICQlB48aNERERke1aX2iXu1KlSrC2tsbMmTPVFkLatC+2QkND4enpiWnTpgFgU81Lly6hSJEi6Ny5M/bv34/IyEiMGzcOhQoVQkBAAOLj4wFkHAgNGTIERYoUwfz58/VS9j/++AOKoqBnz556+f368PTpU7Rv315tIaT5XjTbeEpKCho0aIDBgwcjJSXFIJ8ma5f5999/R0BAAPLmzasOzKi972v//4gRI6AoChYuXGgw+3ViYiIuXbqErVu34vTp0+rNT3x8PMaNGwc7Ozvkz58fjx8/Tvfey5cvo2nTpuoxOqeJjY3FgAED0KxZMwAvvuvZs2ejZs2ayJUrFyZNmoSFCxeiefPm+Oabb5CcnGww3/v72LRpE3LlyoXAwMBXhmCXLl1Cly5dUKxYsXQ3X9mJ5tgVFRWFfv36oU6dOnB1dVUfSGmfX7RvLrX3/dDQUBQvXjzbtBLLaDvU1PPmzZvw8PCAn5+f2rrg5dZdUVFRyJcvH6ZPn44vvvgCFy9e/PiF/kg039OyZctga2uLvn37ZhgIAS8eWObOnRshISEoVqwYHj58mOXl/dAyq+uxY8dQoUIFeHt7448//lCXZxQI/frrrzoPRQxBTtyvgRffT4cOHVChQgX8888/aj0XLVqE/Pnzw8bGBuXLl8fDhw/TjQl28OBBVKlSBTNnzkz3Gum6e/cu7O3t0bp16wwnlNBsMwMHDkTLli2RP39+NGjQIEuvASIiIrBo0aJs1SKdYRABSN9KQPOz5sBz5MgR+Pr6olixYurT5JdPUiNHjkSDBg2wcuVKWFtb4/r161lV/Exl1Jxa+2lCxYoVYW1tjVmzZqk3VGlpaTp1u3jxIlq3bo1ixYqprV6MkfbfcseOHfDz88OpU6fUbSQuLg5Lly6Fp6cn6tWrl2kg9PjxY/zf//2f3k5qjx8/Rs2aNeHs7Iz//vtPL2X4GG7fvo3169djyJAh+O6777B69Wqd7+zp06cICQlB6dKlMW7cOJ3v5eLFi6hUqRL+7//+T13fEAMhbZs3b0apUqVQoEAB9Vikvc1p1+/bb781mBunqKgo1KpVC+7u7lAUBXZ2dvDw8MCOHTsAAA8ePEDPnj1hZWWFrl27Ijo6Wq330aNH0aZNGzg4OBj0tv/yMUVD851evnwZtra2OlMtx8fHY9KkSShbtiyCg4NhZWUFd3f3HNVMPKN9VvtvtHbtWlhYWKBly5av3N4vXLiAL7/8Mtu2ntTUUzM7WnBwMMaNG4d169bhu+++U1uraj9JT01N1flbXLlyBa1bt4arq2u2qKd2MB8ZGZnuAVVSUhJ++ukn2NnZoVq1amogpN3Sc9++fShdujQuXrxocAFnTEwMnj59iri4OJ3lUVFRaNeuHRwdHfHdd9+p9dXc1CUlJWHu3LmoUKECnjx5giJFimDFihVZXv4PSXt67F69emHt2rU6rx86dAiVKlVCzZo1dcbFefk7T0lJwebNmw3m3JYT92vgf13D+vXrh5kzZ+ocp58/f44uXbogb9686NChQ7r33rhxA23atIGXlxdu3bqVZWXOzl537zBjxgwoioKJEyfqLNf+u9erVw8dO3bE+fPnMxyb6WPSPIDMTkOqMAwidQc5c+YMPv/8czRp0gRdunTBqVOndNZbsWIF3N3d4erqiuPHj6sn4+TkZBw7dgw+Pj4YNmwYnj59igIFCuiliXJGLZY042doDxamnRhnFAhpzJ8/H1WqVIGNjQ1Onz79kUuf/V25cgX16tXDp59+ipYtW6rLNX/3xMRE/PLLL/Dx8ckwEMpo8GJ9OH369Gu7TBiSw4cPw9vbG05OTihWrBjs7e2hKApKly6NHTt2IDIyEsCLLmOffvopSpQogaZNm2LHjh2YNWsWgoKCUL58+QyfpGRn8fHxWLduHX788UesW7dOrafGli1b4O3tjQIFCqgn/MxaCBmC2NhY+Pn5oXLlyli1ahXu3r2LZcuWoXnz5rCyssKUKVMAvAgGR44cCRsbGzg5OaFmzZooX748XF1d4eTklCOOZXFxcahbty5mz56t0xVOc2MwZ84ceHt7q7MMaRw5cgQ//vgj7OzsoCgK7ty5k6Xl/lg0+25CQgL++usvrFy5EocPH0633po1a2Bubo7WrVu/8ibx5bFIspvk5GS0bNkSpUqVSnetcvr0abVrSUZdK6ZPn47g4GDY2tpmi31BcxyKjo5G586d4e/vj8DAQHz66adqN3bgRTgwfvx42Nvbo3Tp0jh16pQaGp05cwYhISHw8fHBo0eP9FKPdxUaGoqGDRvCxcUFn3zySbqxGZ88eYLq1avD0dERn3/+uU63jn379qljPcbFxaFgwYJYuHBhVlfhnWV2TRQVFQV/f39UqFAB06dPR0pKik7ooRlDSHuWMcDwW63npP0a+N84hjVr1kT9+vXx77//AtDtfREWFobOnTsjT548qFWrFjZt2oRjx45h+vTpqF+/PvLkyYMzZ87osxrZhuY8FxcXh+XLl2PEiBGYMGEC1qxZo65z584ddOvWDYqiYNKkSTp/69TUVFy+fBk1a9bEvHnz9FKH69evo2/fvtlqXCiGQUZOs4McPnwY9vb2CAgIwCeffIKAgABYWVlhzpw5OhcWy5Ytg7+/PywtLdGgQQMMHz4cbdu2hYeHB8qXL4/U1FSsWrUKRYsWxf3797OkDs+ePdO5qU9JSVEPGJqLqzJlysDPzw+DBg3KcAR57UAoNjYWwIu+yvXr10dQUBDOnTuXJXXJ7vbt24e6devCysoKNWrUQFJSknrToNmWMguEslvQMG3aNINuHaFx/Phx2Nvbo2PHjti5cydSUlJw584d/PTTTyhfvjzy5cuHJUuWqDcNERER+Oabb9SWJYUKFUL9+vXV79FQApKoqCgEBgbCzc0NVlZWKFy4MMqVK5eua9TrAiFDMmvWLHh5eeHQoUM6A1HeunULiqKgU6dO6s1jREQEDh06hBYtWiAoKAiVK1fG119/nS1abH4IN27cQLVq1WBnZwdPT08sW7ZMJ8w/ceIEqlWrhunTpwPQHbgTeNGCKqe09NS+gaxSpQqcnJxgbW0NU1NTDBkyJN13nlkgZEg3kteuXYOzszMGDBigllv7HHP79m307t0biqJgwYIF6vK7d+9i4MCBKF++fLY4r2vKHh0dDW9vb3h5eaF79+7o27cvXF1dUbJkSRw+fFintcj8+fPh6ekJS0tLBAQEoGLFiihRogQKFSpkcDeNR44cgY2NDapVq4bBgwejW7duKFq0KMqWLYt79+6p6z158gQdOnRA3rx5kTt3bgQHB8PHxwdFihRBcHAwkpKS8O+//8Ld3V3tGpydabeAejkQio+PR6VKlVCpUiUcP34805aQOTEQyin7tUZycjLmzp2Lhg0bIm/evPj2228BpH9A+vTpU/zwww/w8PBQu8RZW1ujcuXKOH/+vN7Kn51o/lZRUVEoW7Ys3N3d4eHhAQcHB1hbW6NGjRpqa7BLly6he/fu6syi69atQ0xMDDZu3Ig2bdqgcOHCeg1jsts1KMMgwvXr1+Hm5oaOHTuqXcDi4+PVAQvHjx+PsLAwdf2zZ89i3Lhx8Pb2RqFChVCxYkUMHjwYycnJePToEWrWrInmzZtnOA7Px7BgwQLkzp1bHeBaIzo6Gl5eXvD19UWXLl3Qq1cv2NnZITg4GAcPHkx3AtYOhDRlv3fvXobjbhizgwcPomHDhrC2tlafvrw8e5gmECpVqhQaNGiQrul3dmDo3aCAF7Nq1K9fHw0aNEjXhDgtLQ1hYWGoW7cuChYsqNNKQDPV9P79+3HlypV03USzu5iYGPj7+yMoKAhbtmzB06dPsX79etjb28PT0/OVgVBGXcYMRadOnRAUFKSzLDIyEj4+PqhYsWKm3UPi4+NzxPaeUd1WrVqFVq1aqYNBao9zM2XKFOTJk0c9f2U2m54h0+5aoRmAdP369Thz5gwWLVoEU1NTfPnll+kGC3/TFkLZxcvf2cmTJ2FiYoK5c+cCyHjbOHnyJBwdHaEoChYvXqwuf/DgQbYYNFT7aXWbNm1QtmxZhIaGqq/funUL1atXR9GiRbFv3z6dsd7u3r2LcePGISQkBA0bNsTYsWOz1ZPmN/HkyRP4+fmhVatW6nE5LS0Nhw8fhr+/P3x8fHRa7sXExGDfvn0YNGgQWrZsic6dO2P+/PlITU1FeHg46tWrp3YXy86ioqJQs2ZNdOrUKcPXN23aBBcXF+zYseO1x6y9e/eqgdDmzZs/Wpk/lpy4X78sPj4eixYtgre3N1xcXNSxYl4OhFJSUhAfH49Nmzbht99+w6lTp7JlffQpISEBlSpVQoUKFdRZbx89eoQ//vgDRYsWRalSpdRr4bt372LevHkoUKAAzM3NoSgK8ubNC09PT4MLzT82hkFGIKOZFbQPsEeOHIGzszP27NmT7sA8ceLEDAMh4EWXBe3xFm7cuIFmzZohf/78WTq9Z1hYmNqfeMCAAeryunXrIiAgAEePHlWXXbt2DSVLloSPjw92796dYSCUJ08efP/99zlqRooPQfvm+ciRI6hUqRIcHBxw9+5dABkHQkuWLIGjoyOGDh2a9QU2AteuXYODg4PO2CgvXzzdvXsXPj4+qFKlyis/y1BukJOTk9GpUydUqFABhw8fVscAW758OSwsLGBubg4XF5d0XSW2bNkCX19fmJqaGmSLkKSkJLRu3VpnpjfN7EGBgYGZ9ns35KfEGi8HdxmNK7Jt2zYEBATAwcEBtWrVwokTJxAeHo42bdqgU6dOr5x5xtAlJSWhZcuWCA4OxqFDh9TvfO3atVAUBSYmJujSpUu6bWTNmjXInTt3tp9dUbsLnKZuFy9ehKWlJTp27IjU1NRMj19jxoxRn7RPmDAhy8r8Opp6JCUlITY2FrVr18a4ceMyHBS6Vq1aaiBkKMfp13n27BkuXLgAX19fbNy4Md3r586dQ5kyZdIFQhraLf2OHz+OJk2awN7ePlu1CsnMgwcP1C5xGXXH/PHHH1GgQIEM33v48GHs378fz549U7eFvXv3omrVqggMDFTHjjMEOXG/Bl6cr548eYInT57oTO6wdOlSODo6Ijg4WO2BYIgPpfTpn3/+QYkSJbBlyxadbePJkycoWrQoPvnkk3THi3v37mH79u2YP38+9uzZgwcPHmR1sbM9hkE53OnTp+Hr64tdu3ale+3y5cuoVasWOnXqpHOj+PIAytqBkPYTd+11xo0bh+DgYDg7O6fr55sVHj16hAkTJsDJyQkDBw7E06dPUaZMGcyZMyfdzBsxMTHw8vKCt7d3hoGQj48Pihcvnq1Ges9q2oM0Ztaq5+jRowgODkbhwoXV5twvB0IJCQnYsWMHT3gfmGab3rZtGywsLLBnzx4AGbfsSUlJwbhx45ArVy4cPHgwS8v5MVy7dg2ffPIJZsyYoV5Ib9y4ESYmJvj666+xYsUKFClSBG5ubumeEP/xxx+oWrWqOsV4dpeYmIizZ8+qP3/99dews7NDfHw80tLS1CAooxYBGzduzBEBiGabjo2NxZQpU9CoUSN4enqiRYsWmDRpEpKTk9Xj1Z07d7BixQqUKlUKjo6O6NixI/r164emTZsaxE3iuzp+/DhKlSqFJUuWqPuEZtbE7777DnPmzIGJiQm6deuWbttfsWIFChUqpNMlJzuKjo6Gp6enzoOFL774Arlz51ZnR9M+z2j+DsuWLYOfnx+GDBmCggUL4tmzZ9kmIE1KSkLHjh1Rt25d2NraYu/evQDSB7gvB0LZpfzv6vr166hRowaaNWuGfPnyqQ8aX65XRoHQyy1lPvvsMwQEBMDLy0vnWJndPXjwINNrq127dsHU1BQrV67E48ePceHCBXTp0gXVq1dXw92yZcti5cqV6ja/e/duNG3a1ODGQMtp+3V0dDRCQkLg5+eHwoULo3Pnzjh06BCAF4HQkiVL4OXlhbp166rfP6+P39z8+fORJ08enRkDw8PD1QHHX74Wyinh+cfGMCiHW7duHczNzTFixIh0r23duhXVqlVD/vz54eXlhadPn2Y62452IPTyDdaFCxfQsWNHfP7553q9yXr48CEmTJig9je3t7fH7du3ASBd3+OYmBh4enpmGggZy6j9R48exe7du3X6pGv+FidPnkS7du3g7++PRo0a4aefflLH/9F+/+sCIQ2e8D6M48ePY+TIkQBeXCxbWFioAwdn5uzZszAzM8Ovv/6aFUX8KDRNqC9fvoyQkBB1sOjQ0FC4urqid+/eiImJQVpaGmbOnAlFUeDt7Z3ueKV5IpfdRUdHo169eujRo4c6CPI///wDJycn9O3bF56enggODk7X/QcA5s6diyZNmhjcjcHLtMfCKVOmDLy9vVGtWjX07dsXJUqUgKIoqFChAnbu3KkTfKWmpmL06NGoUaMGTExMoCgKxowZo69qfHQnT55E586d1X3i0KFDcHJywhdffKE+1Pj222+hKEqGU45nVZfut6V9Xv7ss88QHByMffv2qeeYw4cPq+PGaAYRf3l2oe+//x4VKlTAw4cPs93McXFxcejXrx8qV64MCwsLddaojEL9lwMhQxYREYGmTZvCy8sLefPmVW/gMrpG0ARCpUqVyvB4Nn36dIwbNy7bt/Z89uwZfvrpJ/Vn7W00NjZW7RYFvAi1e/bsidy5cyNPnjxwcXFB1apVMWXKFPz555/YvHkz3N3dUaZMGZ2Hli9fn2VXOXW/1oz75e/vj1GjRmHw4MHw9/dHyZIlsX//fgAvvqPFixejVKlSDIRe8vJMgdrHQc13/9NPPyFfvnxqw4Tnz5+/tnU0vR7DICOgSaUzsm3bNtSqVQtmZmbYtm0bgMynX544cSJMTU0xatQonRkugBc7ZFaPCxMWFoakpKR0M4hNmDAB7u7uyJs3r3rxkNEsY7GxsWoLoYy6yBmD8uXLo1ChQti5c6fOgffw4cOwtbVFcHAwunXrhsDAQLi7u6NVq1bpbhxeFQjRhzd//nwoioJjx44hMjJSDQU0wac2zQn00qVLBh0GRUdHo2fPnvjtt98A/G/7io+Px6BBg+Dh4ZHuOFexYkUoigJ3d/cM/zbZmWZq3aCgIKxbt04n6OjUqRMURYGbm5s6UKp2V4OrV6+iUaNGaNKkSbrZ1QxRfHw8KlSogEqVKmHfvn1qXR8+fIg//vgDJUuWhLu7O3bu3Im0tDSdY8/Vq1cxa9YsuLi4GFSrgbeVkJCg/l1iY2PRpUsXlCtXTn2yDgD//fcfXF1dYW5ujv79+xvMoKQJCQl4/vw5hg4dipkzZ6Z7fcWKFXBzc0PRokXTDRx88+ZNNGzYEJ06dcoW5/eMWi7ExMRg3LhxsLGxQenSpV/ZfSQqKgr16tVDrly50s2SZ2giIiLUAV4bN26cbuZRbefOnUO5cuVQoEABdQIQ7b9ldr/eSE1NRa9evWBpaYnvvvtOXa6p6/Dhw2FmZqY+5AFetHY/efIk/vnnnwxbNf7+++9QFMVgW4rlpP0aeHGeqlatGoKDg3WOuwMGDICiKHBxccHu3bvVdRkIZezs2bMYO3as+pArMjJSp2fLyZMnUaBAAcydOxcRERHw8fHJtHX0vHnzODbQG2IYlINldJA8c+ZMuifJO3bsQFBQEKytrXHixAkAmQdCo0ePRtWqVfV+8nn06BFy586N0aNHA/jf6PJHjhzBkydPMGHCBFhYWKBr167qezILhHx9fVGkSBEcOXIkayuRDaSkpKB8+fIoXrw4/v77bwAvxphxdXVFu3bt1ANsUlISZsyYAT8/P9SqVSvDQKhChQooWrSowd14G5orV66oM+MBL1qBKIqCyZMn69z8a24OU1NTMW/ePJQsWdIgb4ijoqJQqlQpeHt7Y/Xq1Tr7cWJiIjw9PdG6dWt1mebCauTIkWjXrh0qVqwIb2/vbH/DoJGSkoLWrVsjICAAp06dUo+/2oFQ/fr1YWFhgWHDhukMMBkaGop27dqhUKFCBjEo8KtozjFLliyBq6srtm3bpn6HLwc+Hh4eCAoKUv9GL3/XhtIa7HUyu/HRPh8/f/4cBQoU0Bk/T6Nbt26oWrUqFEVBnz59sv308SkpKejTpw+aNWuG8uXLq9MHa08VDEDtGpgrVy706dMHq1evxrRp01CnTh3ky5cvS8cwzIxmm8xoPBHtKePr1av3ypvD6OhotGzZ0mC6umpERkbi/v37uHr1qjrTbHR0NHr16oV8+fKhT58+6v6bWSDUrVs3g71h/u+//9CwYUM4OjqmG+Pm1q1baNmyJQoXLoxhw4Zl+hnax7UffvgBDg4O2b5FVEZy0n6tsWTJEnVCC802umHDBiiKgvbt26Ns2bJwcXFRu4LGxcVh8eLF8Pf3R8WKFQ2mVdfHNmPGDOTKlQvffPMNzp49CwcHB1StWlU9h0dGRuKTTz6Bp6cn3NzcEBgYiFu3bqW7J924cSNKlSqFf/75Rx/VMDgMg4xIVFQUgoODYWpqmu4g+vfffyMgIAA2NjZqCp9ZIKTZ6fQdCE2YMAGWlpYYM2YMihcvjsDAQPUCSRMIOTo66lwUZxQIxcTEoGLFitl6AM2PQfvitEyZMnB1dcW///6LBQsWwMvLK934MomJiWo/7YwCodDQUHh6eqJ58+ZZVgdjNWjQIBQoUEAdJLl3796wsLDAhAkT0t0kHD16FP7+/mjZsqXe99m3FRcXh6CgIAQHB+PEiRPpbvJv3bqF/PnzqxfQ2q0T27Zti1atWuHatWsGFVCGhYXB19cX06ZNA5D508KuXbvCxsYGhQoVQosWLVC5cmW4uLigaNGi6ix/OUGvXr3g5ub2yvGPduzYAVNTU52n7kD2OVd9CJptPzExEadOncLmzZszHQvQysoqw+2nZs2aWLhwIUJDQzPsXpgdzZs3D82aNUPRokUxduxYABnPCnf8+HEMHDgQ9vb2MDU1ReHChVG1atVsMVaU5rt71XgimhZCRYoU0ZmB01DDD21Hjx5FtWrV4OTkBGtraxQsWBDfffcdIiIiEB8fjx49eqBIkSLo06ePOh7eq+ptaH8TzXZ6+fJl1KtXD4ULF8b48eN11rlz5w6aNWsGBwcHnWEdXp5xCngx5lKjRo1Qs2ZNgx3bMifs1xqpqano378/WrVqpe63u3btgq2tLUaMGIHU1FSsWbMGjo6OKFasmHptHR8fj59++gnVq1c3+C7dH9K8efPg4OCAPHnyICAgQL2m1WwXly5dgoODAxRFwapVq9K9//r162jTpg2CgoJ0xhaizDEMMjK///47/P39kT9//nTNxHfu3ImAgABYW1tnGAhpX1Dr++Jac3E1ceJEmJiYwNnZWWdKVuBF66Hx48enm2Uso0DIWGkHQqVLl4aPjw9q166NqlWr6qynPfPJqwKhK1euGNyFmiHRXCg/evQIJUuWRO/evQG8aA2gaY5csmRJfPnll5g3bx66d+8Ob29vBAQEqN91dmlW/SZ++ukneHp6YseOHRne2KelpaFhw4ZwcXFRT/ppaWm4fPkyqlWrhtmzZ+ul3O/j/v37KFq0aLpg48iRI/jhhx/QvXt3TJw4EeHh4Vi7di1CQkLg7u6OypUrY+jQoeoUzYZOs502bNgQn3zyCYDMj9dPnjxB6dKlUa9evSwrX1bSHjupRo0a8PT0hKIosLS0hJ+fH5YtW4aIiAgAL44N+fLlQ6tWrXQCtIsXL6Jy5cpYsmSJPqrw1rT385UrV8Lb2xvW1tZqF4zMgr67d+/i0qVLuH//frru7Pqg+e5eNZ6IZgyg6OhofPPNN3B0dMwxgVBoaChsbW3Rpk0bLFmyBGvWrEHfvn1hYWGBevXq4fTp04iJiUGXLl1QtGhR9O3bVz3PGdK56lVSUlLU7/BdAiHt495ff/2F5s2bw87OzmC6emrLKfv1y27evKkeg2/fvo2aNWuicePGOg/nvvnmG7XLmKbFSkJCQo7ozv0haG/nDg4OsLCwQOfOndUH9drdwA8ePIi8efOiVKlSmDVrFp49e4aIiAj88ccfaNGiBfLly5etAsPsjmFQDqY5kcbExOg0QdyyZQt8fHxeGQhl1kIoO4mKioKjoyOKFy+uzpry8tPjNwmEAP2HW1khowGdtQeMTklJQVBQEBRFQfny5dM1W33TQEjzWfT+jh8/jnHjxqWboS8+Ph5ffPEF/Pz8dJ58rFq1CvXr14eDgwNsbW1RqVIlDB48OMPuNYagY8eOCA4OfuX2tHjxYjg6OqJKlSo4evQo1q5di9atW6NIkSIZ9iPP7qKiolC/fn1UqFABQ4YMwZQpU+Dr6wtPT084ODjAy8sLlpaWKF26tPpUWNNCzNC+3zfx1VdfwcLCItO+/5o6f/XVV3B2dkZkZGSOuYnUFhsbC39/f1SqVAnr1q3DhQsXsGvXLtSpUwf29vbo37+/ejMyd+5cmJmZYejQoTh16hR27tyJVq1awcnJKdtOjpDRwybtZWvWrIGvry8CAgLUi/zs9IDqVd5kPJF///0XQM4KhKKjo1GnTh00bdoUd+/e1XlNM6D58uXLAbw47uW0QGjUqFHYvn07gBf1eJ9AKDU1FStXroSfnx8KFSpkMK0/c/J+nZm9e/fC1tZWZ8Bw4EWg5e7ujvr168Pa2ppdmLRovuf4+Hg4OjoiICAAgwYNQuHChTF06FCdnhuaY8L58+fh6+uL3LlzI3/+/ChYsCAKFCgAb29vjhX0lhgG5VCaneXEiRMICQnBjh07dLpQvEkgZG9vj6NHj2Zpud9UWloaZs+ejSpVquDixYuYNm2aGgi9PD5EZoGQIV9kvI+dO3fqPK148OCBGiikpqaiRo0aUBQF69atS3dzqT0r2/Lly1GmTBmULVs2ywcPNwYpKSnqkyQ3Nzd89tlnuHPnjjrOxLVr12BlZZVu/IHo6Gg8efIEly9f1gn0DPFmolKlSujUqROAVwe4kyZNgru7OxRFgZ2dHUqWLGnQFwMXL15Eu3bt4O3tjWbNmmHw4ME4c+YMoqOjER8fj40bN0JRFHXMNO3ZAA3Ry+XW/lkzUOo333yjbvsZadWqFQIDAz9aGfVt7ty5cHd3x8GDB3X2hZiYGCiKgm7duqnhYFhYGMaMGQNzc3NYWlqiQIECcHNzy7b7hKY+KSkpiIiIwLFjx3D37l11sGCN5cuXIzg4GHXq1FG7uhvCNv+m44ns2bMHwItj+Lhx4+Ds7IwqVaoY7HgiYWFhcHJyUmfK0tT94MGDsLOzQ/fu3fH48WP1WiwqKgpdu3aFs7MzOnXqZNDhdkREBFq0aAFvb2+dsfo02+vFixffOhAKDQ3FrFmzDKb1Z07frzMzduxY5M6dG2FhYQD+N35jVFQUChYsiGnTpul99uXsRPthZVRUFDZv3oz//vsPwItZ4woXLozBgwfrjI+lec+TJ0+wevVq9OvXD7169cIvv/yijklGb45hUA6kObEeOnQI+fPnR4MGDfDnn38C0D3Avi4QKl68OOrWrZt1BX9LsbGxOoOnagKhiRMnpguENGMIubm5oUePHlld1GzjypUrKFu2LGrVqoW4uDicPn0aTk5O2LNnj043ooCAALi6umLHjh2vDIQWLVqE7t27G2TQYCiuXr2KgQMHwsXFBYULF0afPn3Up8ujR49G2bJldZrDZnQRZWgXVpoWa1WrVkXt2rV1lmvTHgD34sWL+Pvvv7Fv374c0U88Pj4eMTExGe5bMTExKFiwoDqIuCHTPu4kJCSoLQK0de7cGTY2NlizZo16bNd+37lz5xAUFISRI0emm4I4p+jevTuCgoJ0lj19+hT+/v4oX768OgWzRnJyMkJDQ/Hjjz9izZo12XZMCu3xdDp37oySJUtCURTkzp0bJUuWxJIlS3RaMy1duhQVK1ZE7dq1DeLG8V3HE4mOjsbIkSNRpkyZdK1qDMXVq1eRN29ebNy4UV126NAh5M6dG127ds1wvJuIiAi0atUKbdu2NfgHdrt370bdunXRqVMntfXmkSNH1NbUV65cyTQQunv3Lpo3b/7KLmPZWU7fr19lzZo1MDExwaZNm9RlKSkp2L59O0qXLo1Hjx4Z/Lb9obw8ntr06dMRHR2t8/eZPHlyukDIULeN7IphUA51+fJlODo6onPnzumahWvvZJs3b840EDp16pTB3ORrDgzTp0/PNBA6e/YsRo8ejYCAAPXEbCy0v/N58+ahWrVqqFixIqysrNC5c2c8ePAAgO4NVtmyZeHm5vbKQEjT1ezl30EfVmJiIp4+fYqBAweiXLly6sDpw4cPh7u7O9atWwcg530Hs2fPhoWFhRpmAxlfBOzevRtr167N0S3UNPtgWloaDh48CDc3N7UZuiFcGD158gSrVq3SuQHUhHmaG4agoCB4enrim2++0Qk4w8LCUL9+fdja2mLKlCk6M6UdO3YMISEhcHR0NMhugRnJ6IavS5cuqF69uvpzZGQkvL29ERQUhGvXrgEwjO1Am+Z4FR0dDR8fH5QrVw6jRo3CgQMHMH78eLWVaocOHdSZTgFg2bJlqFixosG0JHjX8URiY2Px9OlTvZT5QwgLC4OdnZ3agvFVQdCzZ8/U1gCxsbEZDiZsCGbNmoX/+7//U3+eO3cufH19sWbNGvTs2RNBQUF48OCBur2+LhBq1aoVLC0t072WnRnLfp2Z+/fvw8PDA9WrV1fHMr1w4YLa7V37IbYx0x4Lz8fHB/7+/lizZo3OWKYa2oGQdpexgwcP6kx2Y4jbS3bAMCiH0ewIs2fPhr+/v07Ak1mws3HjRvj4+KBAgQIZTtVoKIGQhnYgpGleHRoainnz5uHEiRPpmqjmVEePHsVXX32l/qx9UTVixAiYmprCyclJnc1E400DIdKPGzduYMaMGfDw8ED58uWhKAr8/f1z5AXG/fv3UapUKVStWlVngHjtbfn27duoVq0a6tSpk2MHYtSu7+XLl9GmTRsUL17coGZJ279/PxRFwcyZM3VuBKOiouDl5QUvLy90794dbdu2hbu7OwICAtRpeNPS0vDgwQN8+umnUBQFNjY2aNiwIQIDA1GqVCm4uLgYzBgar6N9gbxgwQJ1+bBhw5AvXz4AL7YHLy8vBAYGZhiAaaZqNgRJSUlo0qQJgoKCcOLECZ1tPTExEfPmzVO7U2nfBKxYsQJVqlRBcHBwulZR2ZmxjSfSt29fuLm5Ye7cubC2tkbXrl3TXYPFxsaiV69eGDNmjE6XOEMLgh4/fozGjRujXr16OHz4sLp8yJAhsLGxga2tLf7++291+ZsEQrdv30bXrl0NLug2tv36ZaGhoShUqBAcHBxQoUIFeHh4IH/+/DpdBulFC+jKlSsjKCgowwYI2tvNlClT4OjoiMGDB+P8+fO4efMm2rVrh+HDh2fYopjeHMOgHGr48OHw9vbWGdw3Pj4eT58+xcqVK/H333/rJPKbNm2Cr68vFEUxqBuMzGgCoQEDBuDnn39Gy5Yt4e7ujidPnui7aFkiISEBnTt3hqIo6W4Mjh8/DgsLC1SqVAlly5ZFo0aN1JOx5sDLQCj7efmJx8WLF7F27VpUrlwZ9vb2WLRoEQDDu4B+nWPHjqFgwYKoUqWKOhinxsmTJ9G2bVvkyZMn246H8iHNmzcPtWvXhrW1tUGGH5op4GfMmKGemxo2bIiAgACdsG/jxo1o1KgRXF1d090Ur127FgMHDkS1atXQtGlTzJw5U2csgZwgLi4Ovr6+8Pb2VltBXbhwAYUKFUK7du1QqlQpVKhQIcObpSVLlqBChQoGMx7F2bNn4eLighkzZmQ60P1PP/0ERVGwcOFCneWLFy9GkyZNsm0XuIwY23giV69eRfHixWFiYoImTZqoy7Vb/uzYsQO+vr6YOXOmvor5wezfvx8+Pj7o1auX+oDm66+/hqIoKFq0qLpMu1U18OpAyNAeyALGt19n5MKFC+jXrx9q166N/v37G3S49aFptv3Vq1ejZMmS2LRpU6YzyWlf0/7www8oXrw4nJ2d4e7uDjs7uwwbMdDbYRiUA2jvKJr/18wm8vvvv2PXrl1Yv349qlevjpIlS8LExARmZmZwdXXFypUr1fdu2LABn332WY654V+0aBHMzMyQO3duODg4GOTN0/s4e/asOnAj8L8D7LRp09CvXz88fvwYixYtQlBQEBo2bKh2NcgoECpXrhxKlCiBbdu25biwwdAlJiaicePGqFKlir6L8tEcO3YMJUuWhIWFBerWrYuePXuiWbNm8PPzg6Ojo1Hs22FhYQgICICPj49BP13866+/YGJigpkzZ2Lv3r0oUaIEFi1alO6GZ//+/WjatKnOTEvacsp5SkO7/vv27YOfnx9CQ0PVesbExGDQoEGwtbVF4cKF1QBM+31XrlxBw4YN0bRpU7VbUnbz8vnj119/haIoGU4DrH1T0LhxY7i4uKghikZ2nGb6VYxxPJFjx44hT548qFKlik64m5KSggMHDqB06dKoUaOGQYYeGVm7di1MTEywevVqDBo0CLlz58Zvv/2GcuXKITg4WF3v5eOaJhBycnLCqFGjsrrY78XY9+tXSU5OzjHb9ofWt29feHt7p5sJ+mXa29dvv/2Gfv36ISQkhEHQB8IwKIc4efIk/vrrL51lHTp0QN68eVGoUCGUKFEC/fr1w8qVK3H//n2sXbsWpUuXRunSpTMcbDWnHLjOnz+P/fv3G93o8hn1m9UehFL7RuHnn3/OMBB6OWR0cnJCSEjIRyw1vS3Nfnrjxg04ODjg999/13OJPp7bt29jwoQJ8PDwgLOzM8qVK4fBgwcbXPP596E9858h++uvv2BlZYXWrVvDwcFB7d738uDPBw4cSBcIaQYXz+wpoiGLi4vDpEmT8Mcff+DTTz9Vl2v28wcPHqBNmzYoUKAAunTponOeDg0NRZs2bVCoUCGdMZWyo/j4eNy7dw/Ai3FCTE1NcfLkSQCZt2z88ccfYWlpadBjiQDGO55IaGgoSpYsCTs7O3Ts2BHDhw9H/fr14erqioCAALWFlCFce75q29OEt3PnzlW7tGpa4e/duxdeXl744osvMGbMGFhZWaktWrW7jFWuXBn+/v4GN16UMe/X9G6aNWumThLyuu/+5W3IEI4VhoJhUA4QFxeHVq1aQVEUbNu2Tee1w4cP49q1axk2t5wwYQKsrKx0+utSznT79m04OjqiZ8+e6jLtmZgWLlyYLhACXmxbmpN0amoqD77ZVGRkJHx8fDBnzhx9F+Wji4uLU6cY54Wj4fr777+hKAoURUnXxe/lQKhZs2Y6U2/n1O998+bN8PPzQ7ly5VCjRg3ExMSkG0j3/v376NOnD+zt7VGkSBHUq1cPgYGBKFasGIoWLWoQreQaNGiAOnXqAHgxBpaVlRX69++vvq79/WrOORcvXoSiKNixY0fWFvYjMNbxRK5fv44vv/wShQsXRt68eVG+fHkMGTIk025E2d26desynelt2LBhMDExQYECBdTvNS4uDvPnz4eDgwOsrKywfPlynfdotvvr168b5Axyxr5f05vTnM86dOgAX19fdXt4+dyu/bP2sCf0YZkIGbxcuXLJ4MGDpV69etK4cWPZtm2b+lpwcLC4u7uLs7OziIikpqaq/3369KmUKFFCbG1t9VJu+rjS0tJERASAWFhYSNWqVeW3336TwYMHi4iIubm5JCUliYhIjx49pGfPnvL06VMZOHCg3L9/X0REjhw5IiNHjpSzZ8+KiYmJmJqaqtsQZR8PHz4UMzMziYqKEpEX33lOlStXLrG2ttZ3Meg91alTR/bs2SMiImvWrJHw8HD1NUVR1G24cuXKMmzYMClXrpw0b95cDh48KIqi6KPIH13t2rWlX79+YmVlJWfPnpXr16+LiYmJpKSkiImJiaSlpUmRIkVk8uTJsm7dOvnkk0/k6dOnYmlpKZ06dZJ9+/ZJ6dKl9V2N1+rcubOcPXtWNm3aJC4uLlKhQgXZunWr7N69W0R0v39TU1MREdm+fbs4ODiIv7+/3sr9oQQGBsru3bulTZs2YmNjI/Xq1ZODBw+Kn5+fvov2URUvXlxmzZolp0+flmPHjsk///wj06ZNEzMzM0lNTRUzMzN9F/GNAJDr169L27ZtZeHChSIikpKSor4+adIk+emnn2TWrFkSHBwsAwcOlJs3b0quXLnk2rVr8vjxY3F3d5dChQrpfK5muy9evLg4OTllaZ0+BGPfryljGV2Pmpi8iB/atWsnFy5ckJUrV4qI7jai+VlEZPfu3bJw4UJ5/PhxFpTYCOkthqIPQjs1PXLkCOrUqQNFUbB161YAGY8nlJqaiqNHj8LX1xddu3bNsU9ZjZnmuz5+/DhmzpyJ5ORkdVaK3LlzY9CgQeq62qPwL1y4EMHBwWpT5pIlS6JatWpsEZSNRUZGIiQkBGZmZkbVZYpyhr///hsmJiaYMWNGuummtc9Ne/fuRdeuXXVaLuYkmmNsfHw8Fi1ahMKFC6N06dLpus+8fL6OjIxMNxhtdnfjxg1UqlQJn332GQDg3LlzsLOzQ5UqVdTWX9quX7+Ohg0bolGjRjlqLBGA44kAhtHSL6MyLlq0CKampvjtt9/UZfHx8di5cyeOHDkC4MUYUUFBQRg9ejQGDBgAc3NzTJ8+HWXLlkXTpk3VrmCG8Dd4He7X9DLtaeLj4uJw7do1xMbGqq/fvHkTderUQbFixdL1bNG4c+cO6tati4oVKxpc10lDwTAoB3hdIJSWlqauEx4ejhUrVsDHx0fnQjMnnIhI18WLF2FmZobu3bur/bhv3Ljx2kDojz/+QJMmTdQuCJptJKcNaplTpKWlYfPmzTh//ry+i0L0Tv766y91lrFXBULa004bstfd/MfHx2Px4sVwdnZG3bp11eOz9vsM4ZytXcaXzx+rVq2CoijYtWsXAGDnzp2ws7ODl5cXRo4ciWvXriEsLAzr1q1Dy5YtkTdvXg4WSnq3dOlS9f/j4uIwYMAAODo64urVqxg1ahTq16+frrvbd999h4IFC8LKygq//PILgBfXZ9bW1hgyZEiWlv9D4H5Nb0JzvoqOjkb79u3h5+cHRVHg5+eH3r17q69v27YNfn5+KFasGJYuXaozmPThw4fRunVr2NnZGcWMsfrCMMiAaA7AT548SZeivxwI1apVC4qiqFMxp6WlISUlBbNnz0bu3LlRuXJlgxqwj96M9ol59erVqFevXroxod4kEIqPj8edO3fSzSxGRPQxvGkgZOg0x9L4+HisW7cOI0aMwODBg/Hdd9/hzp076lPTuLg4LFy4EN7e3mjYsGGGgVB2pqmn9th0gG75mzVrhurVq6uDop8/fx5VqlRBrly5YGpqCgsLCxQtWhRBQUE5fjwdyv7u3LkDRVF0xl48f/482rdvD0dHR9jY2GDfvn3qa5rj1jfffAMTExPkz58fp06dUl///fffoSgKlixZklVVeG/cr+lNaLb96OhoeHt7o0KFCpgwYQL++OMPdOnSBYUKFUJAQIC6P2zatAnVq1eHoijw9fVVZ8jVTBhiCGPhGTKGQQYmMjIS+fLlw/fff//KC+Z///0Xrq6uUBRFncozLS0NYWFh+PXXX9UDN2/yc54TJ05gypQpWLZsGdq1a6cu194+MguEXj7BA2wRRERZ41WBUE6gOe9GRUWhcuXKcHFxgaOjI1xcXGBubg4HBwd8++236rTxsbGxGQZChnLejo2NhZeXF9q1a4cjR46oD7E055SlS5eiePHiaitm4MVMl6dPn8b8+fMxd+5cHDlyBE+ePNFL+YletnjxYhQrVgzff/+9uqxVq1YwMTFBu3bt0m2rK1euhKIoWLp0KYKCghAUFKS+Fh4ejh9++AGXLl3KsvJ/CNyv6U2kpaXhyy+/ROnSpXHy5Emd89b69euhKAq++uorddm1a9ewcuVKVKtWDWXKlEH16tXx7bffcpKjLMAwyABt2bIFVlZWr32COnXqVHW2ls2bN6f7HEN5wkhvZ9OmTVAUBYGBgTpPsF6WWSDE8IeI9OWvv/6CpaUlvv/++xw5jkRsbCz8/f0RFBSELVu2IDExEdHR0Th8+DBatWoFKysrdOnSRb0AjouLw6JFi+Dt7Y0mTZrotODM7o4cOaJeg1SsWBGffvppuhvf6tWrIzAwUE8lJHo7YWFhGDBgAKpUqYKjR49i1KhRsLGxQdeuXVGiRAksWrRIvbZ+9OgRjh8/jn///RcAcOHCBTg5OSEkJET9PEO8Dud+TW8iLS0NFStWRI8ePXSWx8bGwsfHB1WqVMF///2XruUv70GyHsMgA/WqJ6iaHenBgweoUKECWrduDUVRcODAAT2UlPRh06ZNsLW1Re7cubFz585M19MEQra2tukO2ERE+rBlyxY4OjrmyNZBY8aMgYeHBw4fPpxh97fBgwcjd+7cGDt2rNplTDOGUJEiRdCxY8esLvI7S0xMRP/+/fHZZ59h5MiRqFOnDnLlyoWpU6ciNDQUAHDy5Em4urpi+vTp6vtyUrdAynkuXLiAWrVqwd3dHba2turgyD179kS+fPlw48YNDBs2DE2aNEFkZKT6vuTkZCxbtgxFixbFzJkz9VX898b9ml4nNTUVjx8/hpubG+bOnasuj4iIgLe3NwIDAzOdDMLQxsbLCRgGGbDMAiHNznPnzh3Y29tj1apVmDp1qsE0Lae3o/m+Xz5obt68GVZWVmjZsiXu3r2b6ftv3LiB1q1bo169ekzkiShbiImJ0XcRPri0tDTUqlUL9evXT/ea9gVwSEgI8ufPj1u3bqnL4uPj8euvvxpck/nFixfD398fx48fR1JSEsaPH4/ixYsjODgYU6dOxbNnz9CuXTu0aNECz58/13dxiTKlfX3UoUMHKIqCkJAQxMXFqcsbNGiAPHnywNLSEqtWrUrX9f7Ro0cYNWqU2hXUUHG/pjdRsWJF1K1bF8CLc5gmCMpo5lsOJK4/DIMMnCYQmj59uk7/25SUFPz111+oVKkSwsLC1OUMhHKOl4ObjJobb9iwARYWFujZs+crT8gPHz5UP49JPBHRh/f06VOUKFECffv2BZD+fKw5hp8/fx7W1tbqeArZvStJRtcV2uenRo0aoWLFiurPBw8exJgxY5ArVy60bt0a/fr1g6IoWLlyZZaUl+h9DB8+HIqiYNiwYTAxMcHixYvV14YMGQJFUVC2bNl0M7Fqrq2y+/6swf2a3lVqairS0tIwduxYuLq6YtWqVfDy8kJQUBCuXLmSbv2xY8diyJAhOWbGUENjImTQ6tWrJ9u2bZNhw4bJtGnT5OLFiyIicuLECZk4caLY29tLwYIF1fXNzMz0VVT6gNLS0sTExESuXr0q8+fPl5CQEOncubNMnTpV9u7dq67XsmVLWb16tSxdulS++uoref78eYafV7hwYTExMZG0tDRRFCWrqkFElCOlpqamW5Y7d26xtLSUS5cuiciL8zEA9XVTU1MREXF1dRVXV1d5+PChzvLsyszMTGJjY2XLli3qMhMTE0lMTBQRkXHjxklqaqosXLhQREQqVaok48ePl3PnzklsbKxcuHBBRETmzZsniYmJOn8TouwkISFB8uTJI3///bf88MMPMmrUKOndu7ecOXNGpk6dKrNmzZI+ffpIWlqajBkzRkRe7AsA1Gur7L4/a3C/ptfRPs+lpaWp/29iYiKKokiPHj0kMTFRPv30U7GwsJBVq1ZJyZIlJTk5WV334sWLsnfvXomJieH9h77oN4uiD+Wff/6Bvb09rK2t4eLigkKFCqFcuXLpnkyQ4dN8l4cPH0axYsXg7u4OHx8fBAQEQFEUFChQAP369dN5z7p162Bubo7PP/+cTXaJiLJAdHQ0pk6dCuB/T9m//vpr5MqVC1u2bFHXy6g1pq+vLzp37pw1BX1PaWlpGDp0KBRFQYsWLbBmzRqd1yMjI9G+fXs0a9ZMXab5e8TGxmLnzp3o27cvzp8/n5XFJnon2i17Hj9+jI4dO8LS0hLm5ubYtGkTwsPDMWbMGFSuXDndvmBIuF/Tq2i+65iYGHz99ddo0aIFvvzySyxdulRnvWPHjqFgwYLw9/fXmWEOAC5evIi2bduiWLFiGbYYoqzBMCgHuXz5Mr755hv07t0bP/74o7qjsmtYznP69GnkzZsXn332GY4cOaIuP3HiBD777DOYmJigbdu2Ou9Zv349LC0t0bdv3xw5MCsRUVZ68uQJVq1alemsnl27dtWZShoA9u/fj8KFC6NevXo4ceKEulz7PH3gwAG4u7vjl19+0fm87Oz58+dYu3YtXFxcULBgQTRu3Bj//fefOiPcpUuXYGlpidmzZ6vvMZTuMkQv094ne/fuDUVRUKVKFVy/fh3Ai7GBGjdujODgYHWZIeJ+TRnRPJSOjo6Gt7c3nJ2dUaFCBXh7e6NYsWJo3769zvoHDx5Evnz5UKhQIbRq1QqzZs1Cjx49UK5cOeTPnx9nzpzRRzXo/2MYlMPxoJyzpKWlISEhAR06dECNGjV0BmHTtAJ79OgRvv32W5ibm2PEiBE671+/fj0URcGPP/6YpeUmIspp9u/fD0VRMHPmzAwD9uHDh6NcuXIAdMOeX375BYqioGHDhvjrr7903nPp0iW0adMGJUuWxJ07dz5q+T+Ge/fuYdq0afDw8ED+/PnRt29fnDx5EgAwefJk1KxZkwOFUo4xZswYmJqaolOnTihTpgymTp2qBiXXrl2Dp6fnKyfwMBTcr+llSUlJqFWrFgICAnDo0CEAL8Khn3/+GYUKFULt2rV11r9x4wY6deqEEiVKQFEUuLi4oH379rh06ZI+ik9aFICdOIkMSVJSkpQoUUJat24tP/74Y4brhIWFyRdffCGnT5+W7du3i4eHh/ra/v37pWLFihw/iojoPf3999/SsGFDmTZtmnTp0kXy5Mmjvvb999/L3Llz5dq1a2Jpaakzbsgvv/wiX331lSQlJUm1atWkTJkycv/+fTl//rxcv35ddu7cKf7+/nqq1ftJS0uTpKQkGTlypOzevVtu3rwpEydOFCsrK/nll19kwIAB0r59e0lNTTWY8VOIMnLo0CG5ffu2hISEyIABA2T9+vWyYsUKqVmzpoiIJCcni7m5uZ5L+WFwvybNdwtArl+/Lm3atJFBgwZJx44d1e88ISFB/vzzT/n888+ldOnSsnPnTvX9CQkJkpycLJcuXZISJUqIhYWFWFtb66s69P8xDCIyMPfv3xc3NzeZO3eu9OrVS+cGQ9u+ffukevXqsnr1amnfvn2611NSUhgIERG9px07dkijRo3SBUJLliyRcePGyZUrV8TS0lJEROd4vW/fPtm2bZv89ttvkpiYKAULFpRPPvlEBg4cqBPgGxrtOp44cULWr18vc+fOlVq1asmOHTukQIECcu7cOZ3gjMhQaSb0EBFp2LChnD59Wi5fviy2traZXp8ZIu7XJCISHx8vQ4YMkdy5c8tPP/0kFy5cEFdXV53tIzExUbZt25YuENKEozlpv8gJOJsYUTb2clabmpoq5ubmYmdnJ3v27JHU1FSdEfw1kpOTpVy5cuLo6ChXrlzJ8LMYBBERvT/NrJ5Dhw6VpUuXSnh4uIiIlCxZUuLi4uThw4c6x1/NMbtq1ary/fffy/nz5+X48eNy7NgxmTNnjkEHQSIiiqKo9Q0ICJDJkyfL1q1bxdHRUezs7OT+/fsSGxur51ISfRiamVhFRH7++WdxdXWVZ8+eiYjkqBte7tckIhIVFSVbtmyRP/74QwoWLKjzoEPD0tJSGjVqJPPnz5czZ85InTp1RETE3NxcUlNTc9R+kROwZRBRNnf79m1JSEgQT09PdVnPnj1l3bp1cu7cOXF2ds6wWW5cXJwULFhQvv32Wxk6dGhWF5uIyKhotxDq0aOH3L17V/z9/eXAgQMSHBys7+LpXUxMjNy/f18sLCzEzc1N38Uh+igSExPVG2RjwP3aeGhawT148EBCQkJk//790r9/f5k5c6YoiqLTSk7kfy2E+vXrJ87OznL06FE9lp4yw6YBRNlYdHS0dOrUSQoWLCgTJkwQHx8fERHp2rWr7NixQ1q0aCH//POP2Nvbq6m8oiiSnJwsmzdvlgIFCkhgYKA+q0BEZBQ0LYQaNmwoAKRRo0Zib28vrVq1Eh8fH3F0dBRfX18pX7682Nvbi5+fn6SkpEiuXLn0XfQsYWNjo/NQgygnMqYgSIT7dU6m3Z1LE/QkJydLkSJFZO3atdKyZUtZs2aNFC9eXAYMGKC2ktMEQpoWQikpKTJ+/Hi5c+eOFCtWTJ9VogywZRBRNqM5kD5//lwsLCykY8eOcuPGDfH395exY8eKp6enJCcny//93//J5MmTpXDhwvLrr79K0aJFxc7OThISEuTYsWMycOBAyZ8/v2zfvp2D+RERZZEdO3ZI48aNZcqUKbJ48WJxcXERDw8POXjwoISHh8uDBw8kLS1NChYsKL6+vrJt2zYeo4mIKNvQjCuquSeJiooSOzs7EflfSPTgwQNp2bKlPH78WAYNGiT9+/cXEUnXQig5OVmSkpI4WHQ2xTCIKBvRHECPHj0qAwYMkJiYGHn27Jk8f/5ckpOTpV27djJmzBjx8fGRxMREWbx4scyaNUtu374tgYGBEhwcLOfOnZO7d++KjY2NHDx4UMzNzdMdmImI6OP566+/pGHDhiIism7dOmnVqpX62n///SdPnz6Vc+fOSe3atflUnYiIsg1NEBQTEyMDBgyQc+fOSVJSkrRs2VK6d+8uRYsWVdd900CIsi+GQUTZhCZpP378uNSoUUNatWolLVu2lBo1asj58+dl5cqVsmzZMmnWrJmMGDFC/P39JSUlRW7evClLly6VHTt2yMOHD6VMmTJSqVIlGTFihJiZmXHWMCIiPdi7d6/UqFFDunfvLj/88IPkzZtXRIQzqRARUbakOT/FxMRIYGCgmJqaSqlSpaRgwYKyYMECqVmzpgwZMkTq1q2rvoeBkGFjGESUxTSDPWc06PPz58+lZcuWYmlpKQsWLBAXFxedm4YZM2bI4sWLpXTp0jJ69Gh1DCGRF0l+QkKC2NjYpPtdRESU9Xbt2iUNGzaUqVOnymeffcZpl4mIKFvSBEEpKSnSrl07uXv3rsydO1eCgoJEROT8+fPSunVrcXBwkBEjRqitX0X+Fwg9e/ZMevfuzYlrDAjjOqIsdPjwYSldurQ8e/ZMDYS0JSQkyNmzZ6VixYri6uqqBkGa9QYNGiQ9e/aUzZs3y3fffadOGy/yYnpTTX9cTcbLIIiISH9q164tW7ZskREjRsjPP/8s0dHR+i4SERGRDk0QlJqaKmZmZnL58mVp1qyZlClTRkREkpKSxNfXVzZu3CjPnz+X77//Xv7880/1/UWKFJHff/9dzM3NZf369RIREaGfitBbYxhElIVu374tt27dkurVq2cYCN29e1ciIiKkRIkSIvJi0DWRF6FOWlqaiIgMGDBAWrRoIb/99ptMnz5dbt++LSIvwiBNeMQuCERE2UO9evVk/fr1MnPmzHQPAIiIiPRNMxNx1apVpW3btmJubi4hISFiYWEhaWlp6n+9vLxk/fr1mQZC//77r/z2229sBWtAGAYRZaFWrVrJ/Pnz5enTp/LJJ5+kC4R8fHzE29tbli1bJklJSWJubq628jExMVHXq169ugCQXbt2ya5du0REeJNBRJRNNW7cWK5evcoLZCIiypaio6OlWrVqsn//fjlz5owcOHBAZ9wfzdTxnp6esmHDBnn+/LlMmTJFtm/frn5G4cKFxcXFRV9VoHfAMIgoiwAQc3Nzadeunfzwww8SERGRLhCytraWOnXqyJEjR2TlypWSkpIiiqLoBEIiIv7+/uLp6Smenp4yZswYSUxMZJcwIqJsjNPqEhFRdpUvXz4ZMGCADB06VGxsbGTLli0SGxurs44mEPLw8JANGzZIdHS0jB49Wv799189lZreF8MgoiyiCXXMzc2lffv2GQZCJiYmMnnyZHF1dZXvvvtOduzYIcnJyaIoiiQlJan/3bJli+TLl09mz54tIiKnTp3Sc+2IiIiIiCi7y2z+KAcHB/n0009l1KhRsnHjRhk5cmS6dbQDodWrV4u9vb06vAUZHoZBRFlIURRJS0vLNBASEcmVK5ds3bpVFEWRAQMGyNKlSyUxMVEsLCxEROTkyZOyZcsWKVeunBQoUECeP3+uvpeIiIiIiCgjqamp6v1IUlKS3L59Wx48eKC+7uDgIF27dpVvv/1WFixYoE4Vr00zdIW3t7f8/fffUqxYsaysAn1AnFqeKItoRurXlpSUJGvWrJERI0ZInjx55MCBA5I/f34REbl165Y0bdpUrl69Ks7OzlK1alW5c+eOXL9+XfLnzy8HDhyQbdu2yYABA+TgwYPi7Oysj2oREREREVE2l5KSImZmZhIbGytDhw6V0NBQ+e+//8TS0lJq1aoljRs3lq5du4qIyMOHD2XJkiUyduxY6dOnj8yZMyfDz8zo/oYMB8MgoiygGYAtPDxcnj17JmZmZmJvby958+aVhIQE+e2332TkyJHpAqGEhASZP3++HDhwQM6dOyfFihWTKlWqyOjRoyUqKko6dOggKSkpsm7dOg5MSkRERERE6WjuRaKjo6VChQpibm4uFStWlLJly8q1a9fk119/lYSEBGncuLEsWbJERETCwsLkl19+eW0gRIaLYRDRR6Y5+B47dkwGDhwo169fl4SEBKlevboMGzZMKleu/MpASCM8PFzy5s0rIiKXL1+WYcOGyf79+2X//v3i6+urj6oREREREZEBSEpKksaNG0tkZKT83//9n5QpU0adgObevXsyfvx42bBhgzRu3FiWLVsmIiKPHz+WRYsWyYQJEyQkJER++eUXfVaBPjCOGUT0EWmCoKNHj0qtWrWkQIECMn78eBk9erTcvXtXPvvsMzlw4IBYWVlJu3btZMqUKenGEEpOThYREXt7exER6d69u3Tq1EkuXLgge/bsYRBERERERESvdPLkSTl//rz06dNHypYtK6ampgJA0tLSxMnJSSZNmiTt2rWT7du3y+LFi0VEpFChQtKrVy8ZPny4bN26VR49eqTnWtCHxJZBRB/Z5cuXpVGjRlKxYkUZN26cuLu7i4hIkyZNZNu2bVKsWDFZsmSJ1KhRQ6eFUN68eWX//v2SP39+NVSKj4+XyZMny9OnT2Xw4MEcvZ+IiIiIiF5r3rx50r9/fwkPDxd7e3ud8X40///w4UOpUqWK+Pj4yObNm9X3RkZGSmpqquTLl09fxaePgC2DiD6itLQ0Wb9+vbi5uUn//v3VIGj48OHyzz//yLBhwyRfvnzStWtX2bdvn04LoaioKPH09JSYmBgxMXmxq+bKlUvGjx8vM2bMYBBERERERETppKSkiMiL2cPi4+NFRMTCwkLMzc3lzp07IiI6Az8riiKpqani6OgoXbt2le3bt8uNGzfU1+3t7RkE5UAMg4g+otTUVDl58qSUK1dOgoKCRERk5syZsmDBAlmwYIF8//33MmzYMImOjpZOnTrJoUOH1EDom2++kWrVqkmuXLnSfa6lpWVWV4WIiIiIiLI5zaxhMTEx0rdvX9m6dasa9CQnJ8v58+dF5MV9ijbNw2cfHx8ReTHGEOVsDIOIPiJzc3OZM2eOfP/99yIicvz4cfnll1+kY8eOUq9ePRERCQkJkR49esjdu3elY8eOsmvXLrGyspIuXbrIhg0bxNTUNN3BmoiIiIiISFtqaqqYmZmps4bt379fvY+oVq2aVK5cWb766iu5d++ezj2Gdpexs2fPipOTkzg5OemtHpQ1GAYRfWRFihRR///cuXNy7do1ad26tRQqVEjS0tJERKRt27ZSvHhxCQoKkoYNG0poaKiYm5ur79OM9E9ERERERJQRU1NTiY+Pl9q1a4uNjY2sXLlSWrduLaampmJraytdu3aVpKQk6dmzp9y9e1cdRFoTBF2/fl0OHDgg5cuXFzMzMz3Xhj42fsNEWUBzkN29e7cUKFBAqlWrJiL/m23MxsZGnj17Jo0aNRJfX18JCAjQc4mJiIiIiMhQaO43li1bJpGRkTJ79mwpW7asOh6QqampdOvWTe7cuSOLFi2SWrVqybRp08TT01M8PT3lzz//lCVLlsjJkyfV2Y4pZ2MYRJQFNGl7+fLl5bfffpM9e/ZIrVq1xMzMTJKSkmTv3r3i6+srISEhaosgzUGbiIiIiIjoVTT3GwcOHJA8efJIrVq11GWmpqbqQ+hx48aJk5OTLFmyRJo3by42NjZiZmYmVlZWUrBgQdm9e7d4e3vrsyqURTi1PFEWun37tlSvXl3c3d1l8uTJEhgYKHv27JHhw4dL8eLF5ddff9UZ2Z+IiIiIiOhNVa5cWdzd3WX58uXpHi5rAiERkfDwcNm1a5dcu3ZNYmJipHr16uLn5yeFCxfWV9EpizEMIspioaGh0rRpU4mNjZX8+fNLbGysuLq6yuHDh8XMzEyn3y4REREREdHrABAAUqNGDbGwsJCdO3eqy7XvLdj7gDQYBhHpwbVr12T16tVy79498fLyki+//FLMzMzUqSCJiIiIiIje1pw5c2To0KGyceNGadCggYikD4RERHbt2iXh4eFSv359sbW11UdRSc8YBhFlE0zpiYiIiIjofTx48EDq1q0r+fPnl2nTpklgYKCI6HYRu3PnjnTu3FksLS1l3bp1Ymdnp88ik55wanmibIJBEBERERERvY8iRYrI0qVL5eLFizJkyBD566+/RETUIOjUqVMybNgwOXPmjEydOpVBkBFjyyAiIiIiIiKiHOT48ePSoUMHdQIbFxcXefz4sdy4cUOePn0q27dvl9KlS+u7mKRHDIOIiIiIiIiIcpg7d+7I8uXLZcWKFRIfHy8FCxaU6tWrS58+faREiRL6Lh7pGcMgIiIiIiIiohwqPj5e0tLSxNramjMXk4phEBEREREREZERYBhEGhxAmoiIiIiIiMgIMAgiDYZBRERERERERERGhGEQEREREREREZERYRhERERERERERGREGAYRERERERERERkRhkFEREREREREREaEYRARERERERERkRFhGEREREREREREZEQYBhERERERERERGRGGQURERERERERERoRhEBERERERERGREfl/7ynnoX/kn3EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(len(pred_tokens)*0.8, len(input_tokens)*0.8))\n",
        "x_axis_label = [model.tgt_tokenizer.decode(x) for x in pred_tokens]\n",
        "y_axis_label = [model.src_tokenizer.decode(x) for x in input_tokens]\n",
        "\n",
        "plt.imshow(att_weights.detach())\n",
        "plt.xticks(range(len(x_axis_label)), x_axis_label, fontsize=15,rotation = 45)\n",
        "plt.yticks(range(len(y_axis_label)), y_axis_label, fontsize=15)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7292462",
      "metadata": {
        "id": "d7292462"
      },
      "source": [
        "## Problem 4: TransSelf Attention (14 pts)\n",
        "- In this problem, you will implement the query-key-value calculation that was used for Transformer\n",
        "- Also, you have to implement self-attention and cross-attention, which are the core components of Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c2c04",
      "metadata": {
        "id": "767c2c04"
      },
      "source": [
        "### 4-1 Implement Query-Key-Value Calculation (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fb7765",
      "metadata": {
        "id": "35fb7765"
      },
      "outputs": [],
      "source": [
        "def get_query_key_value(input_tensor, qkv_layer):\n",
        "  '''\n",
        "  This function returns key, query, and value that is calculated by input tensor and nn_layer.\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "\n",
        "  Outputs:\n",
        "    queries (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    keys (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "    values (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "  Hint: Use torch.chunk() to split a tensor into given number of chunks\n",
        "  '''\n",
        "\n",
        "  qkv = qkv_layer(input_tensor)\n",
        "  queries, keys, values = torch.chunk(qkv, 3, dim=-1)\n",
        "\n",
        "  return queries, keys, values\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(4, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "queries, keys, values = get_query_key_value(test, linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c0cae6",
      "metadata": {
        "id": "82c0cae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22006def-f61e-4d14-88f6-d14387c6714f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([ 0.5393,  0.0587,  0.6597, -1.1150, -0.7343,  0.3282,  0.0551,  0.0178,\n",
        "         0.4408, -0.3078,  0.3289, -0.4874,  0.2256, -0.1007, -0.4304, -0.2109])\n",
        "answer2 = torch.Tensor([ 0.8704, -0.2256,  0.6611,  0.0332, -0.5233, -0.1159,  0.1805,  0.7238,\n",
        "         0.5590,  0.7260,  1.3096,  0.2465,  1.1961,  0.1751, -0.9674,  0.6297])\n",
        "assert keys.ndim == queries.ndim == values.ndim == 3\n",
        "assert keys.shape == queries.shape == values.shape == torch.Size([4, 17, 16])\n",
        "assert not (keys==queries).any() and not (keys==values).any() and not (values==queries).any()\n",
        "assert torch.allclose(queries[2, 13], answer, atol=1e-4)\n",
        "assert torch.allclose(values[0, 3], answer2, atol=1e-4)\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7cd9187",
      "metadata": {
        "id": "f7cd9187"
      },
      "source": [
        "### 4-2 Implement 3d masked softmax (1 pts):\n",
        "  - This would be almost similar to ``get_masked_softmax()``\n",
        "  - It is common to use N x Tq x Tk attention score and mask, but we will use N x Tk x Tq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8167e8a2",
      "metadata": {
        "id": "8167e8a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed486fe-b9fc-4233-c83b-897bee233f7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0184, 0.0284, 0.0529, 0.0524, 0.1245, 0.0395, 0.0352, 0.0184,\n",
              "           0.0950],\n",
              "          [0.0161, 0.1276, 0.0925, 0.0911, 0.1836, 0.0604, 0.0377, 0.0395,\n",
              "           0.0126],\n",
              "          [0.1001, 0.1988, 0.1237, 0.0171, 0.0379, 0.1260, 0.1022, 0.0850,\n",
              "           0.0579],\n",
              "          [0.0682, 0.3607, 0.3320, 0.2082, 0.0229, 0.0107, 0.0498, 0.0933,\n",
              "           0.0882],\n",
              "          [0.0881, 0.1006, 0.1290, 0.1256, 0.0481, 0.0436, 0.0361, 0.1610,\n",
              "           0.1161],\n",
              "          [0.5675, 0.0207, 0.0139, 0.0412, 0.1275, 0.0568, 0.0577, 0.1213,\n",
              "           0.0465],\n",
              "          [0.0977, 0.0606, 0.0435, 0.1700, 0.2438, 0.5983, 0.0104, 0.0445,\n",
              "           0.4247],\n",
              "          [0.0327, 0.0509, 0.1705, 0.2454, 0.1935, 0.0045, 0.6289, 0.0952,\n",
              "           0.0963],\n",
              "          [0.0111, 0.0519, 0.0421, 0.0490, 0.0183, 0.0602, 0.0419, 0.3419,\n",
              "           0.0627]],\n",
              " \n",
              "         [[0.9288, 0.3125, 0.7400, 0.8580, 0.3933, 0.5816, 0.2162, 0.7529,\n",
              "           0.8763],\n",
              "          [0.0712, 0.6875, 0.2600, 0.1420, 0.6067, 0.4184, 0.7838, 0.2471,\n",
              "           0.1237],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000]],\n",
              " \n",
              "         [[0.1348, 0.0708, 0.2531, 0.0649, 0.2238, 0.2366, 0.1064, 0.2152,\n",
              "           0.1012],\n",
              "          [0.1429, 0.0576, 0.0317, 0.1361, 0.0259, 0.1193, 0.0543, 0.0365,\n",
              "           0.0801],\n",
              "          [0.2938, 0.0899, 0.4498, 0.2336, 0.0356, 0.0620, 0.3382, 0.2290,\n",
              "           0.2224],\n",
              "          [0.0369, 0.0240, 0.0282, 0.0611, 0.1783, 0.1915, 0.1153, 0.2063,\n",
              "           0.2741],\n",
              "          [0.0748, 0.2190, 0.0866, 0.0359, 0.1548, 0.2310, 0.1507, 0.0520,\n",
              "           0.2096],\n",
              "          [0.0577, 0.2542, 0.1028, 0.2802, 0.2423, 0.0364, 0.2084, 0.1060,\n",
              "           0.0316],\n",
              "          [0.2591, 0.2845, 0.0479, 0.1882, 0.1394, 0.1233, 0.0269, 0.1550,\n",
              "           0.0810],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000]]]),\n",
              " tensor([[[0.0184, 0.0284, 0.0529, 0.0524, 0.1245, 0.0395, 0.0352, 0.0184,\n",
              "           0.0950],\n",
              "          [0.0161, 0.1276, 0.0925, 0.0911, 0.1836, 0.0604, 0.0377, 0.0395,\n",
              "           0.0126],\n",
              "          [0.1001, 0.1988, 0.1237, 0.0171, 0.0379, 0.1260, 0.1022, 0.0850,\n",
              "           0.0579],\n",
              "          [0.0682, 0.3607, 0.3320, 0.2082, 0.0229, 0.0107, 0.0498, 0.0933,\n",
              "           0.0882],\n",
              "          [0.0881, 0.1006, 0.1290, 0.1256, 0.0481, 0.0436, 0.0361, 0.1610,\n",
              "           0.1161],\n",
              "          [0.5675, 0.0207, 0.0139, 0.0412, 0.1275, 0.0568, 0.0577, 0.1213,\n",
              "           0.0465],\n",
              "          [0.0977, 0.0606, 0.0435, 0.1700, 0.2438, 0.5983, 0.0104, 0.0445,\n",
              "           0.4247],\n",
              "          [0.0327, 0.0509, 0.1705, 0.2454, 0.1935, 0.0045, 0.6289, 0.0952,\n",
              "           0.0963],\n",
              "          [0.0111, 0.0519, 0.0421, 0.0490, 0.0183, 0.0602, 0.0419, 0.3419,\n",
              "           0.0627]],\n",
              " \n",
              "         [[0.9288, 0.3125, 0.7400, 0.8580, 0.3933, 0.5816, 0.2162, 0.7529,\n",
              "           0.8763],\n",
              "          [0.0712, 0.6875, 0.2600, 0.1420, 0.6067, 0.4184, 0.7838, 0.2471,\n",
              "           0.1237],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000]],\n",
              " \n",
              "         [[0.1348, 0.0708, 0.2531, 0.0649, 0.2238, 0.2366, 0.1064, 0.2152,\n",
              "           0.1012],\n",
              "          [0.1429, 0.0576, 0.0317, 0.1361, 0.0259, 0.1193, 0.0543, 0.0365,\n",
              "           0.0801],\n",
              "          [0.2938, 0.0899, 0.4498, 0.2336, 0.0356, 0.0620, 0.3382, 0.2290,\n",
              "           0.2224],\n",
              "          [0.0369, 0.0240, 0.0282, 0.0611, 0.1783, 0.1915, 0.1153, 0.2063,\n",
              "           0.2741],\n",
              "          [0.0748, 0.2190, 0.0866, 0.0359, 0.1548, 0.2310, 0.1507, 0.0520,\n",
              "           0.2096],\n",
              "          [0.0577, 0.2542, 0.1028, 0.2802, 0.2423, 0.0364, 0.2084, 0.1060,\n",
              "           0.0316],\n",
              "          [0.2591, 0.2845, 0.0479, 0.1882, 0.1394, 0.1233, 0.0269, 0.1550,\n",
              "           0.0810],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "def get_3d_masked_softmax(attention_score, mask):\n",
        "  '''\n",
        "  During the batch computation, each sequence in the batch can have different length.\n",
        "  To group them as in a single tensor, we usually pad values\n",
        "\n",
        "  Arguments:\n",
        "    attention_score (torch.Tensor): The attention score in real number that represent how much does query have to attend to each vector in keys\n",
        "                                    Has a shape of [N, Tk, Tq]\n",
        "    mask (torch.Tensor): Boolean tensor with a shape of [N, Tk, Tq] that represents whether the corresponding is valid or not.\n",
        "                         mask[n, tk, tq] == 1 if and only if input_batch[n,tk] is not a padded value.\n",
        "                         If input_batch[n,tk] is a padded value, then mask[n,tk, tq] == 0\n",
        "\n",
        "  Output:\n",
        "    attention_weight (torch.Tensor): The attention weight in real number between 0 and 1. The sum of attention_weight along keys timestep dimension is 1.\n",
        "                                    Has a shape of [N, Tk, Tq]\n",
        "\n",
        "    attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "  TODO: Complete this function without using for loop\n",
        "\n",
        "  '''\n",
        "  assert attention_score.ndim == 3 and mask.ndim == 3\n",
        "\n",
        "  masked_attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n",
        "  attention_weight = torch.softmax(masked_attention_score, dim=1)\n",
        "  attention_weight = attention_weight * mask\n",
        "\n",
        "  return attention_weight\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "torch.manual_seed(0)\n",
        "mask = torch.ones([3, 9, 9])\n",
        "mask[1, 2:] = 0\n",
        "mask[2, 7:] = 0\n",
        "att_score = torch.randn([3, 9, 9])\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[1, 2:] = 0\n",
        "attention_weight = get_3d_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_3d_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6513919a",
      "metadata": {
        "id": "6513919a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e68a1d1-afb1-4ff0-a3c6-f1930b43b9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "\n",
        "answer = torch.tensor([0.1348, 0.1429, 0.2938, 0.0369, 0.0748, 0.0577, 0.2591, 0.0000, 0.0000])\n",
        "\n",
        "assert attention_weight.ndim == 3\n",
        "assert torch.allclose(attention_weight[2,:, 0], answer, atol=1e-4)\n",
        "assert not torch.isnan(attention_weight).any(), \"Error: There is a nan value in attention_weight\"\n",
        "assert torch.allclose(attention_weight, attention_weight_for_modified, atol=1e-4), \"Error: The attention_weight are different even though only masked item is different\"\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddeea3c2",
      "metadata": {
        "id": "ddeea3c2"
      },
      "source": [
        "### 4-3 Implement Self-Attention (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc503bd",
      "metadata": {
        "id": "7dc503bd"
      },
      "outputs": [],
      "source": [
        "def get_self_attention(input_tensor, qkv_layer, mask):\n",
        "  '''\n",
        "  This function returns output of self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "  Arguments:\n",
        "    input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "    kqv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "    mask (torch.Tensor):\n",
        "\n",
        "  Outputs:\n",
        "    output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "  TODO: Complete this function using your completed functions of below:\n",
        "        get_query_key_value()\n",
        "        get_attention_score_for_a_batch_multiple_query()\n",
        "        get_3d_masked_softmax()\n",
        "        get_batch_weighted_sum()\n",
        "  '''\n",
        "  queries, keys, values = get_query_key_value(input_tensor, qkv_layer)\n",
        "  attention_scores = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "  attention_weights = get_3d_masked_softmax(attention_scores, mask)\n",
        "  output = get_batch_weighted_sum(values, attention_weights)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 8)\n",
        "linear = nn.Linear(8, 16 * 3)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "att_vecs = get_self_attention(test, linear, mask)\n",
        "modified_test = test.clone()\n",
        "modified_test[2, 4:] = 0\n",
        "modified_test[4, 14:] = 0\n",
        "modified_att_vecs = get_self_attention(modified_test, linear, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602218f9",
      "metadata": {
        "id": "602218f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd25183-bfec-44f3-efbb-4368896b1007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test cases\n",
        "'''\n",
        "answer = torch.Tensor([-0.3925, -0.0043,  0.0343, -0.6713,  0.2388, -0.4703, -0.2195, -0.1550,\n",
        "        -0.0830, -0.4170, -0.1829,  0.3884,  0.2899,  0.1284,  0.0225, -0.5960])\n",
        "answer2 = torch.Tensor([-0.4078,  0.0173,  0.2670, -0.7959, -0.0314, -0.3455,  0.5751, -0.5806,\n",
        "        -0.3328, -0.2571, -0.4913, -0.1833,  0.6236, -0.5167,  0.3256, -0.9818])\n",
        "assert torch.allclose(att_vecs[3, 2], answer, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[0, 11], answer2, atol=1e-4)\n",
        "assert torch.allclose(att_vecs[4, :14], modified_att_vecs[4, :14], atol=1e-6)\n",
        "\n",
        "\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b23835",
      "metadata": {
        "id": "a1b23835"
      },
      "source": [
        "### 4-4 Implement Multi-head split and concat (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837eeac1",
      "metadata": {
        "id": "837eeac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030cc237-4850-4aed-c34a-b24a44a2de30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "def get_multihead_split(x, num_head):\n",
        "  '''\n",
        "  This function returns split tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "    num_head (int): Number of heads\n",
        "\n",
        "  Output:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    The order of N * num_head is [Batch1_head1, Batch1_head2, ..., Batch1_headN, Batch2_head1, Batch2_head2, ..., Batch2_headN, ...]\n",
        "  '''\n",
        "  assert x.shape[-1] % num_head == 0\n",
        "\n",
        "  # TODO: Complete this function\n",
        "  N, T, C = x.shape\n",
        "  C_per_head = C // num_head\n",
        "  x = x.view(N, T, num_head, C_per_head)\n",
        "  x = x.transpose(1, 2)\n",
        "  x = x.reshape(N * num_head, T, C_per_head)\n",
        "\n",
        "  return x\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dummy_input = torch.randn(4, 17, 32)\n",
        "head_split_output = get_multihead_split(dummy_input, 8)\n",
        "head_split_output.shape\n",
        "\n",
        "assert head_split_output.shape == torch.Size([32, 17, 4])\n",
        "assert (dummy_input[0, :, 4:8] == head_split_output[1]).all()\n",
        "assert (dummy_input[0, 3, 8:12] == head_split_output[2, 3, :]).all()\n",
        "assert (dummy_input[2, 10, 16:20] == head_split_output[20, 10, :]).all()\n",
        "print('Passed all the cases!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ac7f68",
      "metadata": {
        "id": "b0ac7f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb253fe-5dc3-4469-df7e-e935c289c821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 17, 32])\n",
            "Passed all the cases!\n"
          ]
        }
      ],
      "source": [
        "def get_multihead_concat(x, num_head):\n",
        "  '''\n",
        "  This function returns concat tensor for multi-head attention\n",
        "\n",
        "  Arguments:\n",
        "    x (torch.Tensor): Has a shape of [N * num_head, T, C // num_head]\n",
        "    num_head (int): Number of heads\n",
        "  Outputs:\n",
        "    x (torch.Tensor): Has a shape of [N, T, C]\n",
        "  '''\n",
        "  # TODO: Complete this function\n",
        "  # Ensure the input dimensions are compatible with the number of heads\n",
        "  assert x.shape[0] % num_head == 0\n",
        "\n",
        "  N = x.shape[0] // num_head\n",
        "  T = x.shape[1]\n",
        "  C_per_head = x.shape[2]\n",
        "\n",
        "  x = x.view(N, num_head, T, C_per_head)\n",
        "  x = x.permute(0, 2, 1, 3).contiguous()\n",
        "  x = x.view(N, T, num_head * C_per_head)\n",
        "\n",
        "  return x\n",
        "\n",
        "head_cat_output = get_multihead_concat(head_split_output, 8)\n",
        "print(f\"Output shape: {head_cat_output.shape}\")\n",
        "assert head_cat_output.shape == torch.Size([4, 17, 32])\n",
        "assert (dummy_input == head_cat_output).all()\n",
        "print('Passed all the cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff032b3",
      "metadata": {
        "id": "6ff032b3"
      },
      "source": [
        "### 4-5 Implement Transformer-like multi-head self-attention (3 pts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_multi_head_self_attention(input_tensor, qkv_layer, output_proj_layer, mask, num_head=8):\n",
        "    '''\n",
        "    This function returns output of multi-headed self-attention for a given input tensor using with a given kqv_layer\n",
        "\n",
        "    Arguments:\n",
        "      input_tensor (torch.Tensor): Has a shape of [N, T, C]\n",
        "      qkv_layer (torch.nn.Linear): Linear layer with in_features=C and out_features=Cn * 3\n",
        "      output_proj_layer (torch.nn.Linear): Linear layer with in_features=Cn and out_features=C\n",
        "      mask (torch.Tensor): Boolean tensor with a shape of [N, T, T] that represents whether the corresponding is valid or not.\n",
        "                          mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                          If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "      num_head (int): Number of heads\n",
        "\n",
        "    Outputs:\n",
        "      output (torch.Tensor): Has a shape of [N, T, Cn]\n",
        "\n",
        "    TODO: Complete this function using your completed functions of below:\n",
        "          get_query_key_value(): Get QKV from input_tensor and kqv_layer\n",
        "\n",
        "          get_multihead_split(): Split QKV into multiple heads\n",
        "\n",
        "          get_attention_score_for_a_batch_multiple_query(): Get attention score for a batch of multiple queries\n",
        "            CAUTION: You have to scale the attention score by dividing by sqrt(Cn // num_head)\n",
        "            HINT: att_score /= keys.shape[-1] ** 0.5\n",
        "\n",
        "          get_3d_masked_softmax(): Get masked softmax/\n",
        "            CAUTION: You have to repeat mask for num_head times to use it for multi-head attention\n",
        "            USE head_repeated_mask\n",
        "          get_batch_weighted_sum(): Get batch weighted sum\n",
        "\n",
        "          get_multihead_concat(): Concatenate multiple heads into a single tensor\n",
        "\n",
        "          Additionally, use output_proj_layer to project concatenated tensor at the final step\n",
        "    '''\n",
        "\n",
        "    head_repeated_mask = mask.unsqueeze(1).repeat(1, num_head, 1, 1).reshape(-1, mask.shape[1], mask.shape[2])\n",
        "    keys, queries, values = get_query_key_value(input_tensor, qkv_layer)\n",
        "    queries_splited = get_multihead_split(queries, num_head)\n",
        "    keys_splited = get_multihead_split(keys, num_head)\n",
        "    values_splited = get_multihead_split(values, num_head)\n",
        "    att_scores = get_attention_score_for_a_batch_multiple_query(queries_splited, keys_splited)\n",
        "    att_scores /= (keys_splited.shape[-1] ** 0.5)\n",
        "\n",
        "    att_weights = get_3d_masked_softmax(att_scores, head_repeated_mask)\n",
        "    attention_out = get_batch_weighted_sum(values_splited, att_weights)\n",
        "    attention_out = get_multihead_concat(attention_out, num_head)\n",
        "    output = output_proj_layer(attention_out)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "dV7aUpbdxw0w"
      },
      "id": "dV7aUpbdxw0w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "test = torch.randn(5, 17, 16)\n",
        "linear = nn.Linear(16, 16 * 3)\n",
        "out_proj = nn.Linear(16, 16)\n",
        "\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "att_vecs = get_multi_head_self_attention(test, linear, out_proj, mask, num_head=4)\n",
        "att_vecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfbrRl5Rx2Mz",
        "outputId": "c28b4b3c-ec42-44e1-b827-bc2588289e38"
      },
      "id": "YfbrRl5Rx2Mz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0155,  0.2163,  0.1873,  ...,  0.2330, -0.0566, -0.0221],\n",
              "         [ 0.0018,  0.2200,  0.2440,  ...,  0.2327, -0.0793, -0.0086],\n",
              "         [-0.0333,  0.2005,  0.1883,  ...,  0.2249, -0.0467,  0.0511],\n",
              "         ...,\n",
              "         [ 0.0049,  0.1976,  0.2011,  ...,  0.2837, -0.0762, -0.0173],\n",
              "         [-0.0048,  0.1969,  0.2040,  ...,  0.2370, -0.0491,  0.0220],\n",
              "         [ 0.0070,  0.2108,  0.2147,  ...,  0.2444, -0.0692,  0.0032]],\n",
              "\n",
              "        [[ 0.1762,  0.0831,  0.3563,  ...,  0.3361, -0.0043,  0.0481],\n",
              "         [ 0.1654,  0.0404,  0.3476,  ...,  0.3637, -0.0096,  0.0640],\n",
              "         [ 0.1985,  0.0251,  0.3585,  ...,  0.3495,  0.0224,  0.0492],\n",
              "         ...,\n",
              "         [ 0.1915, -0.0330,  0.3782,  ...,  0.3602,  0.0114,  0.0978],\n",
              "         [ 0.1528,  0.1721,  0.3121,  ...,  0.2798,  0.0148,  0.0383],\n",
              "         [ 0.1735,  0.0314,  0.3488,  ...,  0.3430,  0.0342,  0.0504]],\n",
              "\n",
              "        [[ 0.0398, -0.0339,  0.2237,  ...,  0.1077,  0.1416,  0.3753],\n",
              "         [ 0.0537, -0.2029,  0.3021,  ...,  0.1250,  0.1193,  0.3847],\n",
              "         [ 0.0837, -0.1654,  0.2692,  ...,  0.0562,  0.1523,  0.3070],\n",
              "         ...,\n",
              "         [ 0.1213, -0.1879,  0.3158,  ...,  0.1442,  0.0738,  0.2045],\n",
              "         [ 0.0485, -0.2003,  0.2496,  ...,  0.0959,  0.1746,  0.3408],\n",
              "         [ 0.1259, -0.2493,  0.3138,  ...,  0.0841,  0.1370,  0.1700]],\n",
              "\n",
              "        [[ 0.0821, -0.0167,  0.4217,  ...,  0.2492, -0.0860,  0.0578],\n",
              "         [ 0.0921, -0.0484,  0.4121,  ...,  0.2906, -0.0558,  0.0778],\n",
              "         [ 0.1070, -0.0406,  0.4942,  ...,  0.2569, -0.1220,  0.0496],\n",
              "         ...,\n",
              "         [ 0.0853, -0.0699,  0.3606,  ...,  0.3099, -0.0510,  0.0605],\n",
              "         [ 0.0762, -0.0082,  0.4228,  ...,  0.2967, -0.0624,  0.0478],\n",
              "         [ 0.0731, -0.0178,  0.4637,  ...,  0.2533, -0.1377,  0.0615]],\n",
              "\n",
              "        [[ 0.0144,  0.1123,  0.3037,  ...,  0.3055, -0.1575,  0.1054],\n",
              "         [ 0.0672,  0.0909,  0.3361,  ...,  0.3751, -0.1283,  0.0861],\n",
              "         [ 0.0732,  0.0594,  0.3635,  ...,  0.4045, -0.1365,  0.0827],\n",
              "         ...,\n",
              "         [ 0.0855,  0.0693,  0.2994,  ...,  0.3498, -0.1078,  0.0768],\n",
              "         [ 0.0711,  0.0446,  0.3318,  ...,  0.3846, -0.1258,  0.0992],\n",
              "         [ 0.0088,  0.0352,  0.3183,  ...,  0.3677, -0.1335,  0.1518]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a3d6ac",
      "metadata": {
        "id": "c7a3d6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2af936e-9fe2-4643-9ff1-d1ec519aed6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test cases!\n"
          ]
        }
      ],
      "source": [
        "wrong_answer = torch.tensor([ 0.0105,  0.2337,  0.1587, -0.0346,  0.1446,  0.0231,  0.0296, -0.0437,\n",
        "        -0.1101,  0.2079, -0.0813, -0.2343, -0.3270,  0.2156, -0.0427, -0.0313])\n",
        "wrong_answer2 = torch.tensor([ 0.0557,  0.1026, -0.0834,  0.1770, -0.4083, -0.1543,  0.2025, -0.1570,\n",
        "         0.0974, -0.0317,  0.0569, -0.0610,  0.0063,  0.0107, -0.0727,  0.0796])\n",
        "\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer, atol=1e-4), \"Error: You did not include attention score scaling. Read the CAUTION!\"\n",
        "assert not torch.allclose(att_vecs[0, 0], wrong_answer2, atol=1e-4), \"Error: You did not include output projection. Read the TODO!\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422a8e6d",
      "metadata": {
        "id": "422a8e6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135a9c41-7619-4bd0-9962-af4628b25c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Compare with official implementation of PyTorch\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(16, num_heads=4, batch_first=True)\n",
        "official_attention.in_proj_weight.data = linear.weight.data\n",
        "official_attention.in_proj_bias.data = linear.bias.data\n",
        "official_attention.out_proj.weight.data = out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = out_proj.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 4, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, attn_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(att_vecs, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169acb5",
      "metadata": {
        "id": "c169acb5"
      },
      "source": [
        "### 4-6 Implement it as a single module (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48M0cSsLW8DA",
      "metadata": {
        "id": "48M0cSsLW8DA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa802d7-b17c-4fd3-d06b-a4a72206c80c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 17, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.qkv = nn.Linear(self.input_size, self.hidden_size * 3)\n",
        "    self.out_proj = nn.Linear(self.hidden_size, self.input_size)\n",
        "    self.mask_value = mask_value\n",
        "    self.num_head = num_head\n",
        "    assert self.hidden_size % self.num_head == 0\n",
        "    self.dim_per_head = self.hidden_size // self.num_head\n",
        "\n",
        "  def _get_qkv(self, x):\n",
        "      return get_query_key_value(x, self.qkv)\n",
        "\n",
        "  def _get_multihead_split(self, x):\n",
        "      return get_multihead_split(x, self.num_head)\n",
        "\n",
        "  def _get_multiheaded_att_score(self, keys, queries):\n",
        "      att_scores = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "      return att_scores / (self.dim_per_head ** 0.5)\n",
        "\n",
        "  def _get_masked_softmax(self, score, masks):\n",
        "      head_repeated_mask = masks.unsqueeze(1).repeat(1, self.num_head, 1, 1).reshape(-1, masks.shape[1], masks.shape[2])\n",
        "      return get_3d_masked_softmax(score, head_repeated_mask)\n",
        "\n",
        "  def _get_weighted_sum(self, values, weights):\n",
        "      return get_batch_weighted_sum(values, weights)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "      if mask is None:\n",
        "          mask = torch.ones([x.shape[0], x.shape[1], x.shape[1]])\n",
        "\n",
        "      queries, keys, values = self._get_qkv(x)\n",
        "\n",
        "      queries = self._get_multihead_split(queries)\n",
        "      keys = self._get_multihead_split(keys)\n",
        "      values = self._get_multihead_split(values)\n",
        "\n",
        "      att_scores = self._get_multiheaded_att_score(keys, queries)\n",
        "\n",
        "      att_weights = self._get_masked_softmax(att_scores, mask)\n",
        "\n",
        "      att_output = self._get_weighted_sum(values, att_weights)\n",
        "\n",
        "      multihead_concated = get_multihead_concat(att_output, self.num_head)\n",
        "\n",
        "      output = self.out_proj(multihead_concated)\n",
        "\n",
        "      return output\n",
        "\n",
        "torch.manual_seed(0)\n",
        "attention_module = SelfAttention(512, 512, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "out = attention_module(test, mask)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0ed205",
      "metadata": {
        "id": "ef0ed205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fedd52-3bc8-4182-b7e2-0ca985c98d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_attention = torch.nn.MultiheadAttention(512, num_heads=8, batch_first=True)\n",
        "official_attention.in_proj_weight.data = attention_module.qkv.weight.data\n",
        "official_attention.in_proj_bias.data = attention_module.qkv.bias.data\n",
        "official_attention.out_proj.weight.data = attention_module.out_proj.weight.data\n",
        "official_attention.out_proj.bias.data = attention_module.out_proj.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_attention_output, attention_weights = official_attention(test, test, test, attn_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(out, official_attention_output, atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc74bbbf",
      "metadata": {
        "id": "cc74bbbf"
      },
      "source": [
        "### Test your SelfAttention module on Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac3e753",
      "metadata": {
        "id": "bac3e753"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.input_size = in_size\n",
        "    self.layer = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(hidden_size, in_size))\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class PosEncoding(nn.Module):\n",
        "  def __init__(self, size, max_t):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.max_t = max_t\n",
        "    self.register_buffer('encoding', self._prepare_emb())\n",
        "\n",
        "  def _prepare_emb(self):\n",
        "    dim_axis = 10000**(torch.arange(self.size//2) * 2 / self.size)\n",
        "    timesteps = torch.arange(self.max_t)\n",
        "    pos_enc_in = timesteps.unsqueeze(1) / dim_axis.unsqueeze(0)\n",
        "    pos_enc_sin = torch.sin(pos_enc_in)\n",
        "    pos_enc_cos = torch.cos(pos_enc_in)\n",
        "\n",
        "    pos_enc = torch.stack([pos_enc_sin, pos_enc_cos], dim=-1).reshape([self.max_t, 512])\n",
        "    return pos_enc\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.encoding[x]\n",
        "\n",
        "class ResidualLayerNormModule(nn.Module):\n",
        "  def __init__(self, submodule):\n",
        "    super().__init__()\n",
        "    self.submodule = submodule\n",
        "    self.layer_norm = nn.LayerNorm(self.submodule.input_size)\n",
        "\n",
        "  def forward(self, x, mask=None, y=None):\n",
        "    if y is not None:\n",
        "      res_x = self.submodule(x, y, mask)\n",
        "    elif mask is not None:\n",
        "      res_x = self.submodule(x, mask)\n",
        "    else:\n",
        "      res_x = self.submodule(x)\n",
        "    x =  x + res_x\n",
        "    return self.layer_norm(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.mlp_block(self.att_block(x['input'], x['mask']))\n",
        "    return {'input':out, 'mask':x['mask']}\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "encoder_layer = EncoderLayer(512, 512, 2048, 8)\n",
        "test = torch.randn(5, 17, 512)\n",
        "mask = torch.ones([5, 17, 17])\n",
        "mask[2, 4:] = 0\n",
        "mask[4, 14:] = 0\n",
        "\n",
        "out = encoder_layer({'input':test, 'mask':mask})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb07b11b",
      "metadata": {
        "id": "bb07b11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7834ed-6421-4e6e-e41e-40f6f6d59ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test case!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test case\n",
        "'''\n",
        "\n",
        "official_encoder_layer = nn.TransformerEncoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "\n",
        "official_encoder_layer.self_attn.in_proj_weight.data = encoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_encoder_layer.self_attn.in_proj_bias.data = encoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_encoder_layer.self_attn.out_proj.weight.data = encoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_encoder_layer.self_attn.out_proj.bias.data = encoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_encoder_layer.linear1.weight.data = encoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_encoder_layer.linear1.bias.data = encoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_encoder_layer.linear2.weight.data = encoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_encoder_layer.linear2.bias.data = encoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_encoder_layer.norm1.weight.data = encoder_layer.att_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm1.bias.data = encoder_layer.att_block.layer_norm.bias.data\n",
        "official_encoder_layer.norm2.weight.data = encoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_encoder_layer.norm2.bias.data = encoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "head_repeated_mask = mask.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask.shape[1], mask.shape[2]).transpose(1,2)\n",
        "official_encoder_output = official_encoder_layer(test, src_mask=head_repeated_mask==0)\n",
        "\n",
        "assert torch.allclose(official_encoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test case!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b169486",
      "metadata": {
        "id": "1b169486"
      },
      "source": [
        "### 4-7 Implement Transformer-like multi-head cross-attention (2 pts)\n",
        "- Implement ``CrossAttention`` inheriting `SelfAttention` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5b8a722",
      "metadata": {
        "id": "b5b8a722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8382b13-2596-406e-fead-e094ce01e1ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 19, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "class CrossAttention(SelfAttention):\n",
        "  def __init__(self, input_size, hidden_size, num_head, mask_value=0):\n",
        "    super().__init__(input_size, hidden_size, num_head, mask_value)\n",
        "\n",
        "  def forward(self, q_seq, kv_seq, mask=None):\n",
        "      '''\n",
        "      Arguments:\n",
        "        q_seq (torch.Tensor): Sequence to be used for query\n",
        "        kv_seq (torch.Tensor): Sequence to be used for key and value\n",
        "        mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ty, Tx]\n",
        "\n",
        "      Outputs:\n",
        "        output (torch.Tensor): Output of cross attention. Shape: [N, Tx, C]\n",
        "      '''\n",
        "      if mask is None:\n",
        "          mask = torch.ones([q_seq.shape[0], kv_seq.shape[1], q_seq.shape[1]])\n",
        "\n",
        "      queries = self.qkv(q_seq)[:, :, :self.hidden_size]\n",
        "      keys, values = self.qkv(kv_seq)[:, :, self.hidden_size:].chunk(2, dim=-1)\n",
        "\n",
        "      queries = self._get_multihead_split(queries)\n",
        "      keys = self._get_multihead_split(keys)\n",
        "      values = self._get_multihead_split(values)\n",
        "\n",
        "      att_scores = self._get_multiheaded_att_score(keys, queries)\n",
        "      att_weights = self._get_masked_softmax(att_scores, mask)\n",
        "      att_output = self._get_weighted_sum(values, att_weights)\n",
        "\n",
        "      multihead_concat = get_multihead_concat(att_output, self.num_head)\n",
        "      output = self.out_proj(multihead_concat)\n",
        "\n",
        "      return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head):\n",
        "    super().__init__()\n",
        "    self.att_block = ResidualLayerNormModule(SelfAttention(in_size, emb_size, num_head))\n",
        "    self.cross_att_block = ResidualLayerNormModule(CrossAttention(in_size, emb_size, num_head))\n",
        "    self.mlp_block = ResidualLayerNormModule(MLP(emb_size, mlp_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.att_block(x['input'], x['decoder_mask'])\n",
        "    out = self.cross_att_block(out,  x['encoder_mask'], x['encoder_out'])\n",
        "    out = self.mlp_block(out)\n",
        "    return {'input':out, 'decoder_mask':x['decoder_mask'], 'encoder_out':x['encoder_out'], 'encoder_mask':x['encoder_mask']}\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "decoder_layer = DecoderLayer(512, 512, 2048, 8)\n",
        "test_src = torch.randn(5, 17, 512)\n",
        "test_tgt = torch.randn(5, 19, 512)\n",
        "mask_src = torch.ones([5, 17, 19])\n",
        "mask_src[2, 4:] = 0\n",
        "mask_src[4, 14:] = 0\n",
        "mask_tgt = torch.tril(torch.ones(test_tgt.shape[0], test_tgt.shape[1], test_tgt.shape[1]))\n",
        "\n",
        "out = decoder_layer({'input':test_tgt, 'decoder_mask':mask_tgt, 'encoder_out':test_src, 'encoder_mask':mask_src})\n",
        "out['input'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd50ff7d",
      "metadata": {
        "id": "fd50ff7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38ede70-4d17-4adb-d7bf-ed7af6a3a206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the test cases!\n"
          ]
        }
      ],
      "source": [
        "official_decoder_layer = nn.TransformerDecoderLayer(512, 8, 2048, batch_first=True, dropout=0)\n",
        "official_decoder_layer.self_attn.in_proj_weight.data = decoder_layer.att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.self_attn.in_proj_bias.data = decoder_layer.att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.self_attn.out_proj.weight.data = decoder_layer.att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.self_attn.out_proj.bias.data = decoder_layer.att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.multihead_attn.in_proj_weight.data = decoder_layer.cross_att_block.submodule.qkv.weight.data\n",
        "official_decoder_layer.multihead_attn.in_proj_bias.data = decoder_layer.cross_att_block.submodule.qkv.bias.data\n",
        "official_decoder_layer.multihead_attn.out_proj.weight.data = decoder_layer.cross_att_block.submodule.out_proj.weight.data\n",
        "official_decoder_layer.multihead_attn.out_proj.bias.data = decoder_layer.cross_att_block.submodule.out_proj.bias.data\n",
        "official_decoder_layer.linear1.weight.data = decoder_layer.mlp_block.submodule.layer[0].weight.data\n",
        "official_decoder_layer.linear1.bias.data = decoder_layer.mlp_block.submodule.layer[0].bias.data\n",
        "official_decoder_layer.linear2.weight.data = decoder_layer.mlp_block.submodule.layer[2].weight.data\n",
        "official_decoder_layer.linear2.bias.data = decoder_layer.mlp_block.submodule.layer[2].bias.data\n",
        "official_decoder_layer.norm1.weight.data = decoder_layer.att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm1.bias.data = decoder_layer.att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm2.weight.data = decoder_layer.cross_att_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm2.bias.data = decoder_layer.cross_att_block.layer_norm.bias.data\n",
        "official_decoder_layer.norm3.weight.data = decoder_layer.mlp_block.layer_norm.weight.data\n",
        "official_decoder_layer.norm3.bias.data = decoder_layer.mlp_block.layer_norm.bias.data\n",
        "\n",
        "# Mask for self-attentionq\n",
        "# In nn.TransformerEncoderLayer or nn.TransformerDecoderLayer, the mask is expected to be [N, Tq, Tk]\n",
        "# where N is the batch size, Tt is the query sequence length, and Ts is the key sequence length.\n",
        "head_repeated_mask_src = mask_src.unsqueeze(1).repeat(1, 8, 1, 1).reshape(-1, mask_src.shape[1], mask_src.shape[2]).transpose(1,2)\n",
        "head_repeated_mask_tgt = mask_tgt.unsqueeze(1).repeat(1,8,1,1).reshape(-1, mask_tgt.shape[1], mask_tgt.shape[2]).transpose(1,2)\n",
        "official_decoder_output = official_decoder_layer(test_tgt, test_src, tgt_mask=head_repeated_mask_tgt==0, memory_mask=head_repeated_mask_src==0)\n",
        "\n",
        "assert torch.allclose(official_decoder_output, out['input'], atol=1e-4), \"Your output is different from the official output\"\n",
        "print('Passed the test cases!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d895a7da",
      "metadata": {
        "id": "d895a7da"
      },
      "source": [
        "### Using your implementation, we can build Transformer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266428cc",
      "metadata": {
        "id": "266428cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db08cf9-24b7-4d80-f1a5-1607978916e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(EncoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = torch.ones([x.shape[0], x.shape[1], x.shape[1]])\n",
        "    mask[x==0] = 0\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'mask':mask})\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_layers, vocab_size):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential()\n",
        "    for i in range(num_layers):\n",
        "      self.layers.append(DecoderLayer(in_size,emb_size,mlp_size,num_head))\n",
        "    self.pos_enc = PosEncoding(emb_size, 10000)\n",
        "    self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    mask = torch.triu(torch.ones(x.shape[0], x.shape[1], x.shape[1]))\n",
        "    cross_attention_mask = torch.ones(x.shape[0], y['input'].shape[1], x.shape[1]) # N, Tk, Tq\n",
        "    cross_attention_mask[y['mask'][:,:, 0]==0] = 0\n",
        "\n",
        "    temp = torch.ones_like(x)\n",
        "    result = torch.arange(x.shape[-1]).to(x.device) * temp\n",
        "    x = self.token_emb(x) + self.pos_enc(result)\n",
        "    return self.layers({'input':x, 'decoder_mask':mask, 'encoder_out':y['input'], 'encoder_mask':cross_attention_mask})\n",
        "\n",
        "class TransformerTranslator(nn.Module):\n",
        "  def __init__(self, in_size, emb_size, mlp_size, num_head, num_enc_layers, num_dec_layers, enc_vocab_size, dec_vocab_size):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(in_size, emb_size, mlp_size, num_head, num_enc_layers, enc_vocab_size)\n",
        "    self.decoder = Decoder(in_size, emb_size, mlp_size, num_head, num_dec_layers, dec_vocab_size)\n",
        "    self.final_proj = nn.Linear(emb_size, dec_vocab_size)\n",
        "\n",
        "  def forward(self, x:torch.Tensor, y:torch.Tensor):\n",
        "    '''\n",
        "    Arguments:\n",
        "    '''\n",
        "    enc_out = self.encoder(x)\n",
        "    dec_out = self.decoder(y, enc_out)\n",
        "    return self.final_proj(dec_out['input']).softmax(dim=-1)\n",
        "\n",
        "class TransformerTrainer(Trainer):\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    super().__init__(model, optimizer, loss_fn, train_loader, valid_loader, device)\n",
        "    self.num_iter = 0\n",
        "    self._adjust_optim()\n",
        "\n",
        "  def _adjust_optim(self):\n",
        "    self.num_iter += 1\n",
        "    self.optimizer.param_groups[0]['lr'] = 512 ** (-0.5) * min(self.num_iter**(-0.5), self.num_iter*4000**(-1.5))\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "\n",
        "    self.model (Translator/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "    '''\n",
        "\n",
        "    src, tgt_i, tgt_o = batch\n",
        "    pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "    pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "    loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "    self._adjust_optim()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    '''\n",
        "\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "\n",
        "    validation_loss = 0\n",
        "    num_correct_guess = 0\n",
        "    num_data = 0\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(loader, leave=False):\n",
        "        src, tgt_i, tgt_o = batch\n",
        "        tgt_o = tgt_o.to(self.device)\n",
        "        pred = self.model(src.to(self.device), tgt_i.to(self.device))\n",
        "        pred = pack_padded_sequence(pred, pad_packed_sequence(tgt_o)[1], batch_first=True, enforce_sorted=False)\n",
        "        loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          loss = self.loss_fn(pred.data, tgt_o.data)\n",
        "        else:\n",
        "          loss = self.loss_fn(pred, tgt_o)\n",
        "\n",
        "        validation_loss += loss.item() * len(pred.data)\n",
        "        if isinstance(pred, PackedSequence):\n",
        "          num_correct_guess += (pred.data.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        else:\n",
        "          num_correct_guess += (pred.argmax(dim=-1) == tgt_o.data).sum().item()\n",
        "        num_data += len(pred.data)\n",
        "    return validation_loss / num_data, num_correct_guess / num_data\n",
        "\n",
        "def pad_collate(raw_batch):\n",
        "  srcs = [x[0] for x in raw_batch]\n",
        "  tgts_i = [x[1][:-1] for x in raw_batch]\n",
        "  tgts_o = [x[1][1:] for x in raw_batch]\n",
        "\n",
        "  srcs = pad_sequence(srcs, batch_first=True)\n",
        "  tgts_i = pad_sequence(tgts_i, batch_first=True)\n",
        "  tgts_o = pack_sequence(tgts_o, enforce_sorted=False)\n",
        "  return srcs, tgts_i, tgts_o\n",
        "\n",
        "single_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "train_loader = DataLoader(trainset, batch_size=4, collate_fn=pad_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pad_collate, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "tfm_model = TransformerTranslator(512,512,2048,8,6,12,32000,32000)\n",
        "out = tfm_model(batch[0], batch[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42f7c9b",
      "metadata": {
        "id": "c42f7c9b"
      },
      "source": [
        "#### Transformer Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c32d97",
      "metadata": {
        "id": "36c32d97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3c01e3-91ee-4c2d-ad47-6ee09c695a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1nxbQPD4a2SVicLBfzrj2pRU4hAapeMmL\n",
            "From (redirected): https://drive.google.com/uc?id=1nxbQPD4a2SVicLBfzrj2pRU4hAapeMmL&confirm=t&uuid=dd738df1-cd5e-4b47-af3a-b56ce3ca7e1b\n",
            "To: /content/kor_eng_translator_tfm_model_best.pt\n",
            "100% 1.46G/1.46G [00:14<00:00, 101MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download pre-trained weight\n",
        "!gdown 1nxbQPD4a2SVicLBfzrj2pRU4hAapeMmL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40d8308",
      "metadata": {
        "id": "e40d8308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dc839a-9046-44f3-f555-89a5b82c19cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "tfm_model = TransformerTranslator(512, 512, 2048, 8, 6, 12, 32000, 32000)\n",
        "state_dict = torch.load('kor_eng_translator_tfm_model_best.pt', map_location=torch.device('cpu'))\n",
        "tfm_model.load_state_dict(state_dict['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec607d0b",
      "metadata": {
        "id": "ec607d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd33287-1b14-4df9-ea9e-9f71448a3199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i want to go home because it's difficult to process natural language.\n"
          ]
        }
      ],
      "source": [
        "def translate_tfm(model, source_sentence, src_tokenizer, tgt_tokenizer):\n",
        "  '''\n",
        "\n",
        "  Arguments:\n",
        "    model (TranslatorAtt): Translator model with attention\n",
        "    source_sentence (str): Sentence to translate\n",
        "\n",
        "  Returns:\n",
        "    input_tokens (list): Source sentence in a list of token in token_id\n",
        "    predicted_tokens (list): Translated sentence in a list of token in token_id\n",
        "    decoded_string (str): Translated sentence in string\n",
        "\n",
        "  '''\n",
        "\n",
        "  input_tokens = src_tokenizer.encode(source_sentence)\n",
        "  input_tensor = torch.LongTensor(input_tokens).unsqueeze(0)\n",
        "  enc_out = model.encoder(input_tensor)\n",
        "\n",
        "  # Setup for 0th step\n",
        "  current_decoder_token = torch.LongTensor([[2]]) # start of sentence token\n",
        "\n",
        "  for i in range(50): # You can chage it to while True:\n",
        "    decoder_out = model.decoder(current_decoder_token, enc_out)['input']\n",
        "    logit = model.final_proj(decoder_out[0, -1])\n",
        "    selected_token = torch.argmax(logit, dim=-1)\n",
        "    current_decoder_token = torch.tensor(current_decoder_token[0].tolist() + [selected_token], dtype=torch.long).unsqueeze(0)\n",
        "    if selected_token == 3: ## end of sentence token\n",
        "      break\n",
        "  predicted_tokens = current_decoder_token.squeeze().tolist()[1:-1]\n",
        "  print(tgt_tokenizer.decode(predicted_tokens))\n",
        "  return input_tokens, predicted_tokens, tgt_tokenizer.decode(predicted_tokens)\n",
        "\n",
        "tfm_model.cpu()\n",
        "tfm_model.eval()\n",
        "input_tokens, pred_tokens, translated_string  = translate_tfm(tfm_model, '자연어 처리는 어렵다 집에 가고 싶다', src_tokenizer, tgt_tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8dda25",
      "metadata": {
        "id": "ce8dda25"
      },
      "source": [
        "# Check Before Submission\n",
        "- Copy and paste your code to the downloaded ``NLP_Assignment_4.py``\n",
        "  - https://raw.githubusercontent.com/jdasam/aat3020/main/NLP_Assignment_4.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ec2b52",
      "metadata": {
        "id": "29ec2b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a50286d-c7ed-4963-858c-25182ab0f005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem 1 Passed!\n",
            "Problem 2 Passed!\n",
            "2024-06-26 09:34:30.727508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-26 09:34:30.741719: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-26 09:34:30.743068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-26 09:34:34.126449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Problem 3 Passed!\n",
            "Passed all the cases!\n",
            "Output shape: torch.Size([4, 17, 32])\n",
            "Problem 4 Passed!\n"
          ]
        }
      ],
      "source": [
        "# Run this code after copy and paste your code to NLP_Assignment_4.py\n",
        "!python NLP_Assignment_4_20191138.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BSA762AXEfs9"
      },
      "id": "BSA762AXEfs9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}